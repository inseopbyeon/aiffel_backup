{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd50330e",
   "metadata": {},
   "source": [
    "# Human Pose Estimation(HPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3ca03",
   "metadata": {},
   "source": [
    "* 첫 번째 방법은 Top-down 방법입니다.**(여기서 다루는 내용)**\n",
    "\n",
    "    모든 사람의 정확한 keypoint 를 찾기 위해 object detection 을 사용합니다.\n",
    "    crop 한 이미지 내에서 keypoint 를 찾아내는 방법으로 표현합니다.\n",
    "    detector가 선행되어야 하고 모든 사람마다 알고리즘을 적용해야 하기 때문에 사람이 많이 등장할 때는 느리다는 단점이 있습니다.  \n",
    "\n",
    "\n",
    "* 두 번째 방법은 Bottom-up 방법입니다.\n",
    "\n",
    "    detector가 없고 keypoint 를 먼저 검출합니다.\n",
    "    예를 들어 손목에 해당하는 모든 점들을 검출합니다.\n",
    "    한 사람에 해당하는 keypoint 를 clustering 합니다.\n",
    "    detector 가 없기 때문에 다수의 사람이 영상에 등장하더라도 속도 저하가 크지 않습니다. 반면 top down 방식에 비해 keypoint 검출 범위가 넓어 성능이 떨어진다는 단점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccd26f",
   "metadata": {},
   "source": [
    " Pose 는 face landmark 랑 비슷해요. human pose estimation 은 keypoint 의 localization 문제를 푼다는 점에서 비슷합니다. 하지만 손목, 팔꿈치 등의 joint keypoint 정보는 얼굴의 keypoint 보다 훨씬 다양한 위치와 변화를 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2034866",
   "metadata": {},
   "source": [
    "위 이미지에서 볼 수 있듯이 손이 얼굴을 가리는 행위, 모든 keypoint 가 영상에 담기지 않는 등 invisible , occlusions, clothing, lighting change 가 face landmark 에 비해 더 어려운 환경을 만들어 냅니다.\n",
    "\n",
    "딥러닝 기반 방법이 적용되기 전에는 다양한 사전 지식이 사용되었습니다.\n",
    "가장 기본이 되는 아이디어는 \"인체는 변형 가능 부분으로 나누어져 있고 각 부분끼리 연결성을 가지고 있다.\" 는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f935e72",
   "metadata": {},
   "source": [
    "#### 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f79dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(PROJECT_PATH, 'best_models') # 수정\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612ba2c",
   "metadata": {},
   "source": [
    "* **json 파싱하기**  \n",
    "\n",
    "이전 스텝에서 train.json과 validation.json 파일이 소개된 것을 기억하시나요? 이 파일들은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있어서 Pose Estimation을 위한 label로 삼을 수 있습니다.\n",
    "\n",
    "우선 json이 어떻게 구성되어 있는지 파악해 보기 위해 json 파일을 열어 샘플로 annotation 정보를 1개만 출력해 봅시다. json.dumps()를 활용해서 좀 더 명확하게 하면 더욱 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13cb05b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json) # json파일객체을 열어서\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2) # 해당부분을 문자열로 변환\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be96dbf",
   "metadata": {},
   "source": [
    "joints 가 우리가 label 로 사용할 keypoint 의 label 입니다. 이미지 형상과 사람의 포즈에 따라 모든 label 이 이미지에 나타나지 않기 때문에 joints_vis 를 이용해서 실제로 사용할 수 있는 keypoint 인지 나타냅니다. MPII 의 경우 1 (visible) / 0(non) 으로만 나누어지기 때문에 조금 더 쉽게 사용할 수 있습니다. coco 의 경우 2 / 1 / 0 으로 표현해서 occlusion 상황까지 label 화 되어 있습니다.\n",
    "\n",
    "joints 순서는 아래와 같은 순서로 배치되어 저장해 뒀습니다.\n",
    "\n",
    ">0 - 오른쪽 발목  \n",
    "1 - 오른쪽 무릎  \n",
    "2 - 오른쪽 엉덩이    \n",
    "3 - 왼쪽 엉덩이  \n",
    "4 - 왼쪽 무릎  \n",
    "5 - 왼쪽 발목  \n",
    "6 - 골반  \n",
    "7 - 가슴(흉부)  \n",
    "8 - 목  \n",
    "9 - 머리 위  \n",
    "10 - 오른쪽 손목  \n",
    "11 - 오른쪽 팔꿈치  \n",
    "12 - 오른쪽 어깨  \n",
    "13 - 왼쪽 어깨  \n",
    "14 - 왼쪽 팔꿈치  \n",
    "15 - 왼쪽 손목  \n",
    "\n",
    "scale과 center는 사람 몸의 크기와 중심점 입니다.scale은 200을 곱해야 온전한 크기가 됩니다. 추후에 전처리 과정에서 200을 곱해서 사용할 예정이에요.\n",
    "\n",
    "이제 json annotation 을 파싱하는 함수를 만들어 보겠습니다.\n",
    "\n",
    "> MPII 데이터셋에서 'scale'은 사람의 크기 비율을 나타내는데, 관절의 위치 및 사람의 신체 부분 크기를 상대적으로 표현하는 데 사용됩니다.\n",
    "일반적으로 관절의 좌표는 이미지의 크기에 따라 상대적으로 바뀌는 값이기 때문에 이를 보정하기 위해 scale 값을 제공하는 것이며, 이를 통해서 다른 크기의 이미지에서도 신체 부위의 위치를 일관성 있게 분석할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b14e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c471bb",
   "metadata": {},
   "source": [
    "한 번 parse_one_annotation()함수를 테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5653e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json) # json파일객체을 열어서\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8311a2",
   "metadata": {},
   "source": [
    "### TFRecord 파일 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210db425",
   "metadata": {},
   "source": [
    "이전까지는 tf.keras 의 ImageDataGenerator 를 이용해서 주로 학습 데이터를 읽었습니다. 하지만 실제 프로젝트에서는 튜토리얼 데이터셋보다 훨씬 큰 크기의 데이터를 다뤄야 합니다.\n",
    "\n",
    "학습을 많이 해볼수록 학습 속도에 관심을 가지게 되는데요. tensorflow 튜토리얼 문서에는 다음과 같은 표현으로 나타나 있습니다.\n",
    "\n",
    "unless you are using tf.data and reading data is still the bottleneck to training.\n",
    "\n",
    "일반적으로 학습 과정에서 gpu 의 연산 속도보다 HDD I/O 가 느리기 때문에 병목 현상이 발생하고 대단위 프로젝트 실험에서 효율성이 떨어지는 것을 관찰할 수 있습니다. (답답해요..)\n",
    "\n",
    "따라서 \"학습 데이터를 어떻게 빠르게 읽는가?\" 에 대한 고민을 반드시 수행하셔야 더 많은 실험을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b589c",
   "metadata": {},
   "source": [
    "* 학습 속도를 향상시키기 위해서 데이터 관점에서 고려해야하는 단계는 어떤 단계인가요? 속도 향상을 위한 처리 방법을 위 링크에서 찾아대답해 봅시다.\n",
    "\n",
    "> data read(또는 prefetch) 또는 데이터 변환 단계. gpu 학습과 병렬적으로 수행되도록 prefetch를 적용해야 함. 수행방법은 tf.data의 map 함수를 이용하고 cache 에 저장해두는 방법을 사용해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf5343",
   "metadata": {},
   "source": [
    "내용이 꽤 어렵습니다만 tf 에서는 위 변환을 자동화해주는 도구를 제공합니다. 데이터셋을 TFRecord 형태로 표현하는 것인데요. TFRecord 는 binary record sequence 를 저장하기 위한 형식입니다.\n",
    "\n",
    "내부적으로 protocol buffer 라는 것을 이용합니다.\n",
    "\n",
    "protocol buffer 는 크로스 플랫폼에서 사용할 수 있는 직렬화 데이터 라이브러리라고 생각하시면 됩니다.\n",
    "\n",
    "데이터를 직렬화 한다는 것은 의미가 있는 정보만 추출해 나열하는 것을 뜻합니다. 예를 들어 \"서울 강동구에 사는 32살 고길동\"님을 나타내기 위해 \"서울강동32고길동\"과 같이 쓴다면 데이터만 담을 수 있겠죠? 다만 데이터 형식을 고정해야 하기 때문에 첫 두 글자는 시를 뜻하고, 다음 두 글자는 구를 뜻하고 다음 두 자리 숫자가 오고 이름은 세 글자여야 하는 제한이 생깁니다. 100살이 넘는 사람이나 이름이 세 글자가 아닌 사람을 담을 수 없어요. 그래서 매우 정형화된 데이터는 직렬화 하면 데이터 크기와 처리 속도 측면에서 유리하지만 사용하는데 제한이 있다고 기억하면 좋습니다. 어떤 데이터에서 어떤 방법으로 직렬화를 수행할지에 따라 발생되는 제한이 달라질 수도 있고요.\n",
    "\n",
    "이제 구현을 시작하겠습니다. 하나하나 천천히 가보죠. **앞서 추출한 annotation을 TFRecord로 변환하는 함수를 만들게요. TFRecord 는 tf.train.Example들의 합으로 이루어지므로 하나의 annotation을 하나의 tf.train.Example로 만들어 주는 함수부터 작성합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa78be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2404a",
   "metadata": {},
   "source": [
    "하나의 annotation이 tf.train.Example이 되었다면 이제 여러 annotation에 대해 작업할 수 있도록 함수를 만들어야 합니다.\n",
    "\n",
    "그런데 **여기서 하나의 TFRecord를 만들지 않고 여러 TFRecord를 만들어 볼 거예요. 우선 얼마나 많은 TFRecord를 만들지 결정할 함수를 만들게요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c459a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2454770",
   "metadata": {},
   "source": [
    "이 함수는 전체 데이터를 몇 개의 그룹으로 나눌지 결정해 줍니다. 전체 데이터 l을 n그룹으로 나눕니다. 결과적으로 n개의 TFRecord 파일을 만들겠다는 이야기입니다.\n",
    "\n",
    "여기서 좀 더 전문적으로는 n개로 shard 했다고 말합니다. 기업 단위의 데이터는 매우 크고 여러 장비에 나누어져 있기 때문에 sharding은 자주 이루어 집니다. 하나의 큰 데이터를 여러 개의 파일로 쪼개고 여러 컴퓨터에 나누어 담는 것이라고 여기면 됩니다. 이렇게 하면 데이터 저장에도 용이하고 병렬 처리에 하는데 이점이 있습니다.\n",
    "\n",
    "이 때, 이 데이터는 원래 하나의 큰 데이터였기 때문에 저장 후 어떻게 사용할 것인가에 따라서 sharding 전략이 달라지는데요. 그 내용을 다루기에는 너무 방대하니 여기서는 간단히만 다룹니다. 모델을 학습하는데 각 파일의 크기와 개수만 고려해보면, 너무 작은 파일로 많이 나누는 것도, 너무 큰 파일로 적게 나누는 것도 좋지 않습니다. 너무 작은 파일로 많이 나누면 학습 중간에 너무 잦은 입출력이 요구되고, 너무 큰 파일로 적게 나누면 입출력마다 걸리는 시간이 길어집니다. 입출력에 걸리는 시간이 GPU의 계산 시간보다 길어지면 그만큼 손해가 됩니다. 적절한 파일 크기와 개수는 케바케라고 할 수 있어요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc6658",
   "metadata": {},
   "source": [
    "설명은 어렵지만 실행해보면 단순합니다. 그럼 chunkify함수를 테스트 해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0582ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473a5ff",
   "metadata": {},
   "source": [
    "0이 1000개 들어 있는 리스트가 64개로 쪼개졌네요. chunkify 함수를 테스트 해봤으니 **하나의 chunk를 TFRecord로 만들어 줄 함수를 만듭시다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21b38178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad9036",
   "metadata": {},
   "source": [
    "chunk안에는 여러 annotation들이 있고, annotation들은 tf.train.Example로 변환된 후에 문자열로 직렬화되어 TFRecord에 담기는군요.\n",
    "\n",
    "또 한 가지 주의해서 봐야할 것은 함수 정의 위에 @ray.remote가 있다는 점이예요.\n",
    "\n",
    "RAY\n",
    "Ray는 병렬 처리를 위한 라이브러리인데요. 파이썬에서 기본적으로 제공하는 multiprocessing 패키지보다 편하게 다양한 환경에서 사용할 수 있습니다.\n",
    "\n",
    "이제 모든 준비가 되었으니 **전체 데이터를 적당한 수의 TFRecord 파일로 만들어주는 함수를 만듭시다.** ray를 사용하기 때문에 함수를 호출하는 문법이 약간 다르다는 점에 주의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54109598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e88036",
   "metadata": {},
   "source": [
    "### Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a481dad",
   "metadata": {},
   "source": [
    "앞서 작성한 함수를 사용해 데이터를 TFRecord로 만들어 줍니다. train 데이터는 64개로, val 데이터는 8개의 파일로 만듭니다. 시간이 꽤 걸립니다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "33226b46",
   "metadata": {},
   "source": [
    "# 필요 'ESC+Y' \n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc37510",
   "metadata": {},
   "source": [
    "함수나 클래스에 @ray.remote 데코레이터를 붙이고 some_function.remote()형식으로 함수를 만들어 냅니다. 클래스의 경우에는 메서드를 호출할 때 remote()를 이용하네요. 함수나 메서드는 이 시점에 실행되는 것이 아니라 생성만 됩니다. 그리고 ray.get()을 통해 실행이 되는 구조입니다. 함수를 바로 호출하는 것이 아니라 작업으로써 생성만 해놓고 나중에 실행한다는 점에 주의하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346c3aa",
   "metadata": {},
   "source": [
    "### data label 로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86024d",
   "metadata": {},
   "source": [
    "TFRecord로 저장된 데이터를 모델 학습에 필요한 데이터로 바꿔줄 함수가 필요합니다. tensorflow에서 이미 제공해주는 함수를 사용하면 되기 때문에 간단합니다. **주의할 점은 TFRecord가 직렬화된 데이터이기 때문에 만들 때 데이터 순서와 읽어올 때 데이터 순서가 같아야 한다는 점이에요. 데이터의 형식도 동일하게 맞춰 줘야 합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d72636f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad7d08",
   "metadata": {},
   "source": [
    "이렇게 얻은 image 와 label 을 이용해서 적절한 학습 형태로 변환합니다. 이미지를 그대로 사용하지 않고 적당히 정사각형으로 crop하여 사용합니다.\n",
    "\n",
    "우리가 알고 있는 것은 joints 의 위치, center 의 좌표, body height 값입니다. 균일하게 학습하기 위해 body width 를 적절히 정하는 것도 중요합니다. 이와 관련해서는 여러 방법이 있을 수 있겠지만 배우는 단계에서 더 중요하게 봐야 할 부분은 우리가 임의로 조정한 crop box 가 이미지 바깥으로 나가지 않는지 예외 처리를 잘 해주어야 한다는 점입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dbeaa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f6b4a",
   "metadata": {},
   "source": [
    "(x, y) 좌표로 되어있는 keypoint 를 heatmap 으로 변경시킵니다. 하나의 점에만 표시 되어있는 정보를 좌표 근처 여러 지점에 확률 분포 형태로 학습시키면 결과가 더 좋았던 점을 떠올려 주세요. 이런 확률 분표 형태의 정보를 heatmap이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1202b374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f393b5",
   "metadata": {},
   "source": [
    "**지금까지 만든 함수들을 개별 함수로도 만들 수 있지만 객체 형태로 조합해 볼게요.** 객체 형태로 만들면 선언부는 복잡해 보여도 훨씬 장점이 많습니다. 함수에서 객체의 메서드로 수정할 때는 self를 추가해야 하는 점을 잊지 마세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23d61b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5cbc6",
   "metadata": {},
   "source": [
    "### 모델을 학습해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429354e",
   "metadata": {},
   "source": [
    "![.hrglass](./hrglass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e2c04",
   "metadata": {},
   "source": [
    "#### Hourglass 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8f058",
   "metadata": {},
   "source": [
    "> **여기에서는 BottleneckBlock를 모델의 기본으로 사용하였습니다.**  \n",
    "주어진 코드에서는 BottleneckBlock을 사용하여 복잡한 연산을 효율적으로 수행하고 있습니다. 기본 Residual Block이 사용되지 않은 이유는 모델의 복잡성과 효율성을 높이기 위한 선택일 가능성이 큽니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11861c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e307b",
   "metadata": {},
   "source": [
    "다시 돌아와서 hourglass 모델을 잘 생각해 보면 마치 양파처럼 가장 바깥의 layer 를 제거하면 똑같은 구조가 나타나는 것을 알 수 있습니다. 이 점을 이용해서 간단하게 모델을 표현할 수 있는데요.\n",
    "\n",
    "바로 재귀 함수를 이용하는 것이겠죠! 바깥부터 5개의 양파껍질(층)을 만들고 싶다면 order 를 이용해서 5,4...1 이 될 때까지 HourglassModule 을 반복하면 order 가 1이 되면 BottleneckBlock 으로 대체해 주면 아주 간결하게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc25ceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2ec8d",
   "metadata": {},
   "source": [
    "여러 모듈을 쌓을수록 모델이 깊어지는 만큼 학습이 어려워, 저자들은 Intermediate supervision을 적용하였습니다.\n",
    "도식에서 보이는 모듈 사이의 네트워크의 파란 박스는 모델 중간에 계산되는 히트맵 결과를 출력하는 convolution layer입니다. 이 히트맵과 ground truth의 차이를 intermediate loss (auxilary loss) 로 계산합니다. 이로써 stacked hourglass module은 보다 정교한 결과를 도출해냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed06bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0275e",
   "metadata": {},
   "source": [
    "따라서 stacked 되는 hourglass 층 사이사이에 LinearLayer 를 삽입하고 중간 loss 를 계산해 줍니다.\n",
    "\n",
    "지금까지 만든 hourglass 를 여러 층으로 쌓으면 stacked hourglass 가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a6608a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809cff5",
   "metadata": {},
   "source": [
    "### 학습 엔진 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08687b2a",
   "metadata": {},
   "source": [
    "이제 모델 학습을 진행할 차례입니다.\n",
    "\n",
    "그런데 학습을 할 수 있는 GPU가 여러 개이고 데이터를 병렬로 학습시키려면 어떻게 해야할까요? 여러 GPU를 사용하기 위해서는 약간의 코드를 추가해줘야 합니다. 학습 환경은 GPU가 하나이지만 나중에 기업 환경에서 여러 GPU를 사용할 수 있으니 살짝 공부해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb4260d",
   "metadata": {},
   "source": [
    "가장 핵심 키워드는 **tf.distribute.MirroredStrategy**입니다. 한 컴퓨터에 GPU가 여러 개인 경우 사용할 수 있는 방법인데요. 여러 GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합합니다. 그런 후 모델의 가중치를 업데이트 하도록 하는 것이죠.\n",
    "\n",
    "각 GPU에서 계산한 Loss를 토대로 전체 Loss를 종합해주는 역할은 **strategy.reduce** 함수가 담당합니다.\n",
    "\n",
    "이번에도 각 함수를 별개로 만들지 않고 하나의 객체로 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7ef8a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "        \n",
    "        # Initialize history dictionary\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            \n",
    "            # Save the training and validation loss for each epoch\n",
    "            self.history['train_loss'].append(train_loss.numpy())\n",
    "            self.history['val_loss'].append(val_loss.numpy())\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "        \n",
    "        # Save the history to a file\n",
    "        with open('training_history.pkl', 'wb') as f:\n",
    "            pickle.dump(self.history, f)\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = BEST_MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss) # 수정\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a69b8",
   "metadata": {},
   "source": [
    "이제 데이터셋을 만드는 함수를 작성합니다. TFRecord 파일이 여러개이므로 tf.data.Dataset.list_files를 통해 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0b62c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords) # 여기서 사용\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cf837",
   "metadata": {},
   "source": [
    "이제 데이터셋과 모델, 훈련용 객체를 조립만하면 되겠군요. 하나의 함수로 만들어 줄 텐데요. 주의할 점은 with strategy.scope():부분이 반드시 필요하다는 점이에요.\n",
    "\n",
    "또 데이터셋도 experimental_distribute_dataset를 통해 연결해 줘야 한다는 것도 중요해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7899777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(BEST_MODEL_PATH): # 수정\n",
    "        os.makedirs(BEST_MODEL_PATH) # 수정\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c91cf5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.62562418 epoch total loss 2.62562418\n",
      "Trained batch 2 batch loss 2.4533639 epoch total loss 2.53949404\n",
      "Trained batch 3 batch loss 2.64274406 epoch total loss 2.57391071\n",
      "Trained batch 4 batch loss 2.53032923 epoch total loss 2.56301546\n",
      "Trained batch 5 batch loss 2.45593643 epoch total loss 2.54159975\n",
      "Trained batch 6 batch loss 2.40004778 epoch total loss 2.51800752\n",
      "Trained batch 7 batch loss 2.31933403 epoch total loss 2.48962569\n",
      "Trained batch 8 batch loss 2.09833241 epoch total loss 2.44071388\n",
      "Trained batch 9 batch loss 2.18560219 epoch total loss 2.41236806\n",
      "Trained batch 10 batch loss 2.22844219 epoch total loss 2.3939755\n",
      "Trained batch 11 batch loss 2.20921421 epoch total loss 2.37717891\n",
      "Trained batch 12 batch loss 2.12779522 epoch total loss 2.35639691\n",
      "Trained batch 13 batch loss 2.18800402 epoch total loss 2.34344363\n",
      "Trained batch 14 batch loss 1.96954644 epoch total loss 2.3167367\n",
      "Trained batch 15 batch loss 1.98075366 epoch total loss 2.29433799\n",
      "Trained batch 16 batch loss 1.93244684 epoch total loss 2.27171969\n",
      "Trained batch 17 batch loss 2.11425018 epoch total loss 2.26245689\n",
      "Trained batch 18 batch loss 2.13795733 epoch total loss 2.25554013\n",
      "Trained batch 19 batch loss 2.0146749 epoch total loss 2.24286318\n",
      "Trained batch 20 batch loss 2.06508327 epoch total loss 2.23397398\n",
      "Trained batch 21 batch loss 2.02237558 epoch total loss 2.22389793\n",
      "Trained batch 22 batch loss 1.97839367 epoch total loss 2.21273875\n",
      "Trained batch 23 batch loss 1.89276755 epoch total loss 2.19882703\n",
      "Trained batch 24 batch loss 1.91087985 epoch total loss 2.18682933\n",
      "Trained batch 25 batch loss 2.02484703 epoch total loss 2.18035\n",
      "Trained batch 26 batch loss 1.96432388 epoch total loss 2.17204142\n",
      "Trained batch 27 batch loss 1.87469244 epoch total loss 2.16102839\n",
      "Trained batch 28 batch loss 1.93118882 epoch total loss 2.15281987\n",
      "Trained batch 29 batch loss 1.81105542 epoch total loss 2.14103484\n",
      "Trained batch 30 batch loss 1.85769331 epoch total loss 2.13159013\n",
      "Trained batch 31 batch loss 1.78108704 epoch total loss 2.1202836\n",
      "Trained batch 32 batch loss 1.87821472 epoch total loss 2.11271882\n",
      "Trained batch 33 batch loss 1.94048595 epoch total loss 2.1074996\n",
      "Trained batch 34 batch loss 1.88035429 epoch total loss 2.10081887\n",
      "Trained batch 35 batch loss 1.9085 epoch total loss 2.09532404\n",
      "Trained batch 36 batch loss 1.90546441 epoch total loss 2.09005022\n",
      "Trained batch 37 batch loss 1.69679761 epoch total loss 2.07942176\n",
      "Trained batch 38 batch loss 1.86206937 epoch total loss 2.07370186\n",
      "Trained batch 39 batch loss 1.81416845 epoch total loss 2.06704736\n",
      "Trained batch 40 batch loss 1.88015223 epoch total loss 2.06237483\n",
      "Trained batch 41 batch loss 1.75582337 epoch total loss 2.05489802\n",
      "Trained batch 42 batch loss 1.74592948 epoch total loss 2.04754138\n",
      "Trained batch 43 batch loss 1.76350451 epoch total loss 2.04093599\n",
      "Trained batch 44 batch loss 1.83874595 epoch total loss 2.03634071\n",
      "Trained batch 45 batch loss 1.85000551 epoch total loss 2.03219986\n",
      "Trained batch 46 batch loss 1.85373831 epoch total loss 2.02832031\n",
      "Trained batch 47 batch loss 1.8213222 epoch total loss 2.02391601\n",
      "Trained batch 48 batch loss 1.86900246 epoch total loss 2.02068877\n",
      "Trained batch 49 batch loss 1.83459067 epoch total loss 2.01689076\n",
      "Trained batch 50 batch loss 1.72780228 epoch total loss 2.01110911\n",
      "Trained batch 51 batch loss 1.67144823 epoch total loss 2.00444889\n",
      "Trained batch 52 batch loss 1.63733172 epoch total loss 1.99738896\n",
      "Trained batch 53 batch loss 1.51863968 epoch total loss 1.98835599\n",
      "Trained batch 54 batch loss 1.64630389 epoch total loss 1.98202157\n",
      "Trained batch 55 batch loss 1.58444262 epoch total loss 1.97479284\n",
      "Trained batch 56 batch loss 1.41091073 epoch total loss 1.96472359\n",
      "Trained batch 57 batch loss 1.51161885 epoch total loss 1.95677435\n",
      "Trained batch 58 batch loss 1.46883976 epoch total loss 1.94836175\n",
      "Trained batch 59 batch loss 1.54532552 epoch total loss 1.9415307\n",
      "Trained batch 60 batch loss 1.61927676 epoch total loss 1.93615973\n",
      "Trained batch 61 batch loss 1.79856431 epoch total loss 1.93390405\n",
      "Trained batch 62 batch loss 1.81929839 epoch total loss 1.93205559\n",
      "Trained batch 63 batch loss 1.8264612 epoch total loss 1.93037951\n",
      "Trained batch 64 batch loss 1.84567451 epoch total loss 1.92905593\n",
      "Trained batch 65 batch loss 1.8234179 epoch total loss 1.92743075\n",
      "Trained batch 66 batch loss 1.82131481 epoch total loss 1.92582285\n",
      "Trained batch 67 batch loss 1.8033452 epoch total loss 1.92399478\n",
      "Trained batch 68 batch loss 1.79954255 epoch total loss 1.92216468\n",
      "Trained batch 69 batch loss 1.80245447 epoch total loss 1.92042983\n",
      "Trained batch 70 batch loss 1.80831611 epoch total loss 1.91882825\n",
      "Trained batch 71 batch loss 1.79853868 epoch total loss 1.91713405\n",
      "Trained batch 72 batch loss 1.81155026 epoch total loss 1.91566765\n",
      "Trained batch 73 batch loss 1.73025441 epoch total loss 1.91312778\n",
      "Trained batch 74 batch loss 1.78628159 epoch total loss 1.91141367\n",
      "Trained batch 75 batch loss 1.79504824 epoch total loss 1.90986204\n",
      "Trained batch 76 batch loss 1.84504104 epoch total loss 1.90900922\n",
      "Trained batch 77 batch loss 1.67939138 epoch total loss 1.90602732\n",
      "Trained batch 78 batch loss 1.71571708 epoch total loss 1.90358734\n",
      "Trained batch 79 batch loss 1.7762804 epoch total loss 1.90197575\n",
      "Trained batch 80 batch loss 1.78867567 epoch total loss 1.90055966\n",
      "Trained batch 81 batch loss 1.80119824 epoch total loss 1.89933288\n",
      "Trained batch 82 batch loss 1.68993914 epoch total loss 1.8967793\n",
      "Trained batch 83 batch loss 1.6859324 epoch total loss 1.89423895\n",
      "Trained batch 84 batch loss 1.68153119 epoch total loss 1.89170671\n",
      "Trained batch 85 batch loss 1.75924098 epoch total loss 1.8901484\n",
      "Trained batch 86 batch loss 1.78667378 epoch total loss 1.8889451\n",
      "Trained batch 87 batch loss 1.70375824 epoch total loss 1.88681662\n",
      "Trained batch 88 batch loss 1.76426017 epoch total loss 1.88542402\n",
      "Trained batch 89 batch loss 1.78811955 epoch total loss 1.88433063\n",
      "Trained batch 90 batch loss 1.74817634 epoch total loss 1.88281775\n",
      "Trained batch 91 batch loss 1.77535748 epoch total loss 1.88163686\n",
      "Trained batch 92 batch loss 1.76089895 epoch total loss 1.88032448\n",
      "Trained batch 93 batch loss 1.7141825 epoch total loss 1.87853801\n",
      "Trained batch 94 batch loss 1.72875631 epoch total loss 1.87694466\n",
      "Trained batch 95 batch loss 1.75715196 epoch total loss 1.87568378\n",
      "Trained batch 96 batch loss 1.74026096 epoch total loss 1.87427318\n",
      "Trained batch 97 batch loss 1.75055182 epoch total loss 1.87299764\n",
      "Trained batch 98 batch loss 1.64691925 epoch total loss 1.87069082\n",
      "Trained batch 99 batch loss 1.56378615 epoch total loss 1.86759067\n",
      "Trained batch 100 batch loss 1.7000227 epoch total loss 1.86591506\n",
      "Trained batch 101 batch loss 1.7388835 epoch total loss 1.86465728\n",
      "Trained batch 102 batch loss 1.78268278 epoch total loss 1.86385357\n",
      "Trained batch 103 batch loss 1.71600628 epoch total loss 1.86241817\n",
      "Trained batch 104 batch loss 1.67689621 epoch total loss 1.86063433\n",
      "Trained batch 105 batch loss 1.74942088 epoch total loss 1.85957515\n",
      "Trained batch 106 batch loss 1.73465824 epoch total loss 1.85839677\n",
      "Trained batch 107 batch loss 1.71721733 epoch total loss 1.85707736\n",
      "Trained batch 108 batch loss 1.74416721 epoch total loss 1.85603189\n",
      "Trained batch 109 batch loss 1.75048101 epoch total loss 1.85506368\n",
      "Trained batch 110 batch loss 1.6624223 epoch total loss 1.85331237\n",
      "Trained batch 111 batch loss 1.68489611 epoch total loss 1.85179508\n",
      "Trained batch 112 batch loss 1.66818154 epoch total loss 1.85015571\n",
      "Trained batch 113 batch loss 1.75103545 epoch total loss 1.84927857\n",
      "Trained batch 114 batch loss 1.6840117 epoch total loss 1.84782875\n",
      "Trained batch 115 batch loss 1.72692895 epoch total loss 1.84677744\n",
      "Trained batch 116 batch loss 1.74057448 epoch total loss 1.84586191\n",
      "Trained batch 117 batch loss 1.70473671 epoch total loss 1.84465575\n",
      "Trained batch 118 batch loss 1.70807326 epoch total loss 1.84349823\n",
      "Trained batch 119 batch loss 1.66676128 epoch total loss 1.84201312\n",
      "Trained batch 120 batch loss 1.71117079 epoch total loss 1.84092271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 121 batch loss 1.6291995 epoch total loss 1.83917284\n",
      "Trained batch 122 batch loss 1.71674633 epoch total loss 1.83816946\n",
      "Trained batch 123 batch loss 1.76331663 epoch total loss 1.83756089\n",
      "Trained batch 124 batch loss 1.69552755 epoch total loss 1.83641541\n",
      "Trained batch 125 batch loss 1.82400203 epoch total loss 1.83631611\n",
      "Trained batch 126 batch loss 1.81040084 epoch total loss 1.83611047\n",
      "Trained batch 127 batch loss 1.8073808 epoch total loss 1.83588433\n",
      "Trained batch 128 batch loss 1.78659034 epoch total loss 1.83549917\n",
      "Trained batch 129 batch loss 1.75727344 epoch total loss 1.83489275\n",
      "Trained batch 130 batch loss 1.53699493 epoch total loss 1.83260119\n",
      "Trained batch 131 batch loss 1.45283544 epoch total loss 1.82970226\n",
      "Trained batch 132 batch loss 1.39119899 epoch total loss 1.82638025\n",
      "Trained batch 133 batch loss 1.49168491 epoch total loss 1.82386374\n",
      "Trained batch 134 batch loss 1.65148294 epoch total loss 1.82257736\n",
      "Trained batch 135 batch loss 1.7653265 epoch total loss 1.82215333\n",
      "Trained batch 136 batch loss 1.7283709 epoch total loss 1.8214637\n",
      "Trained batch 137 batch loss 1.59012008 epoch total loss 1.8197751\n",
      "Trained batch 138 batch loss 1.54443049 epoch total loss 1.8177799\n",
      "Trained batch 139 batch loss 1.72275448 epoch total loss 1.81709623\n",
      "Trained batch 140 batch loss 1.69671142 epoch total loss 1.81623638\n",
      "Trained batch 141 batch loss 1.77589297 epoch total loss 1.81595027\n",
      "Trained batch 142 batch loss 1.74745274 epoch total loss 1.81546795\n",
      "Trained batch 143 batch loss 1.76233113 epoch total loss 1.81509638\n",
      "Trained batch 144 batch loss 1.76159501 epoch total loss 1.8147248\n",
      "Trained batch 145 batch loss 1.70078194 epoch total loss 1.81393898\n",
      "Trained batch 146 batch loss 1.69054532 epoch total loss 1.81309378\n",
      "Trained batch 147 batch loss 1.62103271 epoch total loss 1.81178725\n",
      "Trained batch 148 batch loss 1.67785239 epoch total loss 1.81088233\n",
      "Trained batch 149 batch loss 1.72359467 epoch total loss 1.81029654\n",
      "Trained batch 150 batch loss 1.69927108 epoch total loss 1.80955648\n",
      "Trained batch 151 batch loss 1.50759649 epoch total loss 1.80755675\n",
      "Trained batch 152 batch loss 1.46509075 epoch total loss 1.80530369\n",
      "Trained batch 153 batch loss 1.2907033 epoch total loss 1.80194032\n",
      "Trained batch 154 batch loss 1.48010719 epoch total loss 1.79985046\n",
      "Trained batch 155 batch loss 1.55592263 epoch total loss 1.79827666\n",
      "Trained batch 156 batch loss 1.60460651 epoch total loss 1.79703522\n",
      "Trained batch 157 batch loss 1.67498934 epoch total loss 1.79625785\n",
      "Trained batch 158 batch loss 1.67257512 epoch total loss 1.79547501\n",
      "Trained batch 159 batch loss 1.71058846 epoch total loss 1.79494131\n",
      "Trained batch 160 batch loss 1.63155425 epoch total loss 1.79392016\n",
      "Trained batch 161 batch loss 1.63005531 epoch total loss 1.79290235\n",
      "Trained batch 162 batch loss 1.69569373 epoch total loss 1.79230225\n",
      "Trained batch 163 batch loss 1.57057393 epoch total loss 1.79094207\n",
      "Trained batch 164 batch loss 1.5774256 epoch total loss 1.78964007\n",
      "Trained batch 165 batch loss 1.65057254 epoch total loss 1.78879726\n",
      "Trained batch 166 batch loss 1.6324594 epoch total loss 1.78785539\n",
      "Trained batch 167 batch loss 1.60629785 epoch total loss 1.7867682\n",
      "Trained batch 168 batch loss 1.66301394 epoch total loss 1.7860316\n",
      "Trained batch 169 batch loss 1.68111718 epoch total loss 1.78541088\n",
      "Trained batch 170 batch loss 1.61102486 epoch total loss 1.78438509\n",
      "Trained batch 171 batch loss 1.6046207 epoch total loss 1.78333378\n",
      "Trained batch 172 batch loss 1.56387711 epoch total loss 1.78205788\n",
      "Trained batch 173 batch loss 1.5053643 epoch total loss 1.78045845\n",
      "Trained batch 174 batch loss 1.57582641 epoch total loss 1.77928245\n",
      "Trained batch 175 batch loss 1.6012826 epoch total loss 1.77826536\n",
      "Trained batch 176 batch loss 1.75217891 epoch total loss 1.77811706\n",
      "Trained batch 177 batch loss 1.83730793 epoch total loss 1.77845156\n",
      "Trained batch 178 batch loss 1.77177811 epoch total loss 1.77841413\n",
      "Trained batch 179 batch loss 1.74347377 epoch total loss 1.77821887\n",
      "Trained batch 180 batch loss 1.70013952 epoch total loss 1.77778506\n",
      "Trained batch 181 batch loss 1.75496256 epoch total loss 1.77765906\n",
      "Trained batch 182 batch loss 1.67147779 epoch total loss 1.77707565\n",
      "Trained batch 183 batch loss 1.35092616 epoch total loss 1.77474689\n",
      "Trained batch 184 batch loss 1.68234658 epoch total loss 1.77424467\n",
      "Trained batch 185 batch loss 1.7449764 epoch total loss 1.77408648\n",
      "Trained batch 186 batch loss 1.71352267 epoch total loss 1.77376091\n",
      "Trained batch 187 batch loss 1.60960865 epoch total loss 1.77288318\n",
      "Trained batch 188 batch loss 1.5975548 epoch total loss 1.7719506\n",
      "Trained batch 189 batch loss 1.58363807 epoch total loss 1.77095425\n",
      "Trained batch 190 batch loss 1.54968929 epoch total loss 1.7697897\n",
      "Trained batch 191 batch loss 1.58977532 epoch total loss 1.76884723\n",
      "Trained batch 192 batch loss 1.64260721 epoch total loss 1.76818979\n",
      "Trained batch 193 batch loss 1.64425707 epoch total loss 1.76754761\n",
      "Trained batch 194 batch loss 1.6257031 epoch total loss 1.7668165\n",
      "Trained batch 195 batch loss 1.6709007 epoch total loss 1.76632452\n",
      "Trained batch 196 batch loss 1.67554307 epoch total loss 1.76586139\n",
      "Trained batch 197 batch loss 1.68560779 epoch total loss 1.76545393\n",
      "Trained batch 198 batch loss 1.74307275 epoch total loss 1.76534092\n",
      "Trained batch 199 batch loss 1.67343211 epoch total loss 1.76487911\n",
      "Trained batch 200 batch loss 1.7043159 epoch total loss 1.76457632\n",
      "Trained batch 201 batch loss 1.71656203 epoch total loss 1.7643373\n",
      "Trained batch 202 batch loss 1.66007543 epoch total loss 1.76382113\n",
      "Trained batch 203 batch loss 1.59475696 epoch total loss 1.76298833\n",
      "Trained batch 204 batch loss 1.68816328 epoch total loss 1.76262152\n",
      "Trained batch 205 batch loss 1.71891499 epoch total loss 1.76240826\n",
      "Trained batch 206 batch loss 1.7421056 epoch total loss 1.76230967\n",
      "Trained batch 207 batch loss 1.67508268 epoch total loss 1.76188827\n",
      "Trained batch 208 batch loss 1.71675968 epoch total loss 1.7616713\n",
      "Trained batch 209 batch loss 1.7097615 epoch total loss 1.76142287\n",
      "Trained batch 210 batch loss 1.70146167 epoch total loss 1.76113737\n",
      "Trained batch 211 batch loss 1.62400496 epoch total loss 1.76048732\n",
      "Trained batch 212 batch loss 1.66227508 epoch total loss 1.76002407\n",
      "Trained batch 213 batch loss 1.62425017 epoch total loss 1.75938654\n",
      "Trained batch 214 batch loss 1.75161171 epoch total loss 1.75935018\n",
      "Trained batch 215 batch loss 1.57202458 epoch total loss 1.75847888\n",
      "Trained batch 216 batch loss 1.46456993 epoch total loss 1.75711823\n",
      "Trained batch 217 batch loss 1.51152396 epoch total loss 1.75598645\n",
      "Trained batch 218 batch loss 1.53777409 epoch total loss 1.75498557\n",
      "Trained batch 219 batch loss 1.56371009 epoch total loss 1.75411224\n",
      "Trained batch 220 batch loss 1.63505852 epoch total loss 1.75357115\n",
      "Trained batch 221 batch loss 1.72002077 epoch total loss 1.7534194\n",
      "Trained batch 222 batch loss 1.72944784 epoch total loss 1.7533114\n",
      "Trained batch 223 batch loss 1.76767266 epoch total loss 1.75337577\n",
      "Trained batch 224 batch loss 1.6300602 epoch total loss 1.75282538\n",
      "Trained batch 225 batch loss 1.47870314 epoch total loss 1.75160694\n",
      "Trained batch 226 batch loss 1.60716474 epoch total loss 1.75096798\n",
      "Trained batch 227 batch loss 1.58721316 epoch total loss 1.75024652\n",
      "Trained batch 228 batch loss 1.61413646 epoch total loss 1.74964952\n",
      "Trained batch 229 batch loss 1.70301485 epoch total loss 1.74944592\n",
      "Trained batch 230 batch loss 1.61770058 epoch total loss 1.74887311\n",
      "Trained batch 231 batch loss 1.66042387 epoch total loss 1.74849021\n",
      "Trained batch 232 batch loss 1.67572856 epoch total loss 1.74817657\n",
      "Trained batch 233 batch loss 1.67713785 epoch total loss 1.74787164\n",
      "Trained batch 234 batch loss 1.64515507 epoch total loss 1.74743259\n",
      "Trained batch 235 batch loss 1.70215178 epoch total loss 1.74724\n",
      "Trained batch 236 batch loss 1.67873049 epoch total loss 1.74694967\n",
      "Trained batch 237 batch loss 1.6575228 epoch total loss 1.74657238\n",
      "Trained batch 238 batch loss 1.62164187 epoch total loss 1.7460475\n",
      "Trained batch 239 batch loss 1.67058492 epoch total loss 1.74573171\n",
      "Trained batch 240 batch loss 1.69575417 epoch total loss 1.74552345\n",
      "Trained batch 241 batch loss 1.65719628 epoch total loss 1.745157\n",
      "Trained batch 242 batch loss 1.51420093 epoch total loss 1.74420249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 243 batch loss 1.63217115 epoch total loss 1.74374151\n",
      "Trained batch 244 batch loss 1.66107106 epoch total loss 1.74340272\n",
      "Trained batch 245 batch loss 1.65116692 epoch total loss 1.74302614\n",
      "Trained batch 246 batch loss 1.63123441 epoch total loss 1.74257171\n",
      "Trained batch 247 batch loss 1.59750795 epoch total loss 1.74198437\n",
      "Trained batch 248 batch loss 1.51177919 epoch total loss 1.74105608\n",
      "Trained batch 249 batch loss 1.67358756 epoch total loss 1.74078512\n",
      "Trained batch 250 batch loss 1.64724791 epoch total loss 1.74041104\n",
      "Trained batch 251 batch loss 1.67537522 epoch total loss 1.74015188\n",
      "Trained batch 252 batch loss 1.73083508 epoch total loss 1.74011493\n",
      "Trained batch 253 batch loss 1.75179577 epoch total loss 1.74016118\n",
      "Trained batch 254 batch loss 1.62117612 epoch total loss 1.73969281\n",
      "Trained batch 255 batch loss 1.6252234 epoch total loss 1.73924387\n",
      "Trained batch 256 batch loss 1.34978986 epoch total loss 1.73772252\n",
      "Trained batch 257 batch loss 1.65072894 epoch total loss 1.73738396\n",
      "Trained batch 258 batch loss 1.63014245 epoch total loss 1.7369684\n",
      "Trained batch 259 batch loss 1.64804554 epoch total loss 1.73662508\n",
      "Trained batch 260 batch loss 1.71893179 epoch total loss 1.73655701\n",
      "Trained batch 261 batch loss 1.69380617 epoch total loss 1.73639321\n",
      "Trained batch 262 batch loss 1.59097445 epoch total loss 1.73583817\n",
      "Trained batch 263 batch loss 1.50053585 epoch total loss 1.73494363\n",
      "Trained batch 264 batch loss 1.39990664 epoch total loss 1.73367453\n",
      "Trained batch 265 batch loss 1.51095581 epoch total loss 1.73283398\n",
      "Trained batch 266 batch loss 1.61383569 epoch total loss 1.73238671\n",
      "Trained batch 267 batch loss 1.61704552 epoch total loss 1.73195457\n",
      "Trained batch 268 batch loss 1.57274103 epoch total loss 1.73136055\n",
      "Trained batch 269 batch loss 1.6136682 epoch total loss 1.73092306\n",
      "Trained batch 270 batch loss 1.62454891 epoch total loss 1.73052907\n",
      "Trained batch 271 batch loss 1.65901208 epoch total loss 1.73026526\n",
      "Trained batch 272 batch loss 1.52317822 epoch total loss 1.72950399\n",
      "Trained batch 273 batch loss 1.47382164 epoch total loss 1.72856736\n",
      "Trained batch 274 batch loss 1.64430857 epoch total loss 1.72825992\n",
      "Trained batch 275 batch loss 1.5943197 epoch total loss 1.72777283\n",
      "Trained batch 276 batch loss 1.60303581 epoch total loss 1.72732091\n",
      "Trained batch 277 batch loss 1.61333203 epoch total loss 1.7269094\n",
      "Trained batch 278 batch loss 1.64278722 epoch total loss 1.72660685\n",
      "Trained batch 279 batch loss 1.62957823 epoch total loss 1.72625911\n",
      "Trained batch 280 batch loss 1.64356875 epoch total loss 1.72596371\n",
      "Trained batch 281 batch loss 1.66467786 epoch total loss 1.72574556\n",
      "Trained batch 282 batch loss 1.66617811 epoch total loss 1.72553432\n",
      "Trained batch 283 batch loss 1.63659716 epoch total loss 1.72522008\n",
      "Trained batch 284 batch loss 1.65185833 epoch total loss 1.72496176\n",
      "Trained batch 285 batch loss 1.65988231 epoch total loss 1.72473335\n",
      "Trained batch 286 batch loss 1.63509178 epoch total loss 1.72442\n",
      "Trained batch 287 batch loss 1.59737706 epoch total loss 1.72397733\n",
      "Trained batch 288 batch loss 1.65898287 epoch total loss 1.72375166\n",
      "Trained batch 289 batch loss 1.61661613 epoch total loss 1.72338092\n",
      "Trained batch 290 batch loss 1.56554759 epoch total loss 1.72283673\n",
      "Trained batch 291 batch loss 1.6268245 epoch total loss 1.72250676\n",
      "Trained batch 292 batch loss 1.55190778 epoch total loss 1.72192252\n",
      "Trained batch 293 batch loss 1.55151939 epoch total loss 1.72134101\n",
      "Trained batch 294 batch loss 1.63510478 epoch total loss 1.72104764\n",
      "Trained batch 295 batch loss 1.51713192 epoch total loss 1.72035635\n",
      "Trained batch 296 batch loss 1.59824145 epoch total loss 1.71994376\n",
      "Trained batch 297 batch loss 1.59433043 epoch total loss 1.71952081\n",
      "Trained batch 298 batch loss 1.5416832 epoch total loss 1.71892405\n",
      "Trained batch 299 batch loss 1.48138022 epoch total loss 1.71812963\n",
      "Trained batch 300 batch loss 1.57409608 epoch total loss 1.71764958\n",
      "Trained batch 301 batch loss 1.52516162 epoch total loss 1.71701\n",
      "Trained batch 302 batch loss 1.63196969 epoch total loss 1.71672833\n",
      "Trained batch 303 batch loss 1.5887754 epoch total loss 1.71630597\n",
      "Trained batch 304 batch loss 1.57581949 epoch total loss 1.7158438\n",
      "Trained batch 305 batch loss 1.5325197 epoch total loss 1.71524274\n",
      "Trained batch 306 batch loss 1.59041035 epoch total loss 1.71483481\n",
      "Trained batch 307 batch loss 1.56670535 epoch total loss 1.71435225\n",
      "Trained batch 308 batch loss 1.63965714 epoch total loss 1.71410978\n",
      "Trained batch 309 batch loss 1.57132304 epoch total loss 1.71364772\n",
      "Trained batch 310 batch loss 1.56882739 epoch total loss 1.71318066\n",
      "Trained batch 311 batch loss 1.50643361 epoch total loss 1.71251583\n",
      "Trained batch 312 batch loss 1.55819619 epoch total loss 1.71202111\n",
      "Trained batch 313 batch loss 1.51421738 epoch total loss 1.71138906\n",
      "Trained batch 314 batch loss 1.51744485 epoch total loss 1.71077144\n",
      "Trained batch 315 batch loss 1.54716945 epoch total loss 1.71025217\n",
      "Trained batch 316 batch loss 1.50881243 epoch total loss 1.70961463\n",
      "Trained batch 317 batch loss 1.46871984 epoch total loss 1.70885479\n",
      "Trained batch 318 batch loss 1.44558656 epoch total loss 1.70802677\n",
      "Trained batch 319 batch loss 1.52954435 epoch total loss 1.70746732\n",
      "Trained batch 320 batch loss 1.57347322 epoch total loss 1.70704865\n",
      "Trained batch 321 batch loss 1.47170687 epoch total loss 1.7063154\n",
      "Trained batch 322 batch loss 1.61563945 epoch total loss 1.70603383\n",
      "Trained batch 323 batch loss 1.64183354 epoch total loss 1.7058351\n",
      "Trained batch 324 batch loss 1.61806273 epoch total loss 1.70556414\n",
      "Trained batch 325 batch loss 1.7187711 epoch total loss 1.70560467\n",
      "Trained batch 326 batch loss 1.70521 epoch total loss 1.70560348\n",
      "Trained batch 327 batch loss 1.66207886 epoch total loss 1.70547044\n",
      "Trained batch 328 batch loss 1.66015804 epoch total loss 1.70533228\n",
      "Trained batch 329 batch loss 1.62050378 epoch total loss 1.70507443\n",
      "Trained batch 330 batch loss 1.70973742 epoch total loss 1.7050885\n",
      "Trained batch 331 batch loss 1.627092 epoch total loss 1.70485282\n",
      "Trained batch 332 batch loss 1.54603386 epoch total loss 1.70437443\n",
      "Trained batch 333 batch loss 1.59259427 epoch total loss 1.70403874\n",
      "Trained batch 334 batch loss 1.6059649 epoch total loss 1.70374501\n",
      "Trained batch 335 batch loss 1.57438493 epoch total loss 1.70335889\n",
      "Trained batch 336 batch loss 1.58656192 epoch total loss 1.70301127\n",
      "Trained batch 337 batch loss 1.65924585 epoch total loss 1.70288146\n",
      "Trained batch 338 batch loss 1.67137027 epoch total loss 1.70278823\n",
      "Trained batch 339 batch loss 1.57872808 epoch total loss 1.70242226\n",
      "Trained batch 340 batch loss 1.65064681 epoch total loss 1.70226991\n",
      "Trained batch 341 batch loss 1.649086 epoch total loss 1.70211411\n",
      "Trained batch 342 batch loss 1.6713264 epoch total loss 1.7020241\n",
      "Trained batch 343 batch loss 1.63411808 epoch total loss 1.70182598\n",
      "Trained batch 344 batch loss 1.45883155 epoch total loss 1.70111954\n",
      "Trained batch 345 batch loss 1.50504017 epoch total loss 1.70055127\n",
      "Trained batch 346 batch loss 1.58734488 epoch total loss 1.70022404\n",
      "Trained batch 347 batch loss 1.61174762 epoch total loss 1.69996917\n",
      "Trained batch 348 batch loss 1.59778714 epoch total loss 1.69967544\n",
      "Trained batch 349 batch loss 1.55186605 epoch total loss 1.69925201\n",
      "Trained batch 350 batch loss 1.67223811 epoch total loss 1.69917476\n",
      "Trained batch 351 batch loss 1.52346635 epoch total loss 1.69867408\n",
      "Trained batch 352 batch loss 1.44726956 epoch total loss 1.6979599\n",
      "Trained batch 353 batch loss 1.53053713 epoch total loss 1.69748557\n",
      "Trained batch 354 batch loss 1.61604452 epoch total loss 1.69725549\n",
      "Trained batch 355 batch loss 1.63034391 epoch total loss 1.69706702\n",
      "Trained batch 356 batch loss 1.634583 epoch total loss 1.69689155\n",
      "Trained batch 357 batch loss 1.57311571 epoch total loss 1.69654489\n",
      "Trained batch 358 batch loss 1.52975082 epoch total loss 1.6960789\n",
      "Trained batch 359 batch loss 1.58143473 epoch total loss 1.69575942\n",
      "Trained batch 360 batch loss 1.50526297 epoch total loss 1.69523025\n",
      "Trained batch 361 batch loss 1.53450346 epoch total loss 1.694785\n",
      "Trained batch 362 batch loss 1.55737281 epoch total loss 1.69440544\n",
      "Trained batch 363 batch loss 1.5784241 epoch total loss 1.69408596\n",
      "Trained batch 364 batch loss 1.72135699 epoch total loss 1.69416094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 365 batch loss 1.68420696 epoch total loss 1.69413364\n",
      "Trained batch 366 batch loss 1.73616242 epoch total loss 1.69424844\n",
      "Trained batch 367 batch loss 1.71913147 epoch total loss 1.69431615\n",
      "Trained batch 368 batch loss 1.53994548 epoch total loss 1.69389653\n",
      "Trained batch 369 batch loss 1.53278279 epoch total loss 1.69346\n",
      "Trained batch 370 batch loss 1.56211364 epoch total loss 1.69310498\n",
      "Trained batch 371 batch loss 1.58942425 epoch total loss 1.69282556\n",
      "Trained batch 372 batch loss 1.6277889 epoch total loss 1.69265079\n",
      "Trained batch 373 batch loss 1.54341102 epoch total loss 1.69225061\n",
      "Trained batch 374 batch loss 1.62637913 epoch total loss 1.69207454\n",
      "Trained batch 375 batch loss 1.62569165 epoch total loss 1.69189751\n",
      "Trained batch 376 batch loss 1.60896277 epoch total loss 1.69167686\n",
      "Trained batch 377 batch loss 1.6186223 epoch total loss 1.69148314\n",
      "Trained batch 378 batch loss 1.64882159 epoch total loss 1.69137025\n",
      "Trained batch 379 batch loss 1.63166404 epoch total loss 1.69121265\n",
      "Trained batch 380 batch loss 1.5156976 epoch total loss 1.69075072\n",
      "Trained batch 381 batch loss 1.49660301 epoch total loss 1.6902411\n",
      "Trained batch 382 batch loss 1.39328659 epoch total loss 1.68946385\n",
      "Trained batch 383 batch loss 1.44211257 epoch total loss 1.6888181\n",
      "Trained batch 384 batch loss 1.59248483 epoch total loss 1.68856716\n",
      "Trained batch 385 batch loss 1.51655447 epoch total loss 1.68812037\n",
      "Trained batch 386 batch loss 1.49146318 epoch total loss 1.68761086\n",
      "Trained batch 387 batch loss 1.51087928 epoch total loss 1.68715417\n",
      "Trained batch 388 batch loss 1.48512387 epoch total loss 1.68663335\n",
      "Trained batch 389 batch loss 1.53402305 epoch total loss 1.68624103\n",
      "Trained batch 390 batch loss 1.41785133 epoch total loss 1.68555284\n",
      "Trained batch 391 batch loss 1.53646278 epoch total loss 1.68517148\n",
      "Trained batch 392 batch loss 1.48127782 epoch total loss 1.68465126\n",
      "Trained batch 393 batch loss 1.63420653 epoch total loss 1.68452299\n",
      "Trained batch 394 batch loss 1.5336417 epoch total loss 1.68414\n",
      "Trained batch 395 batch loss 1.64761674 epoch total loss 1.68404758\n",
      "Trained batch 396 batch loss 1.61457825 epoch total loss 1.6838721\n",
      "Trained batch 397 batch loss 1.64438677 epoch total loss 1.68377268\n",
      "Trained batch 398 batch loss 1.62161815 epoch total loss 1.68361664\n",
      "Trained batch 399 batch loss 1.62364924 epoch total loss 1.68346632\n",
      "Trained batch 400 batch loss 1.60045254 epoch total loss 1.68325877\n",
      "Trained batch 401 batch loss 1.64759195 epoch total loss 1.68316984\n",
      "Trained batch 402 batch loss 1.52245951 epoch total loss 1.68277013\n",
      "Trained batch 403 batch loss 1.48521745 epoch total loss 1.68228\n",
      "Trained batch 404 batch loss 1.41055632 epoch total loss 1.68160737\n",
      "Trained batch 405 batch loss 1.52388835 epoch total loss 1.68121791\n",
      "Trained batch 406 batch loss 1.53417587 epoch total loss 1.68085575\n",
      "Trained batch 407 batch loss 1.66591036 epoch total loss 1.68081892\n",
      "Trained batch 408 batch loss 1.55924523 epoch total loss 1.68052101\n",
      "Trained batch 409 batch loss 1.58163095 epoch total loss 1.68027925\n",
      "Trained batch 410 batch loss 1.42265069 epoch total loss 1.6796509\n",
      "Trained batch 411 batch loss 1.47390258 epoch total loss 1.67915022\n",
      "Trained batch 412 batch loss 1.49241495 epoch total loss 1.67869699\n",
      "Trained batch 413 batch loss 1.59943902 epoch total loss 1.67850506\n",
      "Trained batch 414 batch loss 1.67361069 epoch total loss 1.67849314\n",
      "Trained batch 415 batch loss 1.54622102 epoch total loss 1.67817438\n",
      "Trained batch 416 batch loss 1.59643877 epoch total loss 1.67797792\n",
      "Trained batch 417 batch loss 1.64616323 epoch total loss 1.67790163\n",
      "Trained batch 418 batch loss 1.59155917 epoch total loss 1.67769516\n",
      "Trained batch 419 batch loss 1.62079585 epoch total loss 1.67755926\n",
      "Trained batch 420 batch loss 1.5775398 epoch total loss 1.67732108\n",
      "Trained batch 421 batch loss 1.6483345 epoch total loss 1.67725217\n",
      "Trained batch 422 batch loss 1.60367393 epoch total loss 1.67707789\n",
      "Trained batch 423 batch loss 1.49397957 epoch total loss 1.67664492\n",
      "Trained batch 424 batch loss 1.58993173 epoch total loss 1.67644036\n",
      "Trained batch 425 batch loss 1.59471107 epoch total loss 1.67624807\n",
      "Trained batch 426 batch loss 1.52295518 epoch total loss 1.6758883\n",
      "Trained batch 427 batch loss 1.63023567 epoch total loss 1.67578137\n",
      "Trained batch 428 batch loss 1.63666022 epoch total loss 1.67568994\n",
      "Trained batch 429 batch loss 1.58359182 epoch total loss 1.67547536\n",
      "Trained batch 430 batch loss 1.48578286 epoch total loss 1.67503417\n",
      "Trained batch 431 batch loss 1.62239456 epoch total loss 1.67491198\n",
      "Trained batch 432 batch loss 1.63146877 epoch total loss 1.67481148\n",
      "Trained batch 433 batch loss 1.60168815 epoch total loss 1.67464256\n",
      "Trained batch 434 batch loss 1.48852956 epoch total loss 1.67421377\n",
      "Trained batch 435 batch loss 1.60122085 epoch total loss 1.67404592\n",
      "Trained batch 436 batch loss 1.55864787 epoch total loss 1.67378128\n",
      "Trained batch 437 batch loss 1.59763384 epoch total loss 1.67360699\n",
      "Trained batch 438 batch loss 1.55060065 epoch total loss 1.67332613\n",
      "Trained batch 439 batch loss 1.49670184 epoch total loss 1.6729238\n",
      "Trained batch 440 batch loss 1.56844103 epoch total loss 1.67268634\n",
      "Trained batch 441 batch loss 1.49765682 epoch total loss 1.67228949\n",
      "Trained batch 442 batch loss 1.5202769 epoch total loss 1.67194557\n",
      "Trained batch 443 batch loss 1.54930186 epoch total loss 1.67166877\n",
      "Trained batch 444 batch loss 1.4613831 epoch total loss 1.67119515\n",
      "Trained batch 445 batch loss 1.5549984 epoch total loss 1.67093396\n",
      "Trained batch 446 batch loss 1.62913537 epoch total loss 1.67084026\n",
      "Trained batch 447 batch loss 1.51557577 epoch total loss 1.67049289\n",
      "Trained batch 448 batch loss 1.57763505 epoch total loss 1.67028558\n",
      "Trained batch 449 batch loss 1.54774225 epoch total loss 1.67001271\n",
      "Trained batch 450 batch loss 1.48902059 epoch total loss 1.6696105\n",
      "Trained batch 451 batch loss 1.57522464 epoch total loss 1.66940117\n",
      "Trained batch 452 batch loss 1.51266 epoch total loss 1.66905427\n",
      "Trained batch 453 batch loss 1.56644094 epoch total loss 1.66882777\n",
      "Trained batch 454 batch loss 1.59188473 epoch total loss 1.66865826\n",
      "Trained batch 455 batch loss 1.57897782 epoch total loss 1.6684612\n",
      "Trained batch 456 batch loss 1.53157961 epoch total loss 1.66816092\n",
      "Trained batch 457 batch loss 1.61311543 epoch total loss 1.66804051\n",
      "Trained batch 458 batch loss 1.61363137 epoch total loss 1.66792166\n",
      "Trained batch 459 batch loss 1.44526911 epoch total loss 1.6674366\n",
      "Trained batch 460 batch loss 1.5965178 epoch total loss 1.66728234\n",
      "Trained batch 461 batch loss 1.59447467 epoch total loss 1.66712451\n",
      "Trained batch 462 batch loss 1.58952653 epoch total loss 1.66695654\n",
      "Trained batch 463 batch loss 1.54490769 epoch total loss 1.66669297\n",
      "Trained batch 464 batch loss 1.49240291 epoch total loss 1.66631734\n",
      "Trained batch 465 batch loss 1.66481233 epoch total loss 1.66631413\n",
      "Trained batch 466 batch loss 1.63005209 epoch total loss 1.66623628\n",
      "Trained batch 467 batch loss 1.71126819 epoch total loss 1.66633272\n",
      "Trained batch 468 batch loss 1.66110992 epoch total loss 1.66632164\n",
      "Trained batch 469 batch loss 1.59429348 epoch total loss 1.66616797\n",
      "Trained batch 470 batch loss 1.64097083 epoch total loss 1.66611445\n",
      "Trained batch 471 batch loss 1.59417033 epoch total loss 1.66596174\n",
      "Trained batch 472 batch loss 1.60955334 epoch total loss 1.66584218\n",
      "Trained batch 473 batch loss 1.64932382 epoch total loss 1.66580737\n",
      "Trained batch 474 batch loss 1.71345377 epoch total loss 1.66590786\n",
      "Trained batch 475 batch loss 1.65669298 epoch total loss 1.66588843\n",
      "Trained batch 476 batch loss 1.6101408 epoch total loss 1.66577137\n",
      "Trained batch 477 batch loss 1.5689677 epoch total loss 1.66556847\n",
      "Trained batch 478 batch loss 1.61492646 epoch total loss 1.66546249\n",
      "Trained batch 479 batch loss 1.57801509 epoch total loss 1.66527987\n",
      "Trained batch 480 batch loss 1.6420306 epoch total loss 1.66523147\n",
      "Trained batch 481 batch loss 1.6259737 epoch total loss 1.66514981\n",
      "Trained batch 482 batch loss 1.58468974 epoch total loss 1.66498291\n",
      "Trained batch 483 batch loss 1.62958503 epoch total loss 1.66490972\n",
      "Trained batch 484 batch loss 1.6326139 epoch total loss 1.66484296\n",
      "Trained batch 485 batch loss 1.63211703 epoch total loss 1.66477549\n",
      "Trained batch 486 batch loss 1.61532116 epoch total loss 1.66467369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 487 batch loss 1.60112095 epoch total loss 1.66454327\n",
      "Trained batch 488 batch loss 1.5718019 epoch total loss 1.66435313\n",
      "Trained batch 489 batch loss 1.57243776 epoch total loss 1.66416526\n",
      "Trained batch 490 batch loss 1.56512213 epoch total loss 1.66396308\n",
      "Trained batch 491 batch loss 1.63391292 epoch total loss 1.66390193\n",
      "Trained batch 492 batch loss 1.59104073 epoch total loss 1.66375387\n",
      "Trained batch 493 batch loss 1.59226894 epoch total loss 1.66360891\n",
      "Trained batch 494 batch loss 1.60287094 epoch total loss 1.66348588\n",
      "Trained batch 495 batch loss 1.60724437 epoch total loss 1.66337228\n",
      "Trained batch 496 batch loss 1.64867032 epoch total loss 1.6633426\n",
      "Trained batch 497 batch loss 1.58668613 epoch total loss 1.66318834\n",
      "Trained batch 498 batch loss 1.57414389 epoch total loss 1.66300952\n",
      "Trained batch 499 batch loss 1.50181186 epoch total loss 1.66268659\n",
      "Trained batch 500 batch loss 1.52557147 epoch total loss 1.66241241\n",
      "Trained batch 501 batch loss 1.53988063 epoch total loss 1.66216779\n",
      "Trained batch 502 batch loss 1.65596485 epoch total loss 1.66215539\n",
      "Trained batch 503 batch loss 1.4987185 epoch total loss 1.66183043\n",
      "Trained batch 504 batch loss 1.53440595 epoch total loss 1.66157758\n",
      "Trained batch 505 batch loss 1.59971189 epoch total loss 1.66145515\n",
      "Trained batch 506 batch loss 1.51560628 epoch total loss 1.66116691\n",
      "Trained batch 507 batch loss 1.65461481 epoch total loss 1.66115403\n",
      "Trained batch 508 batch loss 1.6527524 epoch total loss 1.66113746\n",
      "Trained batch 509 batch loss 1.63790703 epoch total loss 1.6610918\n",
      "Trained batch 510 batch loss 1.58374977 epoch total loss 1.66094017\n",
      "Trained batch 511 batch loss 1.55031264 epoch total loss 1.66072357\n",
      "Trained batch 512 batch loss 1.49823666 epoch total loss 1.66040623\n",
      "Trained batch 513 batch loss 1.51854 epoch total loss 1.66012967\n",
      "Trained batch 514 batch loss 1.62684453 epoch total loss 1.66006494\n",
      "Trained batch 515 batch loss 1.663715 epoch total loss 1.66007197\n",
      "Trained batch 516 batch loss 1.59957874 epoch total loss 1.65995467\n",
      "Trained batch 517 batch loss 1.51250494 epoch total loss 1.65966952\n",
      "Trained batch 518 batch loss 1.55708432 epoch total loss 1.65947139\n",
      "Trained batch 519 batch loss 1.50620747 epoch total loss 1.65917611\n",
      "Trained batch 520 batch loss 1.53940368 epoch total loss 1.65894592\n",
      "Trained batch 521 batch loss 1.5764792 epoch total loss 1.65878761\n",
      "Trained batch 522 batch loss 1.53736162 epoch total loss 1.65855491\n",
      "Trained batch 523 batch loss 1.43803549 epoch total loss 1.65813339\n",
      "Trained batch 524 batch loss 1.38619184 epoch total loss 1.65761435\n",
      "Trained batch 525 batch loss 1.47545242 epoch total loss 1.65726733\n",
      "Trained batch 526 batch loss 1.50795698 epoch total loss 1.65698349\n",
      "Trained batch 527 batch loss 1.4340328 epoch total loss 1.65656042\n",
      "Trained batch 528 batch loss 1.3968662 epoch total loss 1.65606856\n",
      "Trained batch 529 batch loss 1.53506982 epoch total loss 1.6558398\n",
      "Trained batch 530 batch loss 1.5927155 epoch total loss 1.65572071\n",
      "Trained batch 531 batch loss 1.49329495 epoch total loss 1.65541482\n",
      "Trained batch 532 batch loss 1.56071472 epoch total loss 1.65523684\n",
      "Trained batch 533 batch loss 1.52700448 epoch total loss 1.65499616\n",
      "Trained batch 534 batch loss 1.61583805 epoch total loss 1.65492284\n",
      "Trained batch 535 batch loss 1.48547554 epoch total loss 1.6546061\n",
      "Trained batch 536 batch loss 1.30272663 epoch total loss 1.65394974\n",
      "Trained batch 537 batch loss 1.15263391 epoch total loss 1.65301621\n",
      "Trained batch 538 batch loss 1.34862757 epoch total loss 1.65245044\n",
      "Trained batch 539 batch loss 1.66471756 epoch total loss 1.65247321\n",
      "Trained batch 540 batch loss 1.71539342 epoch total loss 1.65258968\n",
      "Trained batch 541 batch loss 1.6918453 epoch total loss 1.65266228\n",
      "Trained batch 542 batch loss 1.66016424 epoch total loss 1.65267611\n",
      "Trained batch 543 batch loss 1.64529598 epoch total loss 1.65266252\n",
      "Trained batch 544 batch loss 1.67332792 epoch total loss 1.65270054\n",
      "Trained batch 545 batch loss 1.58915544 epoch total loss 1.65258396\n",
      "Trained batch 546 batch loss 1.57596195 epoch total loss 1.65244365\n",
      "Trained batch 547 batch loss 1.62786603 epoch total loss 1.65239871\n",
      "Trained batch 548 batch loss 1.6196419 epoch total loss 1.65233898\n",
      "Trained batch 549 batch loss 1.56655931 epoch total loss 1.65218282\n",
      "Trained batch 550 batch loss 1.59783244 epoch total loss 1.65208399\n",
      "Trained batch 551 batch loss 1.53921986 epoch total loss 1.65187919\n",
      "Trained batch 552 batch loss 1.38900089 epoch total loss 1.65140295\n",
      "Trained batch 553 batch loss 1.39650416 epoch total loss 1.65094197\n",
      "Trained batch 554 batch loss 1.45667326 epoch total loss 1.65059125\n",
      "Trained batch 555 batch loss 1.41227353 epoch total loss 1.65016186\n",
      "Trained batch 556 batch loss 1.28826571 epoch total loss 1.64951098\n",
      "Trained batch 557 batch loss 1.29811859 epoch total loss 1.64888012\n",
      "Trained batch 558 batch loss 1.24304986 epoch total loss 1.64815283\n",
      "Trained batch 559 batch loss 1.43012118 epoch total loss 1.64776278\n",
      "Trained batch 560 batch loss 1.61651444 epoch total loss 1.64770699\n",
      "Trained batch 561 batch loss 1.67270291 epoch total loss 1.64775157\n",
      "Trained batch 562 batch loss 1.54050267 epoch total loss 1.64756072\n",
      "Trained batch 563 batch loss 1.51355886 epoch total loss 1.64732265\n",
      "Trained batch 564 batch loss 1.5772413 epoch total loss 1.64719856\n",
      "Trained batch 565 batch loss 1.59384131 epoch total loss 1.64710402\n",
      "Trained batch 566 batch loss 1.38636017 epoch total loss 1.64664328\n",
      "Trained batch 567 batch loss 1.34128547 epoch total loss 1.64610481\n",
      "Trained batch 568 batch loss 1.34147024 epoch total loss 1.64556849\n",
      "Trained batch 569 batch loss 1.37457108 epoch total loss 1.64509225\n",
      "Trained batch 570 batch loss 1.53406405 epoch total loss 1.64489746\n",
      "Trained batch 571 batch loss 1.56148243 epoch total loss 1.64475131\n",
      "Trained batch 572 batch loss 1.68441558 epoch total loss 1.64482069\n",
      "Trained batch 573 batch loss 1.61016846 epoch total loss 1.64476013\n",
      "Trained batch 574 batch loss 1.56199431 epoch total loss 1.64461601\n",
      "Trained batch 575 batch loss 1.59968245 epoch total loss 1.64453781\n",
      "Trained batch 576 batch loss 1.68433785 epoch total loss 1.64460695\n",
      "Trained batch 577 batch loss 1.64817739 epoch total loss 1.64461315\n",
      "Trained batch 578 batch loss 1.65464258 epoch total loss 1.64463055\n",
      "Trained batch 579 batch loss 1.56238151 epoch total loss 1.64448845\n",
      "Trained batch 580 batch loss 1.67910552 epoch total loss 1.64454806\n",
      "Trained batch 581 batch loss 1.68455172 epoch total loss 1.64461696\n",
      "Trained batch 582 batch loss 1.62455928 epoch total loss 1.64458251\n",
      "Trained batch 583 batch loss 1.52967227 epoch total loss 1.64438546\n",
      "Trained batch 584 batch loss 1.47221255 epoch total loss 1.64409065\n",
      "Trained batch 585 batch loss 1.43814445 epoch total loss 1.64373863\n",
      "Trained batch 586 batch loss 1.55997837 epoch total loss 1.6435957\n",
      "Trained batch 587 batch loss 1.498492 epoch total loss 1.64334846\n",
      "Trained batch 588 batch loss 1.47835279 epoch total loss 1.64306784\n",
      "Trained batch 589 batch loss 1.44293118 epoch total loss 1.64272809\n",
      "Trained batch 590 batch loss 1.51174796 epoch total loss 1.642506\n",
      "Trained batch 591 batch loss 1.48995543 epoch total loss 1.6422478\n",
      "Trained batch 592 batch loss 1.46200776 epoch total loss 1.64194345\n",
      "Trained batch 593 batch loss 1.49548566 epoch total loss 1.64169645\n",
      "Trained batch 594 batch loss 1.57940888 epoch total loss 1.64159155\n",
      "Trained batch 595 batch loss 1.3850652 epoch total loss 1.64116049\n",
      "Trained batch 596 batch loss 1.36388791 epoch total loss 1.64069521\n",
      "Trained batch 597 batch loss 1.37100303 epoch total loss 1.64024353\n",
      "Trained batch 598 batch loss 1.34793901 epoch total loss 1.63975477\n",
      "Trained batch 599 batch loss 1.41075242 epoch total loss 1.63937247\n",
      "Trained batch 600 batch loss 1.53095257 epoch total loss 1.63919175\n",
      "Trained batch 601 batch loss 1.5192492 epoch total loss 1.63899219\n",
      "Trained batch 602 batch loss 1.50966632 epoch total loss 1.63877738\n",
      "Trained batch 603 batch loss 1.60910583 epoch total loss 1.63872814\n",
      "Trained batch 604 batch loss 1.62064838 epoch total loss 1.63869822\n",
      "Trained batch 605 batch loss 1.60206962 epoch total loss 1.63863766\n",
      "Trained batch 606 batch loss 1.63944197 epoch total loss 1.63863909\n",
      "Trained batch 607 batch loss 1.62440515 epoch total loss 1.63861561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 608 batch loss 1.52150548 epoch total loss 1.63842297\n",
      "Trained batch 609 batch loss 1.47615325 epoch total loss 1.63815641\n",
      "Trained batch 610 batch loss 1.50167966 epoch total loss 1.63793278\n",
      "Trained batch 611 batch loss 1.65734541 epoch total loss 1.63796449\n",
      "Trained batch 612 batch loss 1.45683742 epoch total loss 1.63766861\n",
      "Trained batch 613 batch loss 1.39244831 epoch total loss 1.63726854\n",
      "Trained batch 614 batch loss 1.41096139 epoch total loss 1.6369\n",
      "Trained batch 615 batch loss 1.39249468 epoch total loss 1.63650262\n",
      "Trained batch 616 batch loss 1.41823208 epoch total loss 1.63614821\n",
      "Trained batch 617 batch loss 1.52652395 epoch total loss 1.63597059\n",
      "Trained batch 618 batch loss 1.603899 epoch total loss 1.63591862\n",
      "Trained batch 619 batch loss 1.59203076 epoch total loss 1.63584781\n",
      "Trained batch 620 batch loss 1.41383219 epoch total loss 1.6354897\n",
      "Trained batch 621 batch loss 1.42906713 epoch total loss 1.63515735\n",
      "Trained batch 622 batch loss 1.53522885 epoch total loss 1.63499665\n",
      "Trained batch 623 batch loss 1.56629729 epoch total loss 1.63488626\n",
      "Trained batch 624 batch loss 1.57501912 epoch total loss 1.63479042\n",
      "Trained batch 625 batch loss 1.51561463 epoch total loss 1.63459969\n",
      "Trained batch 626 batch loss 1.49929762 epoch total loss 1.63438356\n",
      "Trained batch 627 batch loss 1.47993386 epoch total loss 1.63413715\n",
      "Trained batch 628 batch loss 1.49499965 epoch total loss 1.63391566\n",
      "Trained batch 629 batch loss 1.56971622 epoch total loss 1.6338135\n",
      "Trained batch 630 batch loss 1.34561551 epoch total loss 1.63335598\n",
      "Trained batch 631 batch loss 1.63114345 epoch total loss 1.6333524\n",
      "Trained batch 632 batch loss 1.49882877 epoch total loss 1.63313949\n",
      "Trained batch 633 batch loss 1.53934836 epoch total loss 1.63299131\n",
      "Trained batch 634 batch loss 1.61089575 epoch total loss 1.63295639\n",
      "Trained batch 635 batch loss 1.61851 epoch total loss 1.63293362\n",
      "Trained batch 636 batch loss 1.41935706 epoch total loss 1.63259768\n",
      "Trained batch 637 batch loss 1.46967542 epoch total loss 1.6323421\n",
      "Trained batch 638 batch loss 1.55031824 epoch total loss 1.63221347\n",
      "Trained batch 639 batch loss 1.50659204 epoch total loss 1.6320169\n",
      "Trained batch 640 batch loss 1.503227 epoch total loss 1.63181555\n",
      "Trained batch 641 batch loss 1.60337901 epoch total loss 1.63177121\n",
      "Trained batch 642 batch loss 1.58526182 epoch total loss 1.63169861\n",
      "Trained batch 643 batch loss 1.60475111 epoch total loss 1.63165677\n",
      "Trained batch 644 batch loss 1.69008 epoch total loss 1.63174736\n",
      "Trained batch 645 batch loss 1.68100476 epoch total loss 1.63182378\n",
      "Trained batch 646 batch loss 1.65458989 epoch total loss 1.63185894\n",
      "Trained batch 647 batch loss 1.62533331 epoch total loss 1.63184893\n",
      "Trained batch 648 batch loss 1.51721084 epoch total loss 1.63167202\n",
      "Trained batch 649 batch loss 1.57777894 epoch total loss 1.63158894\n",
      "Trained batch 650 batch loss 1.57376599 epoch total loss 1.6315\n",
      "Trained batch 651 batch loss 1.50988138 epoch total loss 1.6313132\n",
      "Trained batch 652 batch loss 1.57912934 epoch total loss 1.6312331\n",
      "Trained batch 653 batch loss 1.50415111 epoch total loss 1.63103843\n",
      "Trained batch 654 batch loss 1.51030171 epoch total loss 1.63085377\n",
      "Trained batch 655 batch loss 1.56366599 epoch total loss 1.63075125\n",
      "Trained batch 656 batch loss 1.5983448 epoch total loss 1.6307019\n",
      "Trained batch 657 batch loss 1.60446477 epoch total loss 1.63066208\n",
      "Trained batch 658 batch loss 1.55927205 epoch total loss 1.6305536\n",
      "Trained batch 659 batch loss 1.52394903 epoch total loss 1.63039184\n",
      "Trained batch 660 batch loss 1.58712029 epoch total loss 1.63032639\n",
      "Trained batch 661 batch loss 1.64149249 epoch total loss 1.6303432\n",
      "Trained batch 662 batch loss 1.57372928 epoch total loss 1.63025773\n",
      "Trained batch 663 batch loss 1.52363896 epoch total loss 1.63009691\n",
      "Trained batch 664 batch loss 1.52463007 epoch total loss 1.62993813\n",
      "Trained batch 665 batch loss 1.51278389 epoch total loss 1.62976205\n",
      "Trained batch 666 batch loss 1.42405486 epoch total loss 1.62945318\n",
      "Trained batch 667 batch loss 1.49597788 epoch total loss 1.62925303\n",
      "Trained batch 668 batch loss 1.54092062 epoch total loss 1.62912083\n",
      "Trained batch 669 batch loss 1.5163188 epoch total loss 1.62895226\n",
      "Trained batch 670 batch loss 1.53332806 epoch total loss 1.62880945\n",
      "Trained batch 671 batch loss 1.4621942 epoch total loss 1.62856114\n",
      "Trained batch 672 batch loss 1.49827659 epoch total loss 1.6283673\n",
      "Trained batch 673 batch loss 1.52167511 epoch total loss 1.62820888\n",
      "Trained batch 674 batch loss 1.57866681 epoch total loss 1.6281352\n",
      "Trained batch 675 batch loss 1.58611465 epoch total loss 1.62807286\n",
      "Trained batch 676 batch loss 1.54281485 epoch total loss 1.62794685\n",
      "Trained batch 677 batch loss 1.46431518 epoch total loss 1.62770522\n",
      "Trained batch 678 batch loss 1.50746095 epoch total loss 1.62752783\n",
      "Trained batch 679 batch loss 1.51502109 epoch total loss 1.62736213\n",
      "Trained batch 680 batch loss 1.58816254 epoch total loss 1.62730443\n",
      "Trained batch 681 batch loss 1.60194266 epoch total loss 1.62726712\n",
      "Trained batch 682 batch loss 1.70659053 epoch total loss 1.62738347\n",
      "Trained batch 683 batch loss 1.68779325 epoch total loss 1.6274718\n",
      "Trained batch 684 batch loss 1.61067641 epoch total loss 1.62744725\n",
      "Trained batch 685 batch loss 1.62285197 epoch total loss 1.62744057\n",
      "Trained batch 686 batch loss 1.48189569 epoch total loss 1.62722838\n",
      "Trained batch 687 batch loss 1.46551609 epoch total loss 1.62699306\n",
      "Trained batch 688 batch loss 1.52673864 epoch total loss 1.62684739\n",
      "Trained batch 689 batch loss 1.67205095 epoch total loss 1.62691295\n",
      "Trained batch 690 batch loss 1.54475892 epoch total loss 1.62679386\n",
      "Trained batch 691 batch loss 1.58080804 epoch total loss 1.62672734\n",
      "Trained batch 692 batch loss 1.53554368 epoch total loss 1.62659562\n",
      "Trained batch 693 batch loss 1.47916079 epoch total loss 1.62638271\n",
      "Trained batch 694 batch loss 1.51887369 epoch total loss 1.62622786\n",
      "Trained batch 695 batch loss 1.50665534 epoch total loss 1.62605596\n",
      "Trained batch 696 batch loss 1.54357755 epoch total loss 1.62593746\n",
      "Trained batch 697 batch loss 1.4483875 epoch total loss 1.62568271\n",
      "Trained batch 698 batch loss 1.56138945 epoch total loss 1.62559056\n",
      "Trained batch 699 batch loss 1.48997951 epoch total loss 1.62539661\n",
      "Trained batch 700 batch loss 1.58505547 epoch total loss 1.62533903\n",
      "Trained batch 701 batch loss 1.59112108 epoch total loss 1.62529016\n",
      "Trained batch 702 batch loss 1.40209389 epoch total loss 1.62497222\n",
      "Trained batch 703 batch loss 1.35536814 epoch total loss 1.62458861\n",
      "Trained batch 704 batch loss 1.46792459 epoch total loss 1.62436604\n",
      "Trained batch 705 batch loss 1.46312344 epoch total loss 1.6241374\n",
      "Trained batch 706 batch loss 1.50893903 epoch total loss 1.6239742\n",
      "Trained batch 707 batch loss 1.57513511 epoch total loss 1.62390518\n",
      "Trained batch 708 batch loss 1.54805863 epoch total loss 1.62379813\n",
      "Trained batch 709 batch loss 1.53771 epoch total loss 1.62367666\n",
      "Trained batch 710 batch loss 1.51401424 epoch total loss 1.62352228\n",
      "Trained batch 711 batch loss 1.51985383 epoch total loss 1.62337649\n",
      "Trained batch 712 batch loss 1.51555181 epoch total loss 1.62322497\n",
      "Trained batch 713 batch loss 1.48723245 epoch total loss 1.62303424\n",
      "Trained batch 714 batch loss 1.4843924 epoch total loss 1.62284\n",
      "Trained batch 715 batch loss 1.65293431 epoch total loss 1.62288213\n",
      "Trained batch 716 batch loss 1.59075356 epoch total loss 1.62283719\n",
      "Trained batch 717 batch loss 1.56637323 epoch total loss 1.62275851\n",
      "Trained batch 718 batch loss 1.56963611 epoch total loss 1.62268436\n",
      "Trained batch 719 batch loss 1.5193336 epoch total loss 1.62254059\n",
      "Trained batch 720 batch loss 1.5582943 epoch total loss 1.62245142\n",
      "Trained batch 721 batch loss 1.56100655 epoch total loss 1.62236631\n",
      "Trained batch 722 batch loss 1.57417262 epoch total loss 1.62229955\n",
      "Trained batch 723 batch loss 1.54079819 epoch total loss 1.62218678\n",
      "Trained batch 724 batch loss 1.51987922 epoch total loss 1.62204552\n",
      "Trained batch 725 batch loss 1.53272688 epoch total loss 1.62192225\n",
      "Trained batch 726 batch loss 1.56875014 epoch total loss 1.62184906\n",
      "Trained batch 727 batch loss 1.62802839 epoch total loss 1.62185752\n",
      "Trained batch 728 batch loss 1.61991715 epoch total loss 1.6218549\n",
      "Trained batch 729 batch loss 1.62895 epoch total loss 1.62186456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 730 batch loss 1.61102724 epoch total loss 1.62184978\n",
      "Trained batch 731 batch loss 1.6233747 epoch total loss 1.62185192\n",
      "Trained batch 732 batch loss 1.63830876 epoch total loss 1.62187433\n",
      "Trained batch 733 batch loss 1.55079651 epoch total loss 1.62177742\n",
      "Trained batch 734 batch loss 1.54805923 epoch total loss 1.62167704\n",
      "Trained batch 735 batch loss 1.55910504 epoch total loss 1.62159181\n",
      "Trained batch 736 batch loss 1.46756625 epoch total loss 1.62138247\n",
      "Trained batch 737 batch loss 1.55431402 epoch total loss 1.62129152\n",
      "Trained batch 738 batch loss 1.38905787 epoch total loss 1.62097681\n",
      "Trained batch 739 batch loss 1.30767322 epoch total loss 1.62055278\n",
      "Trained batch 740 batch loss 1.46834493 epoch total loss 1.62034714\n",
      "Trained batch 741 batch loss 1.58726406 epoch total loss 1.62030256\n",
      "Trained batch 742 batch loss 1.65161514 epoch total loss 1.62034476\n",
      "Trained batch 743 batch loss 1.5932827 epoch total loss 1.62030828\n",
      "Trained batch 744 batch loss 1.63655007 epoch total loss 1.6203301\n",
      "Trained batch 745 batch loss 1.63877523 epoch total loss 1.62035489\n",
      "Trained batch 746 batch loss 1.57615328 epoch total loss 1.62029576\n",
      "Trained batch 747 batch loss 1.58074594 epoch total loss 1.62024271\n",
      "Trained batch 748 batch loss 1.60279131 epoch total loss 1.62021935\n",
      "Trained batch 749 batch loss 1.62249422 epoch total loss 1.62022233\n",
      "Trained batch 750 batch loss 1.52036762 epoch total loss 1.62008917\n",
      "Trained batch 751 batch loss 1.47669554 epoch total loss 1.6198982\n",
      "Trained batch 752 batch loss 1.43863845 epoch total loss 1.61965716\n",
      "Trained batch 753 batch loss 1.43075585 epoch total loss 1.61940634\n",
      "Trained batch 754 batch loss 1.50058126 epoch total loss 1.61924875\n",
      "Trained batch 755 batch loss 1.51394832 epoch total loss 1.61910927\n",
      "Trained batch 756 batch loss 1.5550245 epoch total loss 1.61902452\n",
      "Trained batch 757 batch loss 1.61234188 epoch total loss 1.61901569\n",
      "Trained batch 758 batch loss 1.56758595 epoch total loss 1.61894786\n",
      "Trained batch 759 batch loss 1.50492334 epoch total loss 1.61879754\n",
      "Trained batch 760 batch loss 1.5447495 epoch total loss 1.61870027\n",
      "Trained batch 761 batch loss 1.57631755 epoch total loss 1.61864448\n",
      "Trained batch 762 batch loss 1.45758498 epoch total loss 1.61843324\n",
      "Trained batch 763 batch loss 1.53681254 epoch total loss 1.61832631\n",
      "Trained batch 764 batch loss 1.51031816 epoch total loss 1.61818504\n",
      "Trained batch 765 batch loss 1.51187086 epoch total loss 1.61804593\n",
      "Trained batch 766 batch loss 1.47124803 epoch total loss 1.61785424\n",
      "Trained batch 767 batch loss 1.49594522 epoch total loss 1.61769533\n",
      "Trained batch 768 batch loss 1.47194862 epoch total loss 1.61750555\n",
      "Trained batch 769 batch loss 1.56504393 epoch total loss 1.61743736\n",
      "Trained batch 770 batch loss 1.51247871 epoch total loss 1.61730099\n",
      "Trained batch 771 batch loss 1.42137194 epoch total loss 1.61704695\n",
      "Trained batch 772 batch loss 1.43556595 epoch total loss 1.61681175\n",
      "Trained batch 773 batch loss 1.59638882 epoch total loss 1.61678541\n",
      "Trained batch 774 batch loss 1.53814626 epoch total loss 1.61668372\n",
      "Trained batch 775 batch loss 1.61431074 epoch total loss 1.61668062\n",
      "Trained batch 776 batch loss 1.5618403 epoch total loss 1.61661\n",
      "Trained batch 777 batch loss 1.57943511 epoch total loss 1.61656225\n",
      "Trained batch 778 batch loss 1.57833862 epoch total loss 1.61651313\n",
      "Trained batch 779 batch loss 1.54232585 epoch total loss 1.61641788\n",
      "Trained batch 780 batch loss 1.57685685 epoch total loss 1.61636734\n",
      "Trained batch 781 batch loss 1.55166745 epoch total loss 1.61628437\n",
      "Trained batch 782 batch loss 1.46643782 epoch total loss 1.6160928\n",
      "Trained batch 783 batch loss 1.52998316 epoch total loss 1.61598289\n",
      "Trained batch 784 batch loss 1.53882158 epoch total loss 1.61588442\n",
      "Trained batch 785 batch loss 1.42513669 epoch total loss 1.61564147\n",
      "Trained batch 786 batch loss 1.50969875 epoch total loss 1.61550665\n",
      "Trained batch 787 batch loss 1.50802684 epoch total loss 1.61537015\n",
      "Trained batch 788 batch loss 1.43684506 epoch total loss 1.61514354\n",
      "Trained batch 789 batch loss 1.52653599 epoch total loss 1.61503124\n",
      "Trained batch 790 batch loss 1.55010605 epoch total loss 1.61494899\n",
      "Trained batch 791 batch loss 1.51694715 epoch total loss 1.61482513\n",
      "Trained batch 792 batch loss 1.55094051 epoch total loss 1.61474442\n",
      "Trained batch 793 batch loss 1.61357665 epoch total loss 1.61474288\n",
      "Trained batch 794 batch loss 1.45527864 epoch total loss 1.61454213\n",
      "Trained batch 795 batch loss 1.56167078 epoch total loss 1.61447549\n",
      "Trained batch 796 batch loss 1.58815253 epoch total loss 1.61444247\n",
      "Trained batch 797 batch loss 1.55098414 epoch total loss 1.61436284\n",
      "Trained batch 798 batch loss 1.57472205 epoch total loss 1.61431324\n",
      "Trained batch 799 batch loss 1.46167517 epoch total loss 1.61412215\n",
      "Trained batch 800 batch loss 1.49434733 epoch total loss 1.61397243\n",
      "Trained batch 801 batch loss 1.39209449 epoch total loss 1.6136955\n",
      "Trained batch 802 batch loss 1.49488986 epoch total loss 1.61354733\n",
      "Trained batch 803 batch loss 1.35098839 epoch total loss 1.61322033\n",
      "Trained batch 804 batch loss 1.47400212 epoch total loss 1.61304712\n",
      "Trained batch 805 batch loss 1.53411293 epoch total loss 1.61294901\n",
      "Trained batch 806 batch loss 1.50128031 epoch total loss 1.61281037\n",
      "Trained batch 807 batch loss 1.56840658 epoch total loss 1.6127553\n",
      "Trained batch 808 batch loss 1.5270344 epoch total loss 1.6126492\n",
      "Trained batch 809 batch loss 1.55779326 epoch total loss 1.61258125\n",
      "Trained batch 810 batch loss 1.45157957 epoch total loss 1.61238241\n",
      "Trained batch 811 batch loss 1.47639263 epoch total loss 1.6122148\n",
      "Trained batch 812 batch loss 1.58148539 epoch total loss 1.61217701\n",
      "Trained batch 813 batch loss 1.52362359 epoch total loss 1.61206818\n",
      "Trained batch 814 batch loss 1.54941201 epoch total loss 1.61199129\n",
      "Trained batch 815 batch loss 1.41299 epoch total loss 1.61174703\n",
      "Trained batch 816 batch loss 1.42639804 epoch total loss 1.61151993\n",
      "Trained batch 817 batch loss 1.41294515 epoch total loss 1.61127687\n",
      "Trained batch 818 batch loss 1.37006629 epoch total loss 1.61098206\n",
      "Trained batch 819 batch loss 1.32606518 epoch total loss 1.61063421\n",
      "Trained batch 820 batch loss 1.44749665 epoch total loss 1.61043525\n",
      "Trained batch 821 batch loss 1.5425061 epoch total loss 1.6103524\n",
      "Trained batch 822 batch loss 1.59028029 epoch total loss 1.61032808\n",
      "Trained batch 823 batch loss 1.64340138 epoch total loss 1.61036837\n",
      "Trained batch 824 batch loss 1.56454134 epoch total loss 1.6103127\n",
      "Trained batch 825 batch loss 1.63125134 epoch total loss 1.61033809\n",
      "Trained batch 826 batch loss 1.50176775 epoch total loss 1.6102066\n",
      "Trained batch 827 batch loss 1.44787598 epoch total loss 1.61001027\n",
      "Trained batch 828 batch loss 1.41501296 epoch total loss 1.60977483\n",
      "Trained batch 829 batch loss 1.38166571 epoch total loss 1.60949969\n",
      "Trained batch 830 batch loss 1.40555465 epoch total loss 1.609254\n",
      "Trained batch 831 batch loss 1.474944 epoch total loss 1.60909235\n",
      "Trained batch 832 batch loss 1.51155782 epoch total loss 1.60897517\n",
      "Trained batch 833 batch loss 1.51453829 epoch total loss 1.6088618\n",
      "Trained batch 834 batch loss 1.58629799 epoch total loss 1.60883474\n",
      "Trained batch 835 batch loss 1.56203127 epoch total loss 1.60877872\n",
      "Trained batch 836 batch loss 1.50110424 epoch total loss 1.60864985\n",
      "Trained batch 837 batch loss 1.4894532 epoch total loss 1.60850751\n",
      "Trained batch 838 batch loss 1.48481631 epoch total loss 1.60835993\n",
      "Trained batch 839 batch loss 1.42410421 epoch total loss 1.60814035\n",
      "Trained batch 840 batch loss 1.42032456 epoch total loss 1.60791671\n",
      "Trained batch 841 batch loss 1.47746873 epoch total loss 1.6077615\n",
      "Trained batch 842 batch loss 1.53755605 epoch total loss 1.60767817\n",
      "Trained batch 843 batch loss 1.50293112 epoch total loss 1.60755396\n",
      "Trained batch 844 batch loss 1.51776242 epoch total loss 1.60744762\n",
      "Trained batch 845 batch loss 1.46046674 epoch total loss 1.6072737\n",
      "Trained batch 846 batch loss 1.52787578 epoch total loss 1.60717976\n",
      "Trained batch 847 batch loss 1.52149618 epoch total loss 1.60707855\n",
      "Trained batch 848 batch loss 1.47766316 epoch total loss 1.60692596\n",
      "Trained batch 849 batch loss 1.50720429 epoch total loss 1.60680854\n",
      "Trained batch 850 batch loss 1.50253284 epoch total loss 1.60668588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 851 batch loss 1.44269419 epoch total loss 1.60649323\n",
      "Trained batch 852 batch loss 1.42817831 epoch total loss 1.60628402\n",
      "Trained batch 853 batch loss 1.40749621 epoch total loss 1.60605097\n",
      "Trained batch 854 batch loss 1.48826051 epoch total loss 1.60591304\n",
      "Trained batch 855 batch loss 1.53032923 epoch total loss 1.60582459\n",
      "Trained batch 856 batch loss 1.45305443 epoch total loss 1.60564601\n",
      "Trained batch 857 batch loss 1.50990689 epoch total loss 1.60553432\n",
      "Trained batch 858 batch loss 1.49692714 epoch total loss 1.60540771\n",
      "Trained batch 859 batch loss 1.41563392 epoch total loss 1.60518682\n",
      "Trained batch 860 batch loss 1.48545241 epoch total loss 1.60504758\n",
      "Trained batch 861 batch loss 1.58281088 epoch total loss 1.60502172\n",
      "Trained batch 862 batch loss 1.48033476 epoch total loss 1.60487711\n",
      "Trained batch 863 batch loss 1.57304478 epoch total loss 1.60484016\n",
      "Trained batch 864 batch loss 1.53057146 epoch total loss 1.60475409\n",
      "Trained batch 865 batch loss 1.39490581 epoch total loss 1.6045115\n",
      "Trained batch 866 batch loss 1.41929674 epoch total loss 1.60429764\n",
      "Trained batch 867 batch loss 1.42445242 epoch total loss 1.60409021\n",
      "Trained batch 868 batch loss 1.55453598 epoch total loss 1.60403311\n",
      "Trained batch 869 batch loss 1.55786538 epoch total loss 1.60398006\n",
      "Trained batch 870 batch loss 1.52053165 epoch total loss 1.6038841\n",
      "Trained batch 871 batch loss 1.59410739 epoch total loss 1.6038729\n",
      "Trained batch 872 batch loss 1.423684 epoch total loss 1.60366631\n",
      "Trained batch 873 batch loss 1.29503918 epoch total loss 1.60331273\n",
      "Trained batch 874 batch loss 1.36676621 epoch total loss 1.60304213\n",
      "Trained batch 875 batch loss 1.51651013 epoch total loss 1.60294318\n",
      "Trained batch 876 batch loss 1.57604992 epoch total loss 1.60291255\n",
      "Trained batch 877 batch loss 1.60576272 epoch total loss 1.60291576\n",
      "Trained batch 878 batch loss 1.54908454 epoch total loss 1.60285437\n",
      "Trained batch 879 batch loss 1.58557427 epoch total loss 1.6028347\n",
      "Trained batch 880 batch loss 1.59574962 epoch total loss 1.6028266\n",
      "Trained batch 881 batch loss 1.58240581 epoch total loss 1.60280347\n",
      "Trained batch 882 batch loss 1.53023934 epoch total loss 1.60272121\n",
      "Trained batch 883 batch loss 1.46036494 epoch total loss 1.60255992\n",
      "Trained batch 884 batch loss 1.51629198 epoch total loss 1.60246229\n",
      "Trained batch 885 batch loss 1.52030277 epoch total loss 1.60236943\n",
      "Trained batch 886 batch loss 1.61103272 epoch total loss 1.6023792\n",
      "Trained batch 887 batch loss 1.57263494 epoch total loss 1.60234571\n",
      "Trained batch 888 batch loss 1.50476646 epoch total loss 1.60223579\n",
      "Trained batch 889 batch loss 1.45346415 epoch total loss 1.60206854\n",
      "Trained batch 890 batch loss 1.51997042 epoch total loss 1.60197628\n",
      "Trained batch 891 batch loss 1.35771644 epoch total loss 1.60170209\n",
      "Trained batch 892 batch loss 1.45820773 epoch total loss 1.60154128\n",
      "Trained batch 893 batch loss 1.3528527 epoch total loss 1.60126281\n",
      "Trained batch 894 batch loss 1.38608 epoch total loss 1.60102224\n",
      "Trained batch 895 batch loss 1.51156247 epoch total loss 1.60092223\n",
      "Trained batch 896 batch loss 1.69064736 epoch total loss 1.60102248\n",
      "Trained batch 897 batch loss 1.6465615 epoch total loss 1.60107327\n",
      "Trained batch 898 batch loss 1.54844439 epoch total loss 1.60101473\n",
      "Trained batch 899 batch loss 1.52216065 epoch total loss 1.600927\n",
      "Trained batch 900 batch loss 1.53318214 epoch total loss 1.60085177\n",
      "Trained batch 901 batch loss 1.51535916 epoch total loss 1.60075688\n",
      "Trained batch 902 batch loss 1.53120124 epoch total loss 1.60067987\n",
      "Trained batch 903 batch loss 1.48899198 epoch total loss 1.60055625\n",
      "Trained batch 904 batch loss 1.50541818 epoch total loss 1.60045087\n",
      "Trained batch 905 batch loss 1.51961577 epoch total loss 1.60036159\n",
      "Trained batch 906 batch loss 1.48980677 epoch total loss 1.60023952\n",
      "Trained batch 907 batch loss 1.53154206 epoch total loss 1.6001637\n",
      "Trained batch 908 batch loss 1.45104873 epoch total loss 1.59999955\n",
      "Trained batch 909 batch loss 1.52226329 epoch total loss 1.59991395\n",
      "Trained batch 910 batch loss 1.49297321 epoch total loss 1.59979641\n",
      "Trained batch 911 batch loss 1.40813887 epoch total loss 1.59958589\n",
      "Trained batch 912 batch loss 1.45551038 epoch total loss 1.59942806\n",
      "Trained batch 913 batch loss 1.47679591 epoch total loss 1.59929371\n",
      "Trained batch 914 batch loss 1.45018923 epoch total loss 1.59913063\n",
      "Trained batch 915 batch loss 1.56491268 epoch total loss 1.5990932\n",
      "Trained batch 916 batch loss 1.61772358 epoch total loss 1.59911346\n",
      "Trained batch 917 batch loss 1.56190062 epoch total loss 1.59907293\n",
      "Trained batch 918 batch loss 1.52144694 epoch total loss 1.59898841\n",
      "Trained batch 919 batch loss 1.52650273 epoch total loss 1.5989095\n",
      "Trained batch 920 batch loss 1.50981045 epoch total loss 1.59881258\n",
      "Trained batch 921 batch loss 1.51160252 epoch total loss 1.59871793\n",
      "Trained batch 922 batch loss 1.60004187 epoch total loss 1.59871936\n",
      "Trained batch 923 batch loss 1.52220404 epoch total loss 1.59863651\n",
      "Trained batch 924 batch loss 1.67591465 epoch total loss 1.59872019\n",
      "Trained batch 925 batch loss 1.67937934 epoch total loss 1.59880722\n",
      "Trained batch 926 batch loss 1.67497301 epoch total loss 1.59888947\n",
      "Trained batch 927 batch loss 1.59485972 epoch total loss 1.59888506\n",
      "Trained batch 928 batch loss 1.63564694 epoch total loss 1.59892476\n",
      "Trained batch 929 batch loss 1.71151543 epoch total loss 1.59904599\n",
      "Trained batch 930 batch loss 1.62155914 epoch total loss 1.59907019\n",
      "Trained batch 931 batch loss 1.5887686 epoch total loss 1.5990591\n",
      "Trained batch 932 batch loss 1.48281956 epoch total loss 1.59893429\n",
      "Trained batch 933 batch loss 1.47009945 epoch total loss 1.59879625\n",
      "Trained batch 934 batch loss 1.46853256 epoch total loss 1.59865677\n",
      "Trained batch 935 batch loss 1.50738311 epoch total loss 1.59855902\n",
      "Trained batch 936 batch loss 1.49250603 epoch total loss 1.59844577\n",
      "Trained batch 937 batch loss 1.46931529 epoch total loss 1.59830809\n",
      "Trained batch 938 batch loss 1.4593904 epoch total loss 1.59815991\n",
      "Trained batch 939 batch loss 1.51952481 epoch total loss 1.59807611\n",
      "Trained batch 940 batch loss 1.4148345 epoch total loss 1.5978812\n",
      "Trained batch 941 batch loss 1.36198139 epoch total loss 1.59763038\n",
      "Trained batch 942 batch loss 1.23286796 epoch total loss 1.59724331\n",
      "Trained batch 943 batch loss 1.20634174 epoch total loss 1.5968287\n",
      "Trained batch 944 batch loss 1.41096258 epoch total loss 1.59663188\n",
      "Trained batch 945 batch loss 1.63705564 epoch total loss 1.59667468\n",
      "Trained batch 946 batch loss 1.53564119 epoch total loss 1.59661019\n",
      "Trained batch 947 batch loss 1.563568 epoch total loss 1.59657526\n",
      "Trained batch 948 batch loss 1.52869809 epoch total loss 1.59650362\n",
      "Trained batch 949 batch loss 1.45138073 epoch total loss 1.59635079\n",
      "Trained batch 950 batch loss 1.39263618 epoch total loss 1.59613633\n",
      "Trained batch 951 batch loss 1.41874123 epoch total loss 1.59594965\n",
      "Trained batch 952 batch loss 1.44511008 epoch total loss 1.59579122\n",
      "Trained batch 953 batch loss 1.4183259 epoch total loss 1.59560502\n",
      "Trained batch 954 batch loss 1.44664705 epoch total loss 1.59544885\n",
      "Trained batch 955 batch loss 1.37670016 epoch total loss 1.59521985\n",
      "Trained batch 956 batch loss 1.3268199 epoch total loss 1.59493899\n",
      "Trained batch 957 batch loss 1.43836367 epoch total loss 1.59477544\n",
      "Trained batch 958 batch loss 1.5368644 epoch total loss 1.594715\n",
      "Trained batch 959 batch loss 1.52558255 epoch total loss 1.59464288\n",
      "Trained batch 960 batch loss 1.4958272 epoch total loss 1.59454\n",
      "Trained batch 961 batch loss 1.42773485 epoch total loss 1.59436643\n",
      "Trained batch 962 batch loss 1.3741684 epoch total loss 1.59413755\n",
      "Trained batch 963 batch loss 1.48468804 epoch total loss 1.59402394\n",
      "Trained batch 964 batch loss 1.5518744 epoch total loss 1.59398019\n",
      "Trained batch 965 batch loss 1.46231258 epoch total loss 1.5938437\n",
      "Trained batch 966 batch loss 1.43036938 epoch total loss 1.59367454\n",
      "Trained batch 967 batch loss 1.38391244 epoch total loss 1.59345758\n",
      "Trained batch 968 batch loss 1.38997412 epoch total loss 1.59324741\n",
      "Trained batch 969 batch loss 1.52367246 epoch total loss 1.59317565\n",
      "Trained batch 970 batch loss 1.56195521 epoch total loss 1.59314358\n",
      "Trained batch 971 batch loss 1.54887462 epoch total loss 1.59309793\n",
      "Trained batch 972 batch loss 1.44473529 epoch total loss 1.59294522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 973 batch loss 1.38984764 epoch total loss 1.5927366\n",
      "Trained batch 974 batch loss 1.39027965 epoch total loss 1.5925287\n",
      "Trained batch 975 batch loss 1.38294947 epoch total loss 1.59231365\n",
      "Trained batch 976 batch loss 1.46622539 epoch total loss 1.59218442\n",
      "Trained batch 977 batch loss 1.52310324 epoch total loss 1.59211373\n",
      "Trained batch 978 batch loss 1.51779175 epoch total loss 1.5920378\n",
      "Trained batch 979 batch loss 1.62022364 epoch total loss 1.59206653\n",
      "Trained batch 980 batch loss 1.45678878 epoch total loss 1.59192848\n",
      "Trained batch 981 batch loss 1.57220578 epoch total loss 1.59190845\n",
      "Trained batch 982 batch loss 1.4849391 epoch total loss 1.59179962\n",
      "Trained batch 983 batch loss 1.54974496 epoch total loss 1.59175694\n",
      "Trained batch 984 batch loss 1.60906339 epoch total loss 1.59177446\n",
      "Trained batch 985 batch loss 1.49785662 epoch total loss 1.59167898\n",
      "Trained batch 986 batch loss 1.49946237 epoch total loss 1.59158552\n",
      "Trained batch 987 batch loss 1.55017877 epoch total loss 1.59154356\n",
      "Trained batch 988 batch loss 1.5041306 epoch total loss 1.5914551\n",
      "Trained batch 989 batch loss 1.58178139 epoch total loss 1.59144533\n",
      "Trained batch 990 batch loss 1.5199095 epoch total loss 1.59137309\n",
      "Trained batch 991 batch loss 1.55192065 epoch total loss 1.59133327\n",
      "Trained batch 992 batch loss 1.48177671 epoch total loss 1.59122288\n",
      "Trained batch 993 batch loss 1.46974564 epoch total loss 1.59110045\n",
      "Trained batch 994 batch loss 1.48145175 epoch total loss 1.59099019\n",
      "Trained batch 995 batch loss 1.45278418 epoch total loss 1.59085119\n",
      "Trained batch 996 batch loss 1.48325634 epoch total loss 1.59074318\n",
      "Trained batch 997 batch loss 1.33939815 epoch total loss 1.59049106\n",
      "Trained batch 998 batch loss 1.52979064 epoch total loss 1.59043026\n",
      "Trained batch 999 batch loss 1.3817 epoch total loss 1.59022129\n",
      "Trained batch 1000 batch loss 1.45336556 epoch total loss 1.59008443\n",
      "Trained batch 1001 batch loss 1.44844413 epoch total loss 1.58994305\n",
      "Trained batch 1002 batch loss 1.50100875 epoch total loss 1.58985424\n",
      "Trained batch 1003 batch loss 1.43393886 epoch total loss 1.58969879\n",
      "Trained batch 1004 batch loss 1.4950341 epoch total loss 1.5896045\n",
      "Trained batch 1005 batch loss 1.51907158 epoch total loss 1.58953428\n",
      "Trained batch 1006 batch loss 1.48373091 epoch total loss 1.58942914\n",
      "Trained batch 1007 batch loss 1.51725566 epoch total loss 1.58935738\n",
      "Trained batch 1008 batch loss 1.56301427 epoch total loss 1.58933127\n",
      "Trained batch 1009 batch loss 1.55665052 epoch total loss 1.58929884\n",
      "Trained batch 1010 batch loss 1.51108146 epoch total loss 1.58922148\n",
      "Trained batch 1011 batch loss 1.48405242 epoch total loss 1.58911741\n",
      "Trained batch 1012 batch loss 1.3987999 epoch total loss 1.5889293\n",
      "Trained batch 1013 batch loss 1.4962945 epoch total loss 1.58883786\n",
      "Trained batch 1014 batch loss 1.45555568 epoch total loss 1.58870649\n",
      "Trained batch 1015 batch loss 1.42479014 epoch total loss 1.58854496\n",
      "Trained batch 1016 batch loss 1.49221396 epoch total loss 1.58845019\n",
      "Trained batch 1017 batch loss 1.53201222 epoch total loss 1.58839464\n",
      "Trained batch 1018 batch loss 1.4499197 epoch total loss 1.58825862\n",
      "Trained batch 1019 batch loss 1.47858632 epoch total loss 1.5881511\n",
      "Trained batch 1020 batch loss 1.54441559 epoch total loss 1.58810818\n",
      "Trained batch 1021 batch loss 1.64861882 epoch total loss 1.58816743\n",
      "Trained batch 1022 batch loss 1.6397382 epoch total loss 1.58821785\n",
      "Trained batch 1023 batch loss 1.48380506 epoch total loss 1.58811581\n",
      "Trained batch 1024 batch loss 1.42053914 epoch total loss 1.58795214\n",
      "Trained batch 1025 batch loss 1.50092852 epoch total loss 1.58786726\n",
      "Trained batch 1026 batch loss 1.43822742 epoch total loss 1.58772147\n",
      "Trained batch 1027 batch loss 1.45224094 epoch total loss 1.5875895\n",
      "Trained batch 1028 batch loss 1.46319616 epoch total loss 1.58746862\n",
      "Trained batch 1029 batch loss 1.59611726 epoch total loss 1.58747697\n",
      "Trained batch 1030 batch loss 1.40355742 epoch total loss 1.58729839\n",
      "Trained batch 1031 batch loss 1.49253798 epoch total loss 1.58720648\n",
      "Trained batch 1032 batch loss 1.51358092 epoch total loss 1.5871352\n",
      "Trained batch 1033 batch loss 1.46083665 epoch total loss 1.58701289\n",
      "Trained batch 1034 batch loss 1.49545848 epoch total loss 1.58692431\n",
      "Trained batch 1035 batch loss 1.53226328 epoch total loss 1.5868715\n",
      "Trained batch 1036 batch loss 1.48446059 epoch total loss 1.58677268\n",
      "Trained batch 1037 batch loss 1.42706096 epoch total loss 1.58661854\n",
      "Trained batch 1038 batch loss 1.38569701 epoch total loss 1.58642507\n",
      "Trained batch 1039 batch loss 1.43404293 epoch total loss 1.58627844\n",
      "Trained batch 1040 batch loss 1.47810316 epoch total loss 1.58617449\n",
      "Trained batch 1041 batch loss 1.62997365 epoch total loss 1.58621657\n",
      "Trained batch 1042 batch loss 1.70400095 epoch total loss 1.58632958\n",
      "Trained batch 1043 batch loss 1.63890052 epoch total loss 1.58638\n",
      "Trained batch 1044 batch loss 1.41025639 epoch total loss 1.58621132\n",
      "Trained batch 1045 batch loss 1.3623569 epoch total loss 1.5859971\n",
      "Trained batch 1046 batch loss 1.4496069 epoch total loss 1.58586669\n",
      "Trained batch 1047 batch loss 1.50365508 epoch total loss 1.58578813\n",
      "Trained batch 1048 batch loss 1.41046679 epoch total loss 1.58562088\n",
      "Trained batch 1049 batch loss 1.46813333 epoch total loss 1.58550894\n",
      "Trained batch 1050 batch loss 1.46399045 epoch total loss 1.58539319\n",
      "Trained batch 1051 batch loss 1.56272316 epoch total loss 1.58537161\n",
      "Trained batch 1052 batch loss 1.53417039 epoch total loss 1.58532298\n",
      "Trained batch 1053 batch loss 1.55716681 epoch total loss 1.58529615\n",
      "Trained batch 1054 batch loss 1.5043633 epoch total loss 1.58521938\n",
      "Trained batch 1055 batch loss 1.47761428 epoch total loss 1.58511746\n",
      "Trained batch 1056 batch loss 1.46701956 epoch total loss 1.58500564\n",
      "Trained batch 1057 batch loss 1.50864172 epoch total loss 1.5849334\n",
      "Trained batch 1058 batch loss 1.44339955 epoch total loss 1.58479965\n",
      "Trained batch 1059 batch loss 1.41559601 epoch total loss 1.58463991\n",
      "Trained batch 1060 batch loss 1.39295709 epoch total loss 1.58445907\n",
      "Trained batch 1061 batch loss 1.41114879 epoch total loss 1.58429575\n",
      "Trained batch 1062 batch loss 1.45422554 epoch total loss 1.5841732\n",
      "Trained batch 1063 batch loss 1.45710266 epoch total loss 1.58405375\n",
      "Trained batch 1064 batch loss 1.41208577 epoch total loss 1.58389211\n",
      "Trained batch 1065 batch loss 1.43413472 epoch total loss 1.58375144\n",
      "Trained batch 1066 batch loss 1.41117477 epoch total loss 1.58358955\n",
      "Trained batch 1067 batch loss 1.42160559 epoch total loss 1.5834378\n",
      "Trained batch 1068 batch loss 1.42704952 epoch total loss 1.58329129\n",
      "Trained batch 1069 batch loss 1.50051332 epoch total loss 1.58321381\n",
      "Trained batch 1070 batch loss 1.46115232 epoch total loss 1.58309972\n",
      "Trained batch 1071 batch loss 1.43131971 epoch total loss 1.58295798\n",
      "Trained batch 1072 batch loss 1.52689505 epoch total loss 1.58290565\n",
      "Trained batch 1073 batch loss 1.41968679 epoch total loss 1.58275354\n",
      "Trained batch 1074 batch loss 1.4144702 epoch total loss 1.58259678\n",
      "Trained batch 1075 batch loss 1.41309023 epoch total loss 1.58243918\n",
      "Trained batch 1076 batch loss 1.48008752 epoch total loss 1.58234406\n",
      "Trained batch 1077 batch loss 1.42672634 epoch total loss 1.58219957\n",
      "Trained batch 1078 batch loss 1.52275872 epoch total loss 1.58214438\n",
      "Trained batch 1079 batch loss 1.49933386 epoch total loss 1.58206773\n",
      "Trained batch 1080 batch loss 1.41662669 epoch total loss 1.58191454\n",
      "Trained batch 1081 batch loss 1.45410788 epoch total loss 1.58179629\n",
      "Trained batch 1082 batch loss 1.5723772 epoch total loss 1.58178759\n",
      "Trained batch 1083 batch loss 1.43861496 epoch total loss 1.58165538\n",
      "Trained batch 1084 batch loss 1.43784416 epoch total loss 1.5815227\n",
      "Trained batch 1085 batch loss 1.46244192 epoch total loss 1.58141291\n",
      "Trained batch 1086 batch loss 1.50043178 epoch total loss 1.58133841\n",
      "Trained batch 1087 batch loss 1.45228243 epoch total loss 1.58121967\n",
      "Trained batch 1088 batch loss 1.43254554 epoch total loss 1.58108294\n",
      "Trained batch 1089 batch loss 1.51128864 epoch total loss 1.58101881\n",
      "Trained batch 1090 batch loss 1.50507236 epoch total loss 1.58094919\n",
      "Trained batch 1091 batch loss 1.43283093 epoch total loss 1.58081341\n",
      "Trained batch 1092 batch loss 1.44591534 epoch total loss 1.58068991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1093 batch loss 1.32005537 epoch total loss 1.58045149\n",
      "Trained batch 1094 batch loss 1.44464672 epoch total loss 1.58032739\n",
      "Trained batch 1095 batch loss 1.41791701 epoch total loss 1.5801791\n",
      "Trained batch 1096 batch loss 1.50604463 epoch total loss 1.5801115\n",
      "Trained batch 1097 batch loss 1.5996232 epoch total loss 1.58012927\n",
      "Trained batch 1098 batch loss 1.58385623 epoch total loss 1.58013272\n",
      "Trained batch 1099 batch loss 1.68748415 epoch total loss 1.58023036\n",
      "Trained batch 1100 batch loss 1.54147303 epoch total loss 1.58019519\n",
      "Trained batch 1101 batch loss 1.59243798 epoch total loss 1.58020627\n",
      "Trained batch 1102 batch loss 1.48139012 epoch total loss 1.58011663\n",
      "Trained batch 1103 batch loss 1.35981882 epoch total loss 1.57991695\n",
      "Trained batch 1104 batch loss 1.25697327 epoch total loss 1.57962441\n",
      "Trained batch 1105 batch loss 1.22960126 epoch total loss 1.57930768\n",
      "Trained batch 1106 batch loss 1.28929877 epoch total loss 1.57904553\n",
      "Trained batch 1107 batch loss 1.56382012 epoch total loss 1.57903171\n",
      "Trained batch 1108 batch loss 1.47904515 epoch total loss 1.57894146\n",
      "Trained batch 1109 batch loss 1.38152504 epoch total loss 1.57876337\n",
      "Trained batch 1110 batch loss 1.41969943 epoch total loss 1.57862008\n",
      "Trained batch 1111 batch loss 1.40233707 epoch total loss 1.57846141\n",
      "Trained batch 1112 batch loss 1.43756449 epoch total loss 1.57833481\n",
      "Trained batch 1113 batch loss 1.364959 epoch total loss 1.57814312\n",
      "Trained batch 1114 batch loss 1.29215634 epoch total loss 1.57788634\n",
      "Trained batch 1115 batch loss 1.25028229 epoch total loss 1.57759249\n",
      "Trained batch 1116 batch loss 1.28910279 epoch total loss 1.57733393\n",
      "Trained batch 1117 batch loss 1.35573184 epoch total loss 1.57713556\n",
      "Trained batch 1118 batch loss 1.28251159 epoch total loss 1.57687199\n",
      "Trained batch 1119 batch loss 1.36228871 epoch total loss 1.57668018\n",
      "Trained batch 1120 batch loss 1.46289849 epoch total loss 1.57657862\n",
      "Trained batch 1121 batch loss 1.3688587 epoch total loss 1.57639337\n",
      "Trained batch 1122 batch loss 1.39380407 epoch total loss 1.57623065\n",
      "Trained batch 1123 batch loss 1.32469022 epoch total loss 1.57600665\n",
      "Trained batch 1124 batch loss 1.43203652 epoch total loss 1.5758785\n",
      "Trained batch 1125 batch loss 1.47421086 epoch total loss 1.57578814\n",
      "Trained batch 1126 batch loss 1.50164926 epoch total loss 1.57572246\n",
      "Trained batch 1127 batch loss 1.54514098 epoch total loss 1.57569528\n",
      "Trained batch 1128 batch loss 1.44066203 epoch total loss 1.57557559\n",
      "Trained batch 1129 batch loss 1.31083012 epoch total loss 1.57534111\n",
      "Trained batch 1130 batch loss 1.38824713 epoch total loss 1.57517552\n",
      "Trained batch 1131 batch loss 1.45811284 epoch total loss 1.57507205\n",
      "Trained batch 1132 batch loss 1.47083497 epoch total loss 1.57498\n",
      "Trained batch 1133 batch loss 1.49537206 epoch total loss 1.57490969\n",
      "Trained batch 1134 batch loss 1.5678736 epoch total loss 1.57490349\n",
      "Trained batch 1135 batch loss 1.51539207 epoch total loss 1.57485104\n",
      "Trained batch 1136 batch loss 1.55930603 epoch total loss 1.57483733\n",
      "Trained batch 1137 batch loss 1.56927884 epoch total loss 1.57483256\n",
      "Trained batch 1138 batch loss 1.53288376 epoch total loss 1.5747956\n",
      "Trained batch 1139 batch loss 1.44262207 epoch total loss 1.57467961\n",
      "Trained batch 1140 batch loss 1.44665956 epoch total loss 1.57456732\n",
      "Trained batch 1141 batch loss 1.54928315 epoch total loss 1.57454515\n",
      "Trained batch 1142 batch loss 1.54613805 epoch total loss 1.57452035\n",
      "Trained batch 1143 batch loss 1.46193647 epoch total loss 1.57442176\n",
      "Trained batch 1144 batch loss 1.39377904 epoch total loss 1.57426393\n",
      "Trained batch 1145 batch loss 1.42341483 epoch total loss 1.5741322\n",
      "Trained batch 1146 batch loss 1.41583085 epoch total loss 1.57399404\n",
      "Trained batch 1147 batch loss 1.4107393 epoch total loss 1.5738517\n",
      "Trained batch 1148 batch loss 1.52653241 epoch total loss 1.57381046\n",
      "Trained batch 1149 batch loss 1.44918406 epoch total loss 1.57370198\n",
      "Trained batch 1150 batch loss 1.56855488 epoch total loss 1.57369757\n",
      "Trained batch 1151 batch loss 1.44365144 epoch total loss 1.57358456\n",
      "Trained batch 1152 batch loss 1.53491724 epoch total loss 1.57355094\n",
      "Trained batch 1153 batch loss 1.59678793 epoch total loss 1.57357109\n",
      "Trained batch 1154 batch loss 1.54719543 epoch total loss 1.57354832\n",
      "Trained batch 1155 batch loss 1.58513427 epoch total loss 1.57355833\n",
      "Trained batch 1156 batch loss 1.47159 epoch total loss 1.57347012\n",
      "Trained batch 1157 batch loss 1.53989673 epoch total loss 1.57344103\n",
      "Trained batch 1158 batch loss 1.52570355 epoch total loss 1.5733999\n",
      "Trained batch 1159 batch loss 1.42858434 epoch total loss 1.57327497\n",
      "Trained batch 1160 batch loss 1.52991319 epoch total loss 1.57323754\n",
      "Trained batch 1161 batch loss 1.512658 epoch total loss 1.57318544\n",
      "Trained batch 1162 batch loss 1.45948505 epoch total loss 1.57308757\n",
      "Trained batch 1163 batch loss 1.4756496 epoch total loss 1.57300377\n",
      "Trained batch 1164 batch loss 1.37318659 epoch total loss 1.57283211\n",
      "Trained batch 1165 batch loss 1.38954711 epoch total loss 1.57267475\n",
      "Trained batch 1166 batch loss 1.38966119 epoch total loss 1.57251787\n",
      "Trained batch 1167 batch loss 1.41109407 epoch total loss 1.57237959\n",
      "Trained batch 1168 batch loss 1.43758452 epoch total loss 1.57226419\n",
      "Trained batch 1169 batch loss 1.39207125 epoch total loss 1.57211\n",
      "Trained batch 1170 batch loss 1.30999982 epoch total loss 1.57188606\n",
      "Trained batch 1171 batch loss 1.43289018 epoch total loss 1.57176733\n",
      "Trained batch 1172 batch loss 1.40777063 epoch total loss 1.57162738\n",
      "Trained batch 1173 batch loss 1.5080564 epoch total loss 1.57157314\n",
      "Trained batch 1174 batch loss 1.40068674 epoch total loss 1.57142758\n",
      "Trained batch 1175 batch loss 1.39898515 epoch total loss 1.57128072\n",
      "Trained batch 1176 batch loss 1.39084816 epoch total loss 1.5711273\n",
      "Trained batch 1177 batch loss 1.31718302 epoch total loss 1.57091153\n",
      "Trained batch 1178 batch loss 1.32183027 epoch total loss 1.5707\n",
      "Trained batch 1179 batch loss 1.37451601 epoch total loss 1.57053363\n",
      "Trained batch 1180 batch loss 1.41967034 epoch total loss 1.57040584\n",
      "Trained batch 1181 batch loss 1.39698315 epoch total loss 1.57025898\n",
      "Trained batch 1182 batch loss 1.47857451 epoch total loss 1.57018137\n",
      "Trained batch 1183 batch loss 1.35633039 epoch total loss 1.57000053\n",
      "Trained batch 1184 batch loss 1.4897536 epoch total loss 1.56993282\n",
      "Trained batch 1185 batch loss 1.45511627 epoch total loss 1.5698359\n",
      "Trained batch 1186 batch loss 1.5063386 epoch total loss 1.56978238\n",
      "Trained batch 1187 batch loss 1.3998524 epoch total loss 1.56963921\n",
      "Trained batch 1188 batch loss 1.39586246 epoch total loss 1.56949294\n",
      "Trained batch 1189 batch loss 1.48859441 epoch total loss 1.56942499\n",
      "Trained batch 1190 batch loss 1.50235701 epoch total loss 1.5693686\n",
      "Trained batch 1191 batch loss 1.53005219 epoch total loss 1.56933558\n",
      "Trained batch 1192 batch loss 1.55291331 epoch total loss 1.56932175\n",
      "Trained batch 1193 batch loss 1.45556688 epoch total loss 1.56922638\n",
      "Trained batch 1194 batch loss 1.41602564 epoch total loss 1.569098\n",
      "Trained batch 1195 batch loss 1.3815968 epoch total loss 1.56894112\n",
      "Trained batch 1196 batch loss 1.50752425 epoch total loss 1.56888986\n",
      "Trained batch 1197 batch loss 1.36267209 epoch total loss 1.56871748\n",
      "Trained batch 1198 batch loss 1.41132212 epoch total loss 1.56858623\n",
      "Trained batch 1199 batch loss 1.36932898 epoch total loss 1.56842\n",
      "Trained batch 1200 batch loss 1.4232918 epoch total loss 1.56829917\n",
      "Trained batch 1201 batch loss 1.37283814 epoch total loss 1.56813633\n",
      "Trained batch 1202 batch loss 1.36645603 epoch total loss 1.56796861\n",
      "Trained batch 1203 batch loss 1.54965305 epoch total loss 1.56795335\n",
      "Trained batch 1204 batch loss 1.36805296 epoch total loss 1.56778729\n",
      "Trained batch 1205 batch loss 1.32860255 epoch total loss 1.56758881\n",
      "Trained batch 1206 batch loss 1.44162714 epoch total loss 1.56748438\n",
      "Trained batch 1207 batch loss 1.51170993 epoch total loss 1.56743824\n",
      "Trained batch 1208 batch loss 1.50173545 epoch total loss 1.56738377\n",
      "Trained batch 1209 batch loss 1.49025965 epoch total loss 1.56732\n",
      "Trained batch 1210 batch loss 1.45003104 epoch total loss 1.56722307\n",
      "Trained batch 1211 batch loss 1.35699916 epoch total loss 1.56704962\n",
      "Trained batch 1212 batch loss 1.3832655 epoch total loss 1.56689799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1213 batch loss 1.42624092 epoch total loss 1.566782\n",
      "Trained batch 1214 batch loss 1.39627147 epoch total loss 1.56664157\n",
      "Trained batch 1215 batch loss 1.48038185 epoch total loss 1.56657052\n",
      "Trained batch 1216 batch loss 1.36616421 epoch total loss 1.56640577\n",
      "Trained batch 1217 batch loss 1.39946711 epoch total loss 1.56626856\n",
      "Trained batch 1218 batch loss 1.40131474 epoch total loss 1.56613314\n",
      "Trained batch 1219 batch loss 1.52175248 epoch total loss 1.56609666\n",
      "Trained batch 1220 batch loss 1.55939972 epoch total loss 1.5660913\n",
      "Trained batch 1221 batch loss 1.61023 epoch total loss 1.56612742\n",
      "Trained batch 1222 batch loss 1.5645889 epoch total loss 1.56612611\n",
      "Trained batch 1223 batch loss 1.62089586 epoch total loss 1.56617093\n",
      "Trained batch 1224 batch loss 1.48773217 epoch total loss 1.56610692\n",
      "Trained batch 1225 batch loss 1.56633949 epoch total loss 1.56610703\n",
      "Trained batch 1226 batch loss 1.55173731 epoch total loss 1.56609535\n",
      "Trained batch 1227 batch loss 1.52603376 epoch total loss 1.56606257\n",
      "Trained batch 1228 batch loss 1.47670257 epoch total loss 1.56598985\n",
      "Trained batch 1229 batch loss 1.4557879 epoch total loss 1.56590021\n",
      "Trained batch 1230 batch loss 1.48844314 epoch total loss 1.56583714\n",
      "Trained batch 1231 batch loss 1.55088449 epoch total loss 1.5658251\n",
      "Trained batch 1232 batch loss 1.54598701 epoch total loss 1.56580901\n",
      "Trained batch 1233 batch loss 1.48306119 epoch total loss 1.5657419\n",
      "Trained batch 1234 batch loss 1.41523695 epoch total loss 1.56562\n",
      "Trained batch 1235 batch loss 1.44066513 epoch total loss 1.56551874\n",
      "Trained batch 1236 batch loss 1.35840654 epoch total loss 1.56535113\n",
      "Trained batch 1237 batch loss 1.48084319 epoch total loss 1.56528282\n",
      "Trained batch 1238 batch loss 1.46816385 epoch total loss 1.56520438\n",
      "Trained batch 1239 batch loss 1.3721261 epoch total loss 1.56504846\n",
      "Trained batch 1240 batch loss 1.38075066 epoch total loss 1.56489992\n",
      "Trained batch 1241 batch loss 1.41652524 epoch total loss 1.56478024\n",
      "Trained batch 1242 batch loss 1.48152423 epoch total loss 1.56471324\n",
      "Trained batch 1243 batch loss 1.44443893 epoch total loss 1.56461656\n",
      "Trained batch 1244 batch loss 1.44869447 epoch total loss 1.56452334\n",
      "Trained batch 1245 batch loss 1.34748387 epoch total loss 1.56434906\n",
      "Trained batch 1246 batch loss 1.45075071 epoch total loss 1.56425798\n",
      "Trained batch 1247 batch loss 1.41176271 epoch total loss 1.56413567\n",
      "Trained batch 1248 batch loss 1.44363832 epoch total loss 1.56403911\n",
      "Trained batch 1249 batch loss 1.5574528 epoch total loss 1.56403387\n",
      "Trained batch 1250 batch loss 1.35553801 epoch total loss 1.56386709\n",
      "Trained batch 1251 batch loss 1.37512088 epoch total loss 1.56371617\n",
      "Trained batch 1252 batch loss 1.38801193 epoch total loss 1.56357586\n",
      "Trained batch 1253 batch loss 1.45004594 epoch total loss 1.56348538\n",
      "Trained batch 1254 batch loss 1.57268691 epoch total loss 1.56349266\n",
      "Trained batch 1255 batch loss 1.53226447 epoch total loss 1.56346774\n",
      "Trained batch 1256 batch loss 1.43574214 epoch total loss 1.56336606\n",
      "Trained batch 1257 batch loss 1.32844019 epoch total loss 1.56317925\n",
      "Trained batch 1258 batch loss 1.39672589 epoch total loss 1.56304693\n",
      "Trained batch 1259 batch loss 1.37848139 epoch total loss 1.5629003\n",
      "Trained batch 1260 batch loss 1.49281895 epoch total loss 1.56284475\n",
      "Trained batch 1261 batch loss 1.45419562 epoch total loss 1.56275856\n",
      "Trained batch 1262 batch loss 1.39451 epoch total loss 1.56262529\n",
      "Trained batch 1263 batch loss 1.34772599 epoch total loss 1.56245518\n",
      "Trained batch 1264 batch loss 1.49113977 epoch total loss 1.56239867\n",
      "Trained batch 1265 batch loss 1.41564393 epoch total loss 1.56228268\n",
      "Trained batch 1266 batch loss 1.49874139 epoch total loss 1.56223249\n",
      "Trained batch 1267 batch loss 1.36861098 epoch total loss 1.56207979\n",
      "Trained batch 1268 batch loss 1.27820516 epoch total loss 1.56185591\n",
      "Trained batch 1269 batch loss 1.30222869 epoch total loss 1.56165123\n",
      "Trained batch 1270 batch loss 1.35110271 epoch total loss 1.56148553\n",
      "Trained batch 1271 batch loss 1.38900065 epoch total loss 1.56134975\n",
      "Trained batch 1272 batch loss 1.33758521 epoch total loss 1.5611738\n",
      "Trained batch 1273 batch loss 1.46108556 epoch total loss 1.56109524\n",
      "Trained batch 1274 batch loss 1.43020332 epoch total loss 1.56099248\n",
      "Trained batch 1275 batch loss 1.43226767 epoch total loss 1.56089139\n",
      "Trained batch 1276 batch loss 1.47023 epoch total loss 1.56082034\n",
      "Trained batch 1277 batch loss 1.53137875 epoch total loss 1.56079733\n",
      "Trained batch 1278 batch loss 1.64029837 epoch total loss 1.56085944\n",
      "Trained batch 1279 batch loss 1.64794981 epoch total loss 1.56092763\n",
      "Trained batch 1280 batch loss 1.55407429 epoch total loss 1.56092227\n",
      "Trained batch 1281 batch loss 1.54232538 epoch total loss 1.56090772\n",
      "Trained batch 1282 batch loss 1.57364368 epoch total loss 1.56091762\n",
      "Trained batch 1283 batch loss 1.67796516 epoch total loss 1.56100893\n",
      "Trained batch 1284 batch loss 1.55219376 epoch total loss 1.56100202\n",
      "Trained batch 1285 batch loss 1.57958591 epoch total loss 1.56101656\n",
      "Trained batch 1286 batch loss 1.47089934 epoch total loss 1.56094646\n",
      "Trained batch 1287 batch loss 1.46617734 epoch total loss 1.56087291\n",
      "Trained batch 1288 batch loss 1.4294374 epoch total loss 1.56077087\n",
      "Trained batch 1289 batch loss 1.47892129 epoch total loss 1.56070733\n",
      "Trained batch 1290 batch loss 1.54957271 epoch total loss 1.56069863\n",
      "Trained batch 1291 batch loss 1.46851015 epoch total loss 1.56062722\n",
      "Trained batch 1292 batch loss 1.43163764 epoch total loss 1.56052744\n",
      "Trained batch 1293 batch loss 1.4081161 epoch total loss 1.56040955\n",
      "Trained batch 1294 batch loss 1.32679117 epoch total loss 1.56022894\n",
      "Trained batch 1295 batch loss 1.56014085 epoch total loss 1.56022894\n",
      "Trained batch 1296 batch loss 1.63815641 epoch total loss 1.56028903\n",
      "Trained batch 1297 batch loss 1.68568099 epoch total loss 1.5603857\n",
      "Trained batch 1298 batch loss 1.66221118 epoch total loss 1.56046426\n",
      "Trained batch 1299 batch loss 1.52821088 epoch total loss 1.56043935\n",
      "Trained batch 1300 batch loss 1.64413846 epoch total loss 1.56050372\n",
      "Trained batch 1301 batch loss 1.53082538 epoch total loss 1.56048095\n",
      "Trained batch 1302 batch loss 1.46156073 epoch total loss 1.56040502\n",
      "Trained batch 1303 batch loss 1.38052154 epoch total loss 1.56026697\n",
      "Trained batch 1304 batch loss 1.47752655 epoch total loss 1.56020355\n",
      "Trained batch 1305 batch loss 1.49923468 epoch total loss 1.56015682\n",
      "Trained batch 1306 batch loss 1.44754148 epoch total loss 1.56007051\n",
      "Trained batch 1307 batch loss 1.2526027 epoch total loss 1.55983531\n",
      "Trained batch 1308 batch loss 1.50584602 epoch total loss 1.55979407\n",
      "Trained batch 1309 batch loss 1.51385212 epoch total loss 1.5597589\n",
      "Trained batch 1310 batch loss 1.47714567 epoch total loss 1.55969584\n",
      "Trained batch 1311 batch loss 1.45136392 epoch total loss 1.55961323\n",
      "Trained batch 1312 batch loss 1.40379095 epoch total loss 1.5594945\n",
      "Trained batch 1313 batch loss 1.36478662 epoch total loss 1.5593462\n",
      "Trained batch 1314 batch loss 1.38540769 epoch total loss 1.55921376\n",
      "Trained batch 1315 batch loss 1.36617982 epoch total loss 1.55906701\n",
      "Trained batch 1316 batch loss 1.36751616 epoch total loss 1.55892134\n",
      "Trained batch 1317 batch loss 1.29791784 epoch total loss 1.55872309\n",
      "Trained batch 1318 batch loss 1.44413424 epoch total loss 1.55863619\n",
      "Trained batch 1319 batch loss 1.37807727 epoch total loss 1.55849934\n",
      "Trained batch 1320 batch loss 1.4933672 epoch total loss 1.55845\n",
      "Trained batch 1321 batch loss 1.52081943 epoch total loss 1.55842149\n",
      "Trained batch 1322 batch loss 1.45213401 epoch total loss 1.55834115\n",
      "Trained batch 1323 batch loss 1.45889294 epoch total loss 1.55826604\n",
      "Trained batch 1324 batch loss 1.46292078 epoch total loss 1.55819392\n",
      "Trained batch 1325 batch loss 1.50341666 epoch total loss 1.55815268\n",
      "Trained batch 1326 batch loss 1.48839033 epoch total loss 1.5581\n",
      "Trained batch 1327 batch loss 1.49306667 epoch total loss 1.55805099\n",
      "Trained batch 1328 batch loss 1.55254805 epoch total loss 1.55804682\n",
      "Trained batch 1329 batch loss 1.52883112 epoch total loss 1.55802476\n",
      "Trained batch 1330 batch loss 1.42010164 epoch total loss 1.55792117\n",
      "Trained batch 1331 batch loss 1.45289016 epoch total loss 1.55784225\n",
      "Trained batch 1332 batch loss 1.65505159 epoch total loss 1.55791521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1333 batch loss 1.45354986 epoch total loss 1.55783701\n",
      "Trained batch 1334 batch loss 1.4176302 epoch total loss 1.55773199\n",
      "Trained batch 1335 batch loss 1.39499879 epoch total loss 1.55761\n",
      "Trained batch 1336 batch loss 1.40203774 epoch total loss 1.55749369\n",
      "Trained batch 1337 batch loss 1.47533572 epoch total loss 1.55743217\n",
      "Trained batch 1338 batch loss 1.40730202 epoch total loss 1.55732\n",
      "Trained batch 1339 batch loss 1.45964754 epoch total loss 1.55724704\n",
      "Trained batch 1340 batch loss 1.46003139 epoch total loss 1.55717444\n",
      "Trained batch 1341 batch loss 1.42063737 epoch total loss 1.55707264\n",
      "Trained batch 1342 batch loss 1.32246566 epoch total loss 1.55689788\n",
      "Trained batch 1343 batch loss 1.39016867 epoch total loss 1.55677366\n",
      "Trained batch 1344 batch loss 1.37281823 epoch total loss 1.55663681\n",
      "Trained batch 1345 batch loss 1.43034577 epoch total loss 1.55654299\n",
      "Trained batch 1346 batch loss 1.45418191 epoch total loss 1.55646682\n",
      "Trained batch 1347 batch loss 1.40859473 epoch total loss 1.55635715\n",
      "Trained batch 1348 batch loss 1.41900265 epoch total loss 1.55625522\n",
      "Trained batch 1349 batch loss 1.54581523 epoch total loss 1.55624759\n",
      "Trained batch 1350 batch loss 1.52957606 epoch total loss 1.5562278\n",
      "Trained batch 1351 batch loss 1.45067024 epoch total loss 1.5561496\n",
      "Trained batch 1352 batch loss 1.48614359 epoch total loss 1.55609787\n",
      "Trained batch 1353 batch loss 1.3400023 epoch total loss 1.55593812\n",
      "Trained batch 1354 batch loss 1.34438646 epoch total loss 1.55578196\n",
      "Trained batch 1355 batch loss 1.5242722 epoch total loss 1.55575871\n",
      "Trained batch 1356 batch loss 1.51828849 epoch total loss 1.55573106\n",
      "Trained batch 1357 batch loss 1.44032955 epoch total loss 1.55564606\n",
      "Trained batch 1358 batch loss 1.43798852 epoch total loss 1.5555594\n",
      "Trained batch 1359 batch loss 1.53931105 epoch total loss 1.55554748\n",
      "Trained batch 1360 batch loss 1.38350987 epoch total loss 1.55542099\n",
      "Trained batch 1361 batch loss 1.48415482 epoch total loss 1.55536866\n",
      "Trained batch 1362 batch loss 1.37590933 epoch total loss 1.55523694\n",
      "Trained batch 1363 batch loss 1.37049437 epoch total loss 1.55510139\n",
      "Trained batch 1364 batch loss 1.48922122 epoch total loss 1.55505311\n",
      "Trained batch 1365 batch loss 1.47482073 epoch total loss 1.55499446\n",
      "Trained batch 1366 batch loss 1.47946811 epoch total loss 1.55493915\n",
      "Trained batch 1367 batch loss 1.46440065 epoch total loss 1.55487287\n",
      "Trained batch 1368 batch loss 1.41913021 epoch total loss 1.55477369\n",
      "Trained batch 1369 batch loss 1.37604952 epoch total loss 1.55464315\n",
      "Trained batch 1370 batch loss 1.35616696 epoch total loss 1.5544982\n",
      "Trained batch 1371 batch loss 1.30998909 epoch total loss 1.55432\n",
      "Trained batch 1372 batch loss 1.41158295 epoch total loss 1.55421591\n",
      "Trained batch 1373 batch loss 1.37071037 epoch total loss 1.55408227\n",
      "Trained batch 1374 batch loss 1.40856588 epoch total loss 1.55397618\n",
      "Trained batch 1375 batch loss 1.44494212 epoch total loss 1.55389678\n",
      "Trained batch 1376 batch loss 1.38850379 epoch total loss 1.55377662\n",
      "Trained batch 1377 batch loss 1.38312113 epoch total loss 1.55365264\n",
      "Trained batch 1378 batch loss 1.44226599 epoch total loss 1.55357182\n",
      "Trained batch 1379 batch loss 1.46839762 epoch total loss 1.55351019\n",
      "Trained batch 1380 batch loss 1.48462629 epoch total loss 1.55346024\n",
      "Trained batch 1381 batch loss 1.43339658 epoch total loss 1.55337334\n",
      "Trained batch 1382 batch loss 1.40007317 epoch total loss 1.55326235\n",
      "Trained batch 1383 batch loss 1.41094542 epoch total loss 1.55315948\n",
      "Trained batch 1384 batch loss 1.34431076 epoch total loss 1.55300856\n",
      "Trained batch 1385 batch loss 1.35012281 epoch total loss 1.55286205\n",
      "Trained batch 1386 batch loss 1.43893695 epoch total loss 1.55277979\n",
      "Trained batch 1387 batch loss 1.35905206 epoch total loss 1.5526402\n",
      "Trained batch 1388 batch loss 1.36328924 epoch total loss 1.55250382\n",
      "Epoch 1 train loss 1.5525038242340088\n",
      "Validated batch 1 batch loss 1.38628912\n",
      "Validated batch 2 batch loss 1.38013\n",
      "Validated batch 3 batch loss 1.38206589\n",
      "Validated batch 4 batch loss 1.39809656\n",
      "Validated batch 5 batch loss 1.39207435\n",
      "Validated batch 6 batch loss 1.44818664\n",
      "Validated batch 7 batch loss 1.43299282\n",
      "Validated batch 8 batch loss 1.43633783\n",
      "Validated batch 9 batch loss 1.40939021\n",
      "Validated batch 10 batch loss 1.49261594\n",
      "Validated batch 11 batch loss 1.46829319\n",
      "Validated batch 12 batch loss 1.42926657\n",
      "Validated batch 13 batch loss 1.42503643\n",
      "Validated batch 14 batch loss 1.39541733\n",
      "Validated batch 15 batch loss 1.44175172\n",
      "Validated batch 16 batch loss 1.43599617\n",
      "Validated batch 17 batch loss 1.51911843\n",
      "Validated batch 18 batch loss 1.43622148\n",
      "Validated batch 19 batch loss 1.36291468\n",
      "Validated batch 20 batch loss 1.49318957\n",
      "Validated batch 21 batch loss 1.35501361\n",
      "Validated batch 22 batch loss 1.42905116\n",
      "Validated batch 23 batch loss 1.46409464\n",
      "Validated batch 24 batch loss 1.3252573\n",
      "Validated batch 25 batch loss 1.46658432\n",
      "Validated batch 26 batch loss 1.44341207\n",
      "Validated batch 27 batch loss 1.38657689\n",
      "Validated batch 28 batch loss 1.48829985\n",
      "Validated batch 29 batch loss 1.52145743\n",
      "Validated batch 30 batch loss 1.31622088\n",
      "Validated batch 31 batch loss 1.47864318\n",
      "Validated batch 32 batch loss 1.37418938\n",
      "Validated batch 33 batch loss 1.45506573\n",
      "Validated batch 34 batch loss 1.42054951\n",
      "Validated batch 35 batch loss 1.31104481\n",
      "Validated batch 36 batch loss 1.3009479\n",
      "Validated batch 37 batch loss 1.42536\n",
      "Validated batch 38 batch loss 1.42844009\n",
      "Validated batch 39 batch loss 1.35609221\n",
      "Validated batch 40 batch loss 1.41313744\n",
      "Validated batch 41 batch loss 1.41226637\n",
      "Validated batch 42 batch loss 1.44778013\n",
      "Validated batch 43 batch loss 1.47418523\n",
      "Validated batch 44 batch loss 1.42507625\n",
      "Validated batch 45 batch loss 1.43128109\n",
      "Validated batch 46 batch loss 1.34734523\n",
      "Validated batch 47 batch loss 1.33847618\n",
      "Validated batch 48 batch loss 1.33008075\n",
      "Validated batch 49 batch loss 1.39589787\n",
      "Validated batch 50 batch loss 1.31791\n",
      "Validated batch 51 batch loss 1.4134382\n",
      "Validated batch 52 batch loss 1.47576714\n",
      "Validated batch 53 batch loss 1.45168376\n",
      "Validated batch 54 batch loss 1.49229693\n",
      "Validated batch 55 batch loss 1.46599865\n",
      "Validated batch 56 batch loss 1.43681681\n",
      "Validated batch 57 batch loss 1.41363585\n",
      "Validated batch 58 batch loss 1.48366225\n",
      "Validated batch 59 batch loss 1.45579565\n",
      "Validated batch 60 batch loss 1.51394796\n",
      "Validated batch 61 batch loss 1.50846314\n",
      "Validated batch 62 batch loss 1.47950315\n",
      "Validated batch 63 batch loss 1.48423588\n",
      "Validated batch 64 batch loss 1.29984355\n",
      "Validated batch 65 batch loss 1.39722919\n",
      "Validated batch 66 batch loss 1.45756733\n",
      "Validated batch 67 batch loss 1.44258904\n",
      "Validated batch 68 batch loss 1.44895554\n",
      "Validated batch 69 batch loss 1.39402544\n",
      "Validated batch 70 batch loss 1.37492609\n",
      "Validated batch 71 batch loss 1.46611655\n",
      "Validated batch 72 batch loss 1.3681\n",
      "Validated batch 73 batch loss 1.28457272\n",
      "Validated batch 74 batch loss 1.35384893\n",
      "Validated batch 75 batch loss 1.50122762\n",
      "Validated batch 76 batch loss 1.34931505\n",
      "Validated batch 77 batch loss 1.36300421\n",
      "Validated batch 78 batch loss 1.40992117\n",
      "Validated batch 79 batch loss 1.44731331\n",
      "Validated batch 80 batch loss 1.35793555\n",
      "Validated batch 81 batch loss 1.42698801\n",
      "Validated batch 82 batch loss 1.44814944\n",
      "Validated batch 83 batch loss 1.39872932\n",
      "Validated batch 84 batch loss 1.46652877\n",
      "Validated batch 85 batch loss 1.53340912\n",
      "Validated batch 86 batch loss 1.38817739\n",
      "Validated batch 87 batch loss 1.51420391\n",
      "Validated batch 88 batch loss 1.36843085\n",
      "Validated batch 89 batch loss 1.36902785\n",
      "Validated batch 90 batch loss 1.4234035\n",
      "Validated batch 91 batch loss 1.43369126\n",
      "Validated batch 92 batch loss 1.545542\n",
      "Validated batch 93 batch loss 1.53620183\n",
      "Validated batch 94 batch loss 1.40450108\n",
      "Validated batch 95 batch loss 1.440938\n",
      "Validated batch 96 batch loss 1.43774796\n",
      "Validated batch 97 batch loss 1.36645544\n",
      "Validated batch 98 batch loss 1.46778011\n",
      "Validated batch 99 batch loss 1.40937686\n",
      "Validated batch 100 batch loss 1.36528754\n",
      "Validated batch 101 batch loss 1.37501478\n",
      "Validated batch 102 batch loss 1.39120185\n",
      "Validated batch 103 batch loss 1.44019866\n",
      "Validated batch 104 batch loss 1.44868326\n",
      "Validated batch 105 batch loss 1.36428249\n",
      "Validated batch 106 batch loss 1.33943045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 107 batch loss 1.35569346\n",
      "Validated batch 108 batch loss 1.42751431\n",
      "Validated batch 109 batch loss 1.41085362\n",
      "Validated batch 110 batch loss 1.46041524\n",
      "Validated batch 111 batch loss 1.54238558\n",
      "Validated batch 112 batch loss 1.6565764\n",
      "Validated batch 113 batch loss 1.56721592\n",
      "Validated batch 114 batch loss 1.43219376\n",
      "Validated batch 115 batch loss 1.34128332\n",
      "Validated batch 116 batch loss 1.4044981\n",
      "Validated batch 117 batch loss 1.4141444\n",
      "Validated batch 118 batch loss 1.38128281\n",
      "Validated batch 119 batch loss 1.37395465\n",
      "Validated batch 120 batch loss 1.3811003\n",
      "Validated batch 121 batch loss 1.52036452\n",
      "Validated batch 122 batch loss 1.35015035\n",
      "Validated batch 123 batch loss 1.44428301\n",
      "Validated batch 124 batch loss 1.40942097\n",
      "Validated batch 125 batch loss 1.43824649\n",
      "Validated batch 126 batch loss 1.39405608\n",
      "Validated batch 127 batch loss 1.33756375\n",
      "Validated batch 128 batch loss 1.45481908\n",
      "Validated batch 129 batch loss 1.51233721\n",
      "Validated batch 130 batch loss 1.42572331\n",
      "Validated batch 131 batch loss 1.38354397\n",
      "Validated batch 132 batch loss 1.42555261\n",
      "Validated batch 133 batch loss 1.33747983\n",
      "Validated batch 134 batch loss 1.32659876\n",
      "Validated batch 135 batch loss 1.40184987\n",
      "Validated batch 136 batch loss 1.32339025\n",
      "Validated batch 137 batch loss 1.39702439\n",
      "Validated batch 138 batch loss 1.42022347\n",
      "Validated batch 139 batch loss 1.45340908\n",
      "Validated batch 140 batch loss 1.47049034\n",
      "Validated batch 141 batch loss 1.38381314\n",
      "Validated batch 142 batch loss 1.39162207\n",
      "Validated batch 143 batch loss 1.47738624\n",
      "Validated batch 144 batch loss 1.44274092\n",
      "Validated batch 145 batch loss 1.49337542\n",
      "Validated batch 146 batch loss 1.50593829\n",
      "Validated batch 147 batch loss 1.46591175\n",
      "Validated batch 148 batch loss 1.46089816\n",
      "Validated batch 149 batch loss 1.50587416\n",
      "Validated batch 150 batch loss 1.53145289\n",
      "Validated batch 151 batch loss 1.37489057\n",
      "Validated batch 152 batch loss 1.46280241\n",
      "Validated batch 153 batch loss 1.46003056\n",
      "Validated batch 154 batch loss 1.43823504\n",
      "Validated batch 155 batch loss 1.51178277\n",
      "Validated batch 156 batch loss 1.35437179\n",
      "Validated batch 157 batch loss 1.32712436\n",
      "Validated batch 158 batch loss 1.38619173\n",
      "Validated batch 159 batch loss 1.35220623\n",
      "Validated batch 160 batch loss 1.56626987\n",
      "Validated batch 161 batch loss 1.36979747\n",
      "Validated batch 162 batch loss 1.4534955\n",
      "Validated batch 163 batch loss 1.51650095\n",
      "Validated batch 164 batch loss 1.35732961\n",
      "Validated batch 165 batch loss 1.36572826\n",
      "Validated batch 166 batch loss 1.44596446\n",
      "Validated batch 167 batch loss 1.38412726\n",
      "Validated batch 168 batch loss 1.39179444\n",
      "Validated batch 169 batch loss 1.3720715\n",
      "Validated batch 170 batch loss 1.34271479\n",
      "Validated batch 171 batch loss 1.53144014\n",
      "Validated batch 172 batch loss 1.40375352\n",
      "Validated batch 173 batch loss 1.33052564\n",
      "Validated batch 174 batch loss 1.34722865\n",
      "Validated batch 175 batch loss 1.53301597\n",
      "Validated batch 176 batch loss 1.44203711\n",
      "Validated batch 177 batch loss 1.47038579\n",
      "Validated batch 178 batch loss 1.37948453\n",
      "Validated batch 179 batch loss 1.52740169\n",
      "Validated batch 180 batch loss 1.42139411\n",
      "Validated batch 181 batch loss 1.48927712\n",
      "Validated batch 182 batch loss 1.50200546\n",
      "Validated batch 183 batch loss 1.25369179\n",
      "Validated batch 184 batch loss 1.40492249\n",
      "Validated batch 185 batch loss 1.59475493\n",
      "Epoch 1 val loss 1.4239978790283203\n",
      "Model /aiffel/aiffel/mpii/my_models/model-epoch-1-loss-1.4240.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.38855278 epoch total loss 1.38855278\n",
      "Trained batch 2 batch loss 1.33610284 epoch total loss 1.36232781\n",
      "Trained batch 3 batch loss 1.50524235 epoch total loss 1.40996599\n",
      "Trained batch 4 batch loss 1.43752551 epoch total loss 1.41685581\n",
      "Trained batch 5 batch loss 1.50289321 epoch total loss 1.43406332\n",
      "Trained batch 6 batch loss 1.42028499 epoch total loss 1.43176699\n",
      "Trained batch 7 batch loss 1.37357926 epoch total loss 1.4234544\n",
      "Trained batch 8 batch loss 1.42885458 epoch total loss 1.42412949\n",
      "Trained batch 9 batch loss 1.42981076 epoch total loss 1.4247607\n",
      "Trained batch 10 batch loss 1.4596982 epoch total loss 1.42825449\n",
      "Trained batch 11 batch loss 1.47672606 epoch total loss 1.43266106\n",
      "Trained batch 12 batch loss 1.33194971 epoch total loss 1.42426836\n",
      "Trained batch 13 batch loss 1.37902689 epoch total loss 1.42078829\n",
      "Trained batch 14 batch loss 1.46631074 epoch total loss 1.42403984\n",
      "Trained batch 15 batch loss 1.46239102 epoch total loss 1.42659652\n",
      "Trained batch 16 batch loss 1.45962977 epoch total loss 1.42866111\n",
      "Trained batch 17 batch loss 1.57604909 epoch total loss 1.43733108\n",
      "Trained batch 18 batch loss 1.52940476 epoch total loss 1.44244623\n",
      "Trained batch 19 batch loss 1.50643325 epoch total loss 1.44581401\n",
      "Trained batch 20 batch loss 1.52124 epoch total loss 1.44958532\n",
      "Trained batch 21 batch loss 1.37607 epoch total loss 1.44608462\n",
      "Trained batch 22 batch loss 1.45732057 epoch total loss 1.44659531\n",
      "Trained batch 23 batch loss 1.39801919 epoch total loss 1.4444834\n",
      "Trained batch 24 batch loss 1.4534831 epoch total loss 1.44485843\n",
      "Trained batch 25 batch loss 1.38174152 epoch total loss 1.4423337\n",
      "Trained batch 26 batch loss 1.39721036 epoch total loss 1.44059813\n",
      "Trained batch 27 batch loss 1.25362539 epoch total loss 1.43367314\n",
      "Trained batch 28 batch loss 1.30782807 epoch total loss 1.4291786\n",
      "Trained batch 29 batch loss 1.3449533 epoch total loss 1.4262743\n",
      "Trained batch 30 batch loss 1.3089366 epoch total loss 1.42236304\n",
      "Trained batch 31 batch loss 1.34665787 epoch total loss 1.41992092\n",
      "Trained batch 32 batch loss 1.28382099 epoch total loss 1.41566777\n",
      "Trained batch 33 batch loss 1.41210186 epoch total loss 1.41555977\n",
      "Trained batch 34 batch loss 1.37231064 epoch total loss 1.41428769\n",
      "Trained batch 35 batch loss 1.45095277 epoch total loss 1.4153353\n",
      "Trained batch 36 batch loss 1.47535563 epoch total loss 1.41700256\n",
      "Trained batch 37 batch loss 1.33389068 epoch total loss 1.4147563\n",
      "Trained batch 38 batch loss 1.41143358 epoch total loss 1.4146688\n",
      "Trained batch 39 batch loss 1.42573488 epoch total loss 1.41495264\n",
      "Trained batch 40 batch loss 1.3858645 epoch total loss 1.41422534\n",
      "Trained batch 41 batch loss 1.38610208 epoch total loss 1.41353941\n",
      "Trained batch 42 batch loss 1.5008539 epoch total loss 1.4156183\n",
      "Trained batch 43 batch loss 1.54084516 epoch total loss 1.41853058\n",
      "Trained batch 44 batch loss 1.44516063 epoch total loss 1.41913581\n",
      "Trained batch 45 batch loss 1.51483643 epoch total loss 1.4212625\n",
      "Trained batch 46 batch loss 1.46854711 epoch total loss 1.42229033\n",
      "Trained batch 47 batch loss 1.48453414 epoch total loss 1.42361462\n",
      "Trained batch 48 batch loss 1.60582483 epoch total loss 1.42741072\n",
      "Trained batch 49 batch loss 1.67955732 epoch total loss 1.43255663\n",
      "Trained batch 50 batch loss 1.57522345 epoch total loss 1.43541\n",
      "Trained batch 51 batch loss 1.32597899 epoch total loss 1.43326437\n",
      "Trained batch 52 batch loss 1.37721956 epoch total loss 1.4321866\n",
      "Trained batch 53 batch loss 1.19285667 epoch total loss 1.42767084\n",
      "Trained batch 54 batch loss 1.33245277 epoch total loss 1.42590761\n",
      "Trained batch 55 batch loss 1.34292591 epoch total loss 1.42439878\n",
      "Trained batch 56 batch loss 1.23978639 epoch total loss 1.42110217\n",
      "Trained batch 57 batch loss 1.16431165 epoch total loss 1.41659701\n",
      "Trained batch 58 batch loss 1.17765176 epoch total loss 1.41247725\n",
      "Trained batch 59 batch loss 1.25177491 epoch total loss 1.40975356\n",
      "Trained batch 60 batch loss 1.36973798 epoch total loss 1.40908659\n",
      "Trained batch 61 batch loss 1.53359413 epoch total loss 1.41112769\n",
      "Trained batch 62 batch loss 1.46729946 epoch total loss 1.41203368\n",
      "Trained batch 63 batch loss 1.51754618 epoch total loss 1.41370857\n",
      "Trained batch 64 batch loss 1.52630782 epoch total loss 1.41546786\n",
      "Trained batch 65 batch loss 1.51589596 epoch total loss 1.41701293\n",
      "Trained batch 66 batch loss 1.50608468 epoch total loss 1.41836262\n",
      "Trained batch 67 batch loss 1.36611474 epoch total loss 1.41758275\n",
      "Trained batch 68 batch loss 1.51714861 epoch total loss 1.419047\n",
      "Trained batch 69 batch loss 1.43242764 epoch total loss 1.41924083\n",
      "Trained batch 70 batch loss 1.40252709 epoch total loss 1.41900206\n",
      "Trained batch 71 batch loss 1.43751371 epoch total loss 1.41926289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 72 batch loss 1.44066191 epoch total loss 1.41956\n",
      "Trained batch 73 batch loss 1.43066275 epoch total loss 1.41971207\n",
      "Trained batch 74 batch loss 1.29450798 epoch total loss 1.41802025\n",
      "Trained batch 75 batch loss 1.40010297 epoch total loss 1.41778123\n",
      "Trained batch 76 batch loss 1.47755694 epoch total loss 1.41856778\n",
      "Trained batch 77 batch loss 1.47567534 epoch total loss 1.4193095\n",
      "Trained batch 78 batch loss 1.4225204 epoch total loss 1.41935062\n",
      "Trained batch 79 batch loss 1.38790369 epoch total loss 1.41895258\n",
      "Trained batch 80 batch loss 1.47087955 epoch total loss 1.41960168\n",
      "Trained batch 81 batch loss 1.41392601 epoch total loss 1.41953158\n",
      "Trained batch 82 batch loss 1.42693567 epoch total loss 1.41962183\n",
      "Trained batch 83 batch loss 1.30104089 epoch total loss 1.4181931\n",
      "Trained batch 84 batch loss 1.35302067 epoch total loss 1.41741729\n",
      "Trained batch 85 batch loss 1.48854458 epoch total loss 1.41825414\n",
      "Trained batch 86 batch loss 1.46489096 epoch total loss 1.41879642\n",
      "Trained batch 87 batch loss 1.45973969 epoch total loss 1.41926694\n",
      "Trained batch 88 batch loss 1.41045797 epoch total loss 1.41916692\n",
      "Trained batch 89 batch loss 1.35124493 epoch total loss 1.41840374\n",
      "Trained batch 90 batch loss 1.50112605 epoch total loss 1.41932285\n",
      "Trained batch 91 batch loss 1.49457586 epoch total loss 1.42014992\n",
      "Trained batch 92 batch loss 1.33357811 epoch total loss 1.41920888\n",
      "Trained batch 93 batch loss 1.27995408 epoch total loss 1.4177115\n",
      "Trained batch 94 batch loss 1.30202127 epoch total loss 1.41648066\n",
      "Trained batch 95 batch loss 1.26963353 epoch total loss 1.41493499\n",
      "Trained batch 96 batch loss 1.40930438 epoch total loss 1.41487634\n",
      "Trained batch 97 batch loss 1.47013021 epoch total loss 1.4154458\n",
      "Trained batch 98 batch loss 1.47557986 epoch total loss 1.41605949\n",
      "Trained batch 99 batch loss 1.3281883 epoch total loss 1.41517198\n",
      "Trained batch 100 batch loss 1.34145272 epoch total loss 1.41443467\n",
      "Trained batch 101 batch loss 1.40677 epoch total loss 1.41435874\n",
      "Trained batch 102 batch loss 1.49477863 epoch total loss 1.41514719\n",
      "Trained batch 103 batch loss 1.44840014 epoch total loss 1.41547\n",
      "Trained batch 104 batch loss 1.39383161 epoch total loss 1.41526198\n",
      "Trained batch 105 batch loss 1.35256636 epoch total loss 1.41466486\n",
      "Trained batch 106 batch loss 1.3143518 epoch total loss 1.41371846\n",
      "Trained batch 107 batch loss 1.2594341 epoch total loss 1.41227651\n",
      "Trained batch 108 batch loss 1.34350586 epoch total loss 1.41163969\n",
      "Trained batch 109 batch loss 1.40182877 epoch total loss 1.41154969\n",
      "Trained batch 110 batch loss 1.39575279 epoch total loss 1.41140604\n",
      "Trained batch 111 batch loss 1.42195058 epoch total loss 1.41150105\n",
      "Trained batch 112 batch loss 1.37089956 epoch total loss 1.41113853\n",
      "Trained batch 113 batch loss 1.340204 epoch total loss 1.4105109\n",
      "Trained batch 114 batch loss 1.31959689 epoch total loss 1.40971339\n",
      "Trained batch 115 batch loss 1.33445966 epoch total loss 1.40905893\n",
      "Trained batch 116 batch loss 1.53866076 epoch total loss 1.41017628\n",
      "Trained batch 117 batch loss 1.50563848 epoch total loss 1.41099226\n",
      "Trained batch 118 batch loss 1.68784249 epoch total loss 1.4133383\n",
      "Trained batch 119 batch loss 1.57488799 epoch total loss 1.41469598\n",
      "Trained batch 120 batch loss 1.51412654 epoch total loss 1.4155246\n",
      "Trained batch 121 batch loss 1.45334554 epoch total loss 1.41583705\n",
      "Trained batch 122 batch loss 1.38356078 epoch total loss 1.41557252\n",
      "Trained batch 123 batch loss 1.42132044 epoch total loss 1.41561925\n",
      "Trained batch 124 batch loss 1.36005843 epoch total loss 1.41517127\n",
      "Trained batch 125 batch loss 1.46811223 epoch total loss 1.4155947\n",
      "Trained batch 126 batch loss 1.49965584 epoch total loss 1.41626179\n",
      "Trained batch 127 batch loss 1.49123132 epoch total loss 1.41685212\n",
      "Trained batch 128 batch loss 1.4432621 epoch total loss 1.41705847\n",
      "Trained batch 129 batch loss 1.4091568 epoch total loss 1.41699719\n",
      "Trained batch 130 batch loss 1.39615858 epoch total loss 1.41683686\n",
      "Trained batch 131 batch loss 1.44083405 epoch total loss 1.4170202\n",
      "Trained batch 132 batch loss 1.32614565 epoch total loss 1.41633165\n",
      "Trained batch 133 batch loss 1.31039464 epoch total loss 1.41553509\n",
      "Trained batch 134 batch loss 1.41093493 epoch total loss 1.41550076\n",
      "Trained batch 135 batch loss 1.46379399 epoch total loss 1.41585851\n",
      "Trained batch 136 batch loss 1.49752116 epoch total loss 1.41645908\n",
      "Trained batch 137 batch loss 1.42676163 epoch total loss 1.41653419\n",
      "Trained batch 138 batch loss 1.42735577 epoch total loss 1.41661263\n",
      "Trained batch 139 batch loss 1.44498265 epoch total loss 1.41681671\n",
      "Trained batch 140 batch loss 1.50373578 epoch total loss 1.41743755\n",
      "Trained batch 141 batch loss 1.53685403 epoch total loss 1.41828442\n",
      "Trained batch 142 batch loss 1.37452626 epoch total loss 1.41797626\n",
      "Trained batch 143 batch loss 1.39224577 epoch total loss 1.41779637\n",
      "Trained batch 144 batch loss 1.40305078 epoch total loss 1.41769385\n",
      "Trained batch 145 batch loss 1.3414197 epoch total loss 1.41716778\n",
      "Trained batch 146 batch loss 1.31632733 epoch total loss 1.4164772\n",
      "Trained batch 147 batch loss 1.50381494 epoch total loss 1.41707134\n",
      "Trained batch 148 batch loss 1.45353365 epoch total loss 1.41731763\n",
      "Trained batch 149 batch loss 1.39238071 epoch total loss 1.41715026\n",
      "Trained batch 150 batch loss 1.34683132 epoch total loss 1.41668153\n",
      "Trained batch 151 batch loss 1.38866484 epoch total loss 1.41649604\n",
      "Trained batch 152 batch loss 1.40254736 epoch total loss 1.41640425\n",
      "Trained batch 153 batch loss 1.44291639 epoch total loss 1.41657746\n",
      "Trained batch 154 batch loss 1.44348443 epoch total loss 1.41675222\n",
      "Trained batch 155 batch loss 1.31309938 epoch total loss 1.41608346\n",
      "Trained batch 156 batch loss 1.32689941 epoch total loss 1.41551185\n",
      "Trained batch 157 batch loss 1.4323349 epoch total loss 1.4156189\n",
      "Trained batch 158 batch loss 1.36384892 epoch total loss 1.41529119\n",
      "Trained batch 159 batch loss 1.32144153 epoch total loss 1.41470098\n",
      "Trained batch 160 batch loss 1.42493749 epoch total loss 1.414765\n",
      "Trained batch 161 batch loss 1.34042728 epoch total loss 1.41430318\n",
      "Trained batch 162 batch loss 1.40001273 epoch total loss 1.41421497\n",
      "Trained batch 163 batch loss 1.47131848 epoch total loss 1.41456532\n",
      "Trained batch 164 batch loss 1.50760388 epoch total loss 1.41513252\n",
      "Trained batch 165 batch loss 1.54445052 epoch total loss 1.41591632\n",
      "Trained batch 166 batch loss 1.6080426 epoch total loss 1.41707373\n",
      "Trained batch 167 batch loss 1.55457509 epoch total loss 1.41789711\n",
      "Trained batch 168 batch loss 1.54239678 epoch total loss 1.41863823\n",
      "Trained batch 169 batch loss 1.31939626 epoch total loss 1.418051\n",
      "Trained batch 170 batch loss 1.39585781 epoch total loss 1.41792047\n",
      "Trained batch 171 batch loss 1.58493948 epoch total loss 1.41889727\n",
      "Trained batch 172 batch loss 1.57409906 epoch total loss 1.41979957\n",
      "Trained batch 173 batch loss 1.54113817 epoch total loss 1.42050087\n",
      "Trained batch 174 batch loss 1.57539511 epoch total loss 1.42139113\n",
      "Trained batch 175 batch loss 1.6263206 epoch total loss 1.42256212\n",
      "Trained batch 176 batch loss 1.50853789 epoch total loss 1.42305076\n",
      "Trained batch 177 batch loss 1.43258953 epoch total loss 1.42310452\n",
      "Trained batch 178 batch loss 1.41873646 epoch total loss 1.42308\n",
      "Trained batch 179 batch loss 1.43052924 epoch total loss 1.42312157\n",
      "Trained batch 180 batch loss 1.44551635 epoch total loss 1.42324615\n",
      "Trained batch 181 batch loss 1.3768034 epoch total loss 1.42298949\n",
      "Trained batch 182 batch loss 1.43647671 epoch total loss 1.42306352\n",
      "Trained batch 183 batch loss 1.58828 epoch total loss 1.42396641\n",
      "Trained batch 184 batch loss 1.38799238 epoch total loss 1.4237709\n",
      "Trained batch 185 batch loss 1.3597002 epoch total loss 1.4234246\n",
      "Trained batch 186 batch loss 1.52126193 epoch total loss 1.42395067\n",
      "Trained batch 187 batch loss 1.35728967 epoch total loss 1.42359424\n",
      "Trained batch 188 batch loss 1.40231502 epoch total loss 1.42348111\n",
      "Trained batch 189 batch loss 1.44320142 epoch total loss 1.42358541\n",
      "Trained batch 190 batch loss 1.43533933 epoch total loss 1.42364728\n",
      "Trained batch 191 batch loss 1.33916318 epoch total loss 1.42320502\n",
      "Trained batch 192 batch loss 1.37732697 epoch total loss 1.422966\n",
      "Trained batch 193 batch loss 1.35284412 epoch total loss 1.42260265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 194 batch loss 1.3524127 epoch total loss 1.42224085\n",
      "Trained batch 195 batch loss 1.48370528 epoch total loss 1.42255604\n",
      "Trained batch 196 batch loss 1.40515864 epoch total loss 1.42246723\n",
      "Trained batch 197 batch loss 1.48322296 epoch total loss 1.42277563\n",
      "Trained batch 198 batch loss 1.49990368 epoch total loss 1.4231652\n",
      "Trained batch 199 batch loss 1.44373214 epoch total loss 1.42326856\n",
      "Trained batch 200 batch loss 1.47059226 epoch total loss 1.42350507\n",
      "Trained batch 201 batch loss 1.44132829 epoch total loss 1.42359376\n",
      "Trained batch 202 batch loss 1.34943581 epoch total loss 1.42322659\n",
      "Trained batch 203 batch loss 1.4204272 epoch total loss 1.42321277\n",
      "Trained batch 204 batch loss 1.55536032 epoch total loss 1.42386055\n",
      "Trained batch 205 batch loss 1.43014872 epoch total loss 1.42389119\n",
      "Trained batch 206 batch loss 1.34406841 epoch total loss 1.42350364\n",
      "Trained batch 207 batch loss 1.36780548 epoch total loss 1.42323458\n",
      "Trained batch 208 batch loss 1.49420691 epoch total loss 1.42357576\n",
      "Trained batch 209 batch loss 1.45048785 epoch total loss 1.42370462\n",
      "Trained batch 210 batch loss 1.45423961 epoch total loss 1.42385\n",
      "Trained batch 211 batch loss 1.40450048 epoch total loss 1.42375839\n",
      "Trained batch 212 batch loss 1.36474931 epoch total loss 1.42348\n",
      "Trained batch 213 batch loss 1.46111727 epoch total loss 1.42365682\n",
      "Trained batch 214 batch loss 1.49387109 epoch total loss 1.42398489\n",
      "Trained batch 215 batch loss 1.49611676 epoch total loss 1.42432034\n",
      "Trained batch 216 batch loss 1.45887351 epoch total loss 1.42448032\n",
      "Trained batch 217 batch loss 1.32576454 epoch total loss 1.42402542\n",
      "Trained batch 218 batch loss 1.40267754 epoch total loss 1.42392755\n",
      "Trained batch 219 batch loss 1.3446312 epoch total loss 1.42356551\n",
      "Trained batch 220 batch loss 1.35872388 epoch total loss 1.42327082\n",
      "Trained batch 221 batch loss 1.47238421 epoch total loss 1.42349303\n",
      "Trained batch 222 batch loss 1.47757626 epoch total loss 1.42373657\n",
      "Trained batch 223 batch loss 1.33378 epoch total loss 1.42333317\n",
      "Trained batch 224 batch loss 1.43806875 epoch total loss 1.42339897\n",
      "Trained batch 225 batch loss 1.49152076 epoch total loss 1.42370176\n",
      "Trained batch 226 batch loss 1.47946632 epoch total loss 1.42394841\n",
      "Trained batch 227 batch loss 1.51936722 epoch total loss 1.42436886\n",
      "Trained batch 228 batch loss 1.45790243 epoch total loss 1.42451596\n",
      "Trained batch 229 batch loss 1.45621467 epoch total loss 1.42465436\n",
      "Trained batch 230 batch loss 1.43110549 epoch total loss 1.42468238\n",
      "Trained batch 231 batch loss 1.52034 epoch total loss 1.42509639\n",
      "Trained batch 232 batch loss 1.51391912 epoch total loss 1.42547917\n",
      "Trained batch 233 batch loss 1.41261244 epoch total loss 1.42542398\n",
      "Trained batch 234 batch loss 1.45504296 epoch total loss 1.42555058\n",
      "Trained batch 235 batch loss 1.49256182 epoch total loss 1.42583561\n",
      "Trained batch 236 batch loss 1.62737954 epoch total loss 1.42668962\n",
      "Trained batch 237 batch loss 1.45646405 epoch total loss 1.42681527\n",
      "Trained batch 238 batch loss 1.40587056 epoch total loss 1.42672729\n",
      "Trained batch 239 batch loss 1.46264541 epoch total loss 1.42687762\n",
      "Trained batch 240 batch loss 1.44558716 epoch total loss 1.42695558\n",
      "Trained batch 241 batch loss 1.43548036 epoch total loss 1.42699099\n",
      "Trained batch 242 batch loss 1.48977804 epoch total loss 1.42725039\n",
      "Trained batch 243 batch loss 1.4619199 epoch total loss 1.42739308\n",
      "Trained batch 244 batch loss 1.50191545 epoch total loss 1.42769849\n",
      "Trained batch 245 batch loss 1.42833173 epoch total loss 1.42770112\n",
      "Trained batch 246 batch loss 1.41725039 epoch total loss 1.42765856\n",
      "Trained batch 247 batch loss 1.47965324 epoch total loss 1.42786908\n",
      "Trained batch 248 batch loss 1.4101727 epoch total loss 1.42779779\n",
      "Trained batch 249 batch loss 1.39951336 epoch total loss 1.42768407\n",
      "Trained batch 250 batch loss 1.42185104 epoch total loss 1.42766082\n",
      "Trained batch 251 batch loss 1.41104543 epoch total loss 1.42759454\n",
      "Trained batch 252 batch loss 1.35730636 epoch total loss 1.42731559\n",
      "Trained batch 253 batch loss 1.36957169 epoch total loss 1.42708731\n",
      "Trained batch 254 batch loss 1.35031438 epoch total loss 1.42678511\n",
      "Trained batch 255 batch loss 1.41635036 epoch total loss 1.42674422\n",
      "Trained batch 256 batch loss 1.40607429 epoch total loss 1.4266634\n",
      "Trained batch 257 batch loss 1.43395925 epoch total loss 1.42669177\n",
      "Trained batch 258 batch loss 1.36664379 epoch total loss 1.42645907\n",
      "Trained batch 259 batch loss 1.48106563 epoch total loss 1.42667\n",
      "Trained batch 260 batch loss 1.46058345 epoch total loss 1.42680025\n",
      "Trained batch 261 batch loss 1.42652452 epoch total loss 1.42679918\n",
      "Trained batch 262 batch loss 1.47166955 epoch total loss 1.42697048\n",
      "Trained batch 263 batch loss 1.48534989 epoch total loss 1.42719245\n",
      "Trained batch 264 batch loss 1.52916944 epoch total loss 1.42757881\n",
      "Trained batch 265 batch loss 1.56965041 epoch total loss 1.42811489\n",
      "Trained batch 266 batch loss 1.63489366 epoch total loss 1.42889225\n",
      "Trained batch 267 batch loss 1.63028848 epoch total loss 1.42964649\n",
      "Trained batch 268 batch loss 1.36238015 epoch total loss 1.42939544\n",
      "Trained batch 269 batch loss 1.41057301 epoch total loss 1.42932546\n",
      "Trained batch 270 batch loss 1.3633374 epoch total loss 1.42908108\n",
      "Trained batch 271 batch loss 1.36828876 epoch total loss 1.42885673\n",
      "Trained batch 272 batch loss 1.36090565 epoch total loss 1.42860699\n",
      "Trained batch 273 batch loss 1.37368155 epoch total loss 1.42840576\n",
      "Trained batch 274 batch loss 1.39560735 epoch total loss 1.42828608\n",
      "Trained batch 275 batch loss 1.39181125 epoch total loss 1.4281534\n",
      "Trained batch 276 batch loss 1.31043565 epoch total loss 1.42772686\n",
      "Trained batch 277 batch loss 1.34727788 epoch total loss 1.42743647\n",
      "Trained batch 278 batch loss 1.39339077 epoch total loss 1.42731404\n",
      "Trained batch 279 batch loss 1.35190642 epoch total loss 1.4270438\n",
      "Trained batch 280 batch loss 1.3372066 epoch total loss 1.42672288\n",
      "Trained batch 281 batch loss 1.35845697 epoch total loss 1.42648\n",
      "Trained batch 282 batch loss 1.31941223 epoch total loss 1.42610025\n",
      "Trained batch 283 batch loss 1.35347068 epoch total loss 1.42584372\n",
      "Trained batch 284 batch loss 1.37514031 epoch total loss 1.42566514\n",
      "Trained batch 285 batch loss 1.35941446 epoch total loss 1.42543268\n",
      "Trained batch 286 batch loss 1.40969992 epoch total loss 1.42537773\n",
      "Trained batch 287 batch loss 1.36505461 epoch total loss 1.42516744\n",
      "Trained batch 288 batch loss 1.32396197 epoch total loss 1.42481613\n",
      "Trained batch 289 batch loss 1.48785734 epoch total loss 1.42503428\n",
      "Trained batch 290 batch loss 1.42765558 epoch total loss 1.42504323\n",
      "Trained batch 291 batch loss 1.37825823 epoch total loss 1.42488253\n",
      "Trained batch 292 batch loss 1.44971097 epoch total loss 1.42496753\n",
      "Trained batch 293 batch loss 1.47464323 epoch total loss 1.42513704\n",
      "Trained batch 294 batch loss 1.4206984 epoch total loss 1.4251219\n",
      "Trained batch 295 batch loss 1.45606518 epoch total loss 1.42522681\n",
      "Trained batch 296 batch loss 1.35299659 epoch total loss 1.42498279\n",
      "Trained batch 297 batch loss 1.44407845 epoch total loss 1.42504704\n",
      "Trained batch 298 batch loss 1.54555941 epoch total loss 1.42545152\n",
      "Trained batch 299 batch loss 1.49469543 epoch total loss 1.42568302\n",
      "Trained batch 300 batch loss 1.49536872 epoch total loss 1.42591536\n",
      "Trained batch 301 batch loss 1.4611156 epoch total loss 1.4260323\n",
      "Trained batch 302 batch loss 1.35600591 epoch total loss 1.42580044\n",
      "Trained batch 303 batch loss 1.49141037 epoch total loss 1.42601705\n",
      "Trained batch 304 batch loss 1.46517301 epoch total loss 1.42614591\n",
      "Trained batch 305 batch loss 1.33575296 epoch total loss 1.42584944\n",
      "Trained batch 306 batch loss 1.53539956 epoch total loss 1.42620754\n",
      "Trained batch 307 batch loss 1.37800133 epoch total loss 1.42605042\n",
      "Trained batch 308 batch loss 1.51113951 epoch total loss 1.42632675\n",
      "Trained batch 309 batch loss 1.55800915 epoch total loss 1.42675292\n",
      "Trained batch 310 batch loss 1.52991807 epoch total loss 1.42708564\n",
      "Trained batch 311 batch loss 1.35075784 epoch total loss 1.42684019\n",
      "Trained batch 312 batch loss 1.3860321 epoch total loss 1.42670953\n",
      "Trained batch 313 batch loss 1.36745465 epoch total loss 1.42652023\n",
      "Trained batch 314 batch loss 1.39147186 epoch total loss 1.42640865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 315 batch loss 1.43915129 epoch total loss 1.42644906\n",
      "Trained batch 316 batch loss 1.47864223 epoch total loss 1.42661417\n",
      "Trained batch 317 batch loss 1.54478836 epoch total loss 1.42698705\n",
      "Trained batch 318 batch loss 1.51623273 epoch total loss 1.42726767\n",
      "Trained batch 319 batch loss 1.59098458 epoch total loss 1.42778087\n",
      "Trained batch 320 batch loss 1.57548261 epoch total loss 1.42824244\n",
      "Trained batch 321 batch loss 1.55903888 epoch total loss 1.4286499\n",
      "Trained batch 322 batch loss 1.52659988 epoch total loss 1.42895412\n",
      "Trained batch 323 batch loss 1.48269153 epoch total loss 1.42912054\n",
      "Trained batch 324 batch loss 1.52109659 epoch total loss 1.42940438\n",
      "Trained batch 325 batch loss 1.45557547 epoch total loss 1.42948484\n",
      "Trained batch 326 batch loss 1.48374844 epoch total loss 1.42965126\n",
      "Trained batch 327 batch loss 1.46835196 epoch total loss 1.42976964\n",
      "Trained batch 328 batch loss 1.4219203 epoch total loss 1.42974567\n",
      "Trained batch 329 batch loss 1.43829012 epoch total loss 1.42977166\n",
      "Trained batch 330 batch loss 1.48815227 epoch total loss 1.42994857\n",
      "Trained batch 331 batch loss 1.4805181 epoch total loss 1.43010139\n",
      "Trained batch 332 batch loss 1.44069874 epoch total loss 1.43013334\n",
      "Trained batch 333 batch loss 1.35665834 epoch total loss 1.42991269\n",
      "Trained batch 334 batch loss 1.58322656 epoch total loss 1.43037164\n",
      "Trained batch 335 batch loss 1.47314835 epoch total loss 1.43049932\n",
      "Trained batch 336 batch loss 1.41579843 epoch total loss 1.43045557\n",
      "Trained batch 337 batch loss 1.4051646 epoch total loss 1.43038058\n",
      "Trained batch 338 batch loss 1.32893097 epoch total loss 1.43008029\n",
      "Trained batch 339 batch loss 1.38956738 epoch total loss 1.42996085\n",
      "Trained batch 340 batch loss 1.31516051 epoch total loss 1.42962313\n",
      "Trained batch 341 batch loss 1.44565439 epoch total loss 1.4296701\n",
      "Trained batch 342 batch loss 1.4412756 epoch total loss 1.42970407\n",
      "Trained batch 343 batch loss 1.29666018 epoch total loss 1.42931616\n",
      "Trained batch 344 batch loss 1.32377172 epoch total loss 1.42900932\n",
      "Trained batch 345 batch loss 1.31526172 epoch total loss 1.42867959\n",
      "Trained batch 346 batch loss 1.41809881 epoch total loss 1.42864907\n",
      "Trained batch 347 batch loss 1.31237388 epoch total loss 1.42831397\n",
      "Trained batch 348 batch loss 1.52670598 epoch total loss 1.42859662\n",
      "Trained batch 349 batch loss 1.36870623 epoch total loss 1.42842507\n",
      "Trained batch 350 batch loss 1.32592058 epoch total loss 1.4281323\n",
      "Trained batch 351 batch loss 1.35094059 epoch total loss 1.42791235\n",
      "Trained batch 352 batch loss 1.33063638 epoch total loss 1.42763603\n",
      "Trained batch 353 batch loss 1.38097095 epoch total loss 1.42750382\n",
      "Trained batch 354 batch loss 1.30545211 epoch total loss 1.42715907\n",
      "Trained batch 355 batch loss 1.22070181 epoch total loss 1.42657745\n",
      "Trained batch 356 batch loss 1.17900491 epoch total loss 1.4258821\n",
      "Trained batch 357 batch loss 1.29051113 epoch total loss 1.42550278\n",
      "Trained batch 358 batch loss 1.24631596 epoch total loss 1.42500234\n",
      "Trained batch 359 batch loss 1.19878161 epoch total loss 1.4243722\n",
      "Trained batch 360 batch loss 1.3702035 epoch total loss 1.42422163\n",
      "Trained batch 361 batch loss 1.3450284 epoch total loss 1.42400229\n",
      "Trained batch 362 batch loss 1.34687638 epoch total loss 1.42378914\n",
      "Trained batch 363 batch loss 1.30984378 epoch total loss 1.42347515\n",
      "Trained batch 364 batch loss 1.28317833 epoch total loss 1.42308986\n",
      "Trained batch 365 batch loss 1.33809781 epoch total loss 1.42285693\n",
      "Trained batch 366 batch loss 1.49096906 epoch total loss 1.42304301\n",
      "Trained batch 367 batch loss 1.44902301 epoch total loss 1.42311382\n",
      "Trained batch 368 batch loss 1.38548386 epoch total loss 1.42301166\n",
      "Trained batch 369 batch loss 1.38726532 epoch total loss 1.42291474\n",
      "Trained batch 370 batch loss 1.42586124 epoch total loss 1.42292261\n",
      "Trained batch 371 batch loss 1.36489224 epoch total loss 1.42276621\n",
      "Trained batch 372 batch loss 1.32247686 epoch total loss 1.42249656\n",
      "Trained batch 373 batch loss 1.37023711 epoch total loss 1.42235637\n",
      "Trained batch 374 batch loss 1.45731425 epoch total loss 1.42245\n",
      "Trained batch 375 batch loss 1.70256877 epoch total loss 1.42319691\n",
      "Trained batch 376 batch loss 1.51128614 epoch total loss 1.42343128\n",
      "Trained batch 377 batch loss 1.55486226 epoch total loss 1.42377985\n",
      "Trained batch 378 batch loss 1.57654488 epoch total loss 1.42418396\n",
      "Trained batch 379 batch loss 1.47429109 epoch total loss 1.42431629\n",
      "Trained batch 380 batch loss 1.42220545 epoch total loss 1.42431056\n",
      "Trained batch 381 batch loss 1.49200249 epoch total loss 1.42448831\n",
      "Trained batch 382 batch loss 1.44576478 epoch total loss 1.42454398\n",
      "Trained batch 383 batch loss 1.41573334 epoch total loss 1.42452085\n",
      "Trained batch 384 batch loss 1.39487791 epoch total loss 1.42444372\n",
      "Trained batch 385 batch loss 1.44732487 epoch total loss 1.42450321\n",
      "Trained batch 386 batch loss 1.47466755 epoch total loss 1.42463315\n",
      "Trained batch 387 batch loss 1.44371891 epoch total loss 1.4246825\n",
      "Trained batch 388 batch loss 1.3300693 epoch total loss 1.4244386\n",
      "Trained batch 389 batch loss 1.36173606 epoch total loss 1.42427754\n",
      "Trained batch 390 batch loss 1.24184167 epoch total loss 1.42380965\n",
      "Trained batch 391 batch loss 1.23462427 epoch total loss 1.42332578\n",
      "Trained batch 392 batch loss 1.3430208 epoch total loss 1.42312098\n",
      "Trained batch 393 batch loss 1.52840006 epoch total loss 1.42338872\n",
      "Trained batch 394 batch loss 1.54248405 epoch total loss 1.42369103\n",
      "Trained batch 395 batch loss 1.45498478 epoch total loss 1.42377019\n",
      "Trained batch 396 batch loss 1.47275841 epoch total loss 1.42389393\n",
      "Trained batch 397 batch loss 1.48287845 epoch total loss 1.42404246\n",
      "Trained batch 398 batch loss 1.30621278 epoch total loss 1.42374635\n",
      "Trained batch 399 batch loss 1.30992913 epoch total loss 1.4234612\n",
      "Trained batch 400 batch loss 1.38632488 epoch total loss 1.42336833\n",
      "Trained batch 401 batch loss 1.47032189 epoch total loss 1.42348552\n",
      "Trained batch 402 batch loss 1.42648149 epoch total loss 1.42349291\n",
      "Trained batch 403 batch loss 1.45082569 epoch total loss 1.42356062\n",
      "Trained batch 404 batch loss 1.41094661 epoch total loss 1.42352939\n",
      "Trained batch 405 batch loss 1.40336907 epoch total loss 1.42347968\n",
      "Trained batch 406 batch loss 1.53818285 epoch total loss 1.42376232\n",
      "Trained batch 407 batch loss 1.44082046 epoch total loss 1.42380416\n",
      "Trained batch 408 batch loss 1.58191574 epoch total loss 1.42419159\n",
      "Trained batch 409 batch loss 1.47462857 epoch total loss 1.42431486\n",
      "Trained batch 410 batch loss 1.52306509 epoch total loss 1.42455578\n",
      "Trained batch 411 batch loss 1.52736378 epoch total loss 1.42480588\n",
      "Trained batch 412 batch loss 1.36448574 epoch total loss 1.42465949\n",
      "Trained batch 413 batch loss 1.47915149 epoch total loss 1.42479134\n",
      "Trained batch 414 batch loss 1.3306303 epoch total loss 1.42456388\n",
      "Trained batch 415 batch loss 1.22093201 epoch total loss 1.42407334\n",
      "Trained batch 416 batch loss 1.24099517 epoch total loss 1.4236331\n",
      "Trained batch 417 batch loss 1.20012355 epoch total loss 1.42309713\n",
      "Trained batch 418 batch loss 1.42451453 epoch total loss 1.42310047\n",
      "Trained batch 419 batch loss 1.45858145 epoch total loss 1.42318511\n",
      "Trained batch 420 batch loss 1.49888182 epoch total loss 1.42336547\n",
      "Trained batch 421 batch loss 1.51253808 epoch total loss 1.42357719\n",
      "Trained batch 422 batch loss 1.42172563 epoch total loss 1.4235729\n",
      "Trained batch 423 batch loss 1.34356403 epoch total loss 1.42338371\n",
      "Trained batch 424 batch loss 1.45995164 epoch total loss 1.42347\n",
      "Trained batch 425 batch loss 1.51832914 epoch total loss 1.42369318\n",
      "Trained batch 426 batch loss 1.56314349 epoch total loss 1.42402053\n",
      "Trained batch 427 batch loss 1.45090914 epoch total loss 1.42408359\n",
      "Trained batch 428 batch loss 1.56275892 epoch total loss 1.42440748\n",
      "Trained batch 429 batch loss 1.60266829 epoch total loss 1.42482305\n",
      "Trained batch 430 batch loss 1.48880124 epoch total loss 1.42497194\n",
      "Trained batch 431 batch loss 1.38175488 epoch total loss 1.42487168\n",
      "Trained batch 432 batch loss 1.38265884 epoch total loss 1.42477393\n",
      "Trained batch 433 batch loss 1.29729652 epoch total loss 1.42447948\n",
      "Trained batch 434 batch loss 1.21049047 epoch total loss 1.42398643\n",
      "Trained batch 435 batch loss 1.17068624 epoch total loss 1.42340422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 436 batch loss 1.19287944 epoch total loss 1.42287552\n",
      "Trained batch 437 batch loss 1.2714541 epoch total loss 1.4225291\n",
      "Trained batch 438 batch loss 1.53685737 epoch total loss 1.42279\n",
      "Trained batch 439 batch loss 1.72672629 epoch total loss 1.42348254\n",
      "Trained batch 440 batch loss 1.45995855 epoch total loss 1.42356539\n",
      "Trained batch 441 batch loss 1.35745478 epoch total loss 1.42341554\n",
      "Trained batch 442 batch loss 1.43822873 epoch total loss 1.42344904\n",
      "Trained batch 443 batch loss 1.51020074 epoch total loss 1.4236449\n",
      "Trained batch 444 batch loss 1.51223445 epoch total loss 1.42384434\n",
      "Trained batch 445 batch loss 1.51077902 epoch total loss 1.42403972\n",
      "Trained batch 446 batch loss 1.40277052 epoch total loss 1.42399204\n",
      "Trained batch 447 batch loss 1.47247982 epoch total loss 1.42410052\n",
      "Trained batch 448 batch loss 1.45391238 epoch total loss 1.42416704\n",
      "Trained batch 449 batch loss 1.4643569 epoch total loss 1.42425656\n",
      "Trained batch 450 batch loss 1.28205502 epoch total loss 1.42394054\n",
      "Trained batch 451 batch loss 1.39414728 epoch total loss 1.4238745\n",
      "Trained batch 452 batch loss 1.40671062 epoch total loss 1.42383659\n",
      "Trained batch 453 batch loss 1.42275381 epoch total loss 1.4238342\n",
      "Trained batch 454 batch loss 1.40706933 epoch total loss 1.42379725\n",
      "Trained batch 455 batch loss 1.26550364 epoch total loss 1.42344928\n",
      "Trained batch 456 batch loss 1.27687943 epoch total loss 1.42312777\n",
      "Trained batch 457 batch loss 1.29358375 epoch total loss 1.42284441\n",
      "Trained batch 458 batch loss 1.40428948 epoch total loss 1.42280388\n",
      "Trained batch 459 batch loss 1.31046116 epoch total loss 1.42255914\n",
      "Trained batch 460 batch loss 1.36898041 epoch total loss 1.42244267\n",
      "Trained batch 461 batch loss 1.40040445 epoch total loss 1.42239475\n",
      "Trained batch 462 batch loss 1.43479788 epoch total loss 1.42242169\n",
      "Trained batch 463 batch loss 1.35121131 epoch total loss 1.42226779\n",
      "Trained batch 464 batch loss 1.44791842 epoch total loss 1.42232311\n",
      "Trained batch 465 batch loss 1.42018151 epoch total loss 1.42231858\n",
      "Trained batch 466 batch loss 1.49028563 epoch total loss 1.42246437\n",
      "Trained batch 467 batch loss 1.38720059 epoch total loss 1.42238891\n",
      "Trained batch 468 batch loss 1.35366774 epoch total loss 1.42224205\n",
      "Trained batch 469 batch loss 1.25856423 epoch total loss 1.421893\n",
      "Trained batch 470 batch loss 1.31822288 epoch total loss 1.42167246\n",
      "Trained batch 471 batch loss 1.41423309 epoch total loss 1.42165661\n",
      "Trained batch 472 batch loss 1.41291 epoch total loss 1.42163813\n",
      "Trained batch 473 batch loss 1.50019848 epoch total loss 1.42180419\n",
      "Trained batch 474 batch loss 1.45996094 epoch total loss 1.42188466\n",
      "Trained batch 475 batch loss 1.30894136 epoch total loss 1.42164695\n",
      "Trained batch 476 batch loss 1.28538489 epoch total loss 1.42136073\n",
      "Trained batch 477 batch loss 1.51712275 epoch total loss 1.42156148\n",
      "Trained batch 478 batch loss 1.42430151 epoch total loss 1.42156732\n",
      "Trained batch 479 batch loss 1.45130539 epoch total loss 1.42162931\n",
      "Trained batch 480 batch loss 1.60312068 epoch total loss 1.42200744\n",
      "Trained batch 481 batch loss 1.5090518 epoch total loss 1.4221884\n",
      "Trained batch 482 batch loss 1.56593502 epoch total loss 1.42248666\n",
      "Trained batch 483 batch loss 1.45432413 epoch total loss 1.42255259\n",
      "Trained batch 484 batch loss 1.4020797 epoch total loss 1.42251027\n",
      "Trained batch 485 batch loss 1.32974 epoch total loss 1.42231894\n",
      "Trained batch 486 batch loss 1.44327641 epoch total loss 1.42236221\n",
      "Trained batch 487 batch loss 1.42033052 epoch total loss 1.42235804\n",
      "Trained batch 488 batch loss 1.37966645 epoch total loss 1.42227054\n",
      "Trained batch 489 batch loss 1.38257325 epoch total loss 1.42218924\n",
      "Trained batch 490 batch loss 1.31136751 epoch total loss 1.4219631\n",
      "Trained batch 491 batch loss 1.29242384 epoch total loss 1.42169929\n",
      "Trained batch 492 batch loss 1.38337493 epoch total loss 1.42162132\n",
      "Trained batch 493 batch loss 1.42781091 epoch total loss 1.42163384\n",
      "Trained batch 494 batch loss 1.6901412 epoch total loss 1.42217731\n",
      "Trained batch 495 batch loss 1.58501816 epoch total loss 1.42250633\n",
      "Trained batch 496 batch loss 1.52209687 epoch total loss 1.42270708\n",
      "Trained batch 497 batch loss 1.46074128 epoch total loss 1.42278361\n",
      "Trained batch 498 batch loss 1.5785166 epoch total loss 1.4230963\n",
      "Trained batch 499 batch loss 1.27392042 epoch total loss 1.42279732\n",
      "Trained batch 500 batch loss 1.25241339 epoch total loss 1.42245662\n",
      "Trained batch 501 batch loss 1.23788917 epoch total loss 1.42208827\n",
      "Trained batch 502 batch loss 1.33608091 epoch total loss 1.42191696\n",
      "Trained batch 503 batch loss 1.40805149 epoch total loss 1.42188942\n",
      "Trained batch 504 batch loss 1.36443675 epoch total loss 1.42177546\n",
      "Trained batch 505 batch loss 1.34011722 epoch total loss 1.42161369\n",
      "Trained batch 506 batch loss 1.36629415 epoch total loss 1.42150438\n",
      "Trained batch 507 batch loss 1.44132388 epoch total loss 1.42154348\n",
      "Trained batch 508 batch loss 1.32633924 epoch total loss 1.42135608\n",
      "Trained batch 509 batch loss 1.37317157 epoch total loss 1.42126143\n",
      "Trained batch 510 batch loss 1.28433442 epoch total loss 1.42099297\n",
      "Trained batch 511 batch loss 1.30391264 epoch total loss 1.42076385\n",
      "Trained batch 512 batch loss 1.36677289 epoch total loss 1.42065835\n",
      "Trained batch 513 batch loss 1.34535933 epoch total loss 1.42051148\n",
      "Trained batch 514 batch loss 1.3725965 epoch total loss 1.42041838\n",
      "Trained batch 515 batch loss 1.42584538 epoch total loss 1.42042887\n",
      "Trained batch 516 batch loss 1.60179102 epoch total loss 1.42078042\n",
      "Trained batch 517 batch loss 1.53830445 epoch total loss 1.42100775\n",
      "Trained batch 518 batch loss 1.40112185 epoch total loss 1.42096937\n",
      "Trained batch 519 batch loss 1.2548852 epoch total loss 1.42064941\n",
      "Trained batch 520 batch loss 1.22315526 epoch total loss 1.42026949\n",
      "Trained batch 521 batch loss 1.30885506 epoch total loss 1.42005563\n",
      "Trained batch 522 batch loss 1.42048299 epoch total loss 1.42005646\n",
      "Trained batch 523 batch loss 1.44220865 epoch total loss 1.42009878\n",
      "Trained batch 524 batch loss 1.45613503 epoch total loss 1.42016757\n",
      "Trained batch 525 batch loss 1.44619143 epoch total loss 1.42021704\n",
      "Trained batch 526 batch loss 1.39063036 epoch total loss 1.42016077\n",
      "Trained batch 527 batch loss 1.42471218 epoch total loss 1.42016935\n",
      "Trained batch 528 batch loss 1.44315815 epoch total loss 1.42021298\n",
      "Trained batch 529 batch loss 1.41144657 epoch total loss 1.42019641\n",
      "Trained batch 530 batch loss 1.47167635 epoch total loss 1.42029345\n",
      "Trained batch 531 batch loss 1.45958412 epoch total loss 1.42036748\n",
      "Trained batch 532 batch loss 1.39859378 epoch total loss 1.42032659\n",
      "Trained batch 533 batch loss 1.49306333 epoch total loss 1.42046309\n",
      "Trained batch 534 batch loss 1.50436938 epoch total loss 1.4206202\n",
      "Trained batch 535 batch loss 1.35005987 epoch total loss 1.42048836\n",
      "Trained batch 536 batch loss 1.36933482 epoch total loss 1.42039287\n",
      "Trained batch 537 batch loss 1.43248534 epoch total loss 1.4204154\n",
      "Trained batch 538 batch loss 1.41509843 epoch total loss 1.42040551\n",
      "Trained batch 539 batch loss 1.35122788 epoch total loss 1.42027724\n",
      "Trained batch 540 batch loss 1.37647152 epoch total loss 1.42019606\n",
      "Trained batch 541 batch loss 1.28181994 epoch total loss 1.41994023\n",
      "Trained batch 542 batch loss 1.24013805 epoch total loss 1.41960847\n",
      "Trained batch 543 batch loss 1.35878885 epoch total loss 1.41949642\n",
      "Trained batch 544 batch loss 1.37318385 epoch total loss 1.4194113\n",
      "Trained batch 545 batch loss 1.49377322 epoch total loss 1.41954768\n",
      "Trained batch 546 batch loss 1.6145947 epoch total loss 1.41990495\n",
      "Trained batch 547 batch loss 1.69737864 epoch total loss 1.4204123\n",
      "Trained batch 548 batch loss 1.51509666 epoch total loss 1.42058504\n",
      "Trained batch 549 batch loss 1.48245871 epoch total loss 1.42069781\n",
      "Trained batch 550 batch loss 1.43198836 epoch total loss 1.42071831\n",
      "Trained batch 551 batch loss 1.34820962 epoch total loss 1.42058671\n",
      "Trained batch 552 batch loss 1.22887135 epoch total loss 1.42023945\n",
      "Trained batch 553 batch loss 1.39548326 epoch total loss 1.42019475\n",
      "Trained batch 554 batch loss 1.49007881 epoch total loss 1.42032075\n",
      "Trained batch 555 batch loss 1.42668426 epoch total loss 1.42033231\n",
      "Trained batch 556 batch loss 1.35818529 epoch total loss 1.42022061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 557 batch loss 1.32332611 epoch total loss 1.42004657\n",
      "Trained batch 558 batch loss 1.36575162 epoch total loss 1.41994917\n",
      "Trained batch 559 batch loss 1.37340987 epoch total loss 1.41986597\n",
      "Trained batch 560 batch loss 1.345222 epoch total loss 1.41973269\n",
      "Trained batch 561 batch loss 1.3350656 epoch total loss 1.41958177\n",
      "Trained batch 562 batch loss 1.27203238 epoch total loss 1.41931927\n",
      "Trained batch 563 batch loss 1.35746658 epoch total loss 1.41920936\n",
      "Trained batch 564 batch loss 1.37797034 epoch total loss 1.41913629\n",
      "Trained batch 565 batch loss 1.41981411 epoch total loss 1.41913748\n",
      "Trained batch 566 batch loss 1.44336116 epoch total loss 1.41918027\n",
      "Trained batch 567 batch loss 1.45429718 epoch total loss 1.41924214\n",
      "Trained batch 568 batch loss 1.38600481 epoch total loss 1.41918361\n",
      "Trained batch 569 batch loss 1.36994636 epoch total loss 1.41909707\n",
      "Trained batch 570 batch loss 1.48122263 epoch total loss 1.41920602\n",
      "Trained batch 571 batch loss 1.33896744 epoch total loss 1.41906559\n",
      "Trained batch 572 batch loss 1.44062257 epoch total loss 1.41910326\n",
      "Trained batch 573 batch loss 1.33095372 epoch total loss 1.41894937\n",
      "Trained batch 574 batch loss 1.350999 epoch total loss 1.41883099\n",
      "Trained batch 575 batch loss 1.46753275 epoch total loss 1.41891575\n",
      "Trained batch 576 batch loss 1.46590137 epoch total loss 1.41899729\n",
      "Trained batch 577 batch loss 1.42679548 epoch total loss 1.41901076\n",
      "Trained batch 578 batch loss 1.38703561 epoch total loss 1.41895545\n",
      "Trained batch 579 batch loss 1.39125443 epoch total loss 1.41890752\n",
      "Trained batch 580 batch loss 1.32965708 epoch total loss 1.41875362\n",
      "Trained batch 581 batch loss 1.24184799 epoch total loss 1.41844916\n",
      "Trained batch 582 batch loss 1.33735466 epoch total loss 1.41830981\n",
      "Trained batch 583 batch loss 1.35256219 epoch total loss 1.41819704\n",
      "Trained batch 584 batch loss 1.44067407 epoch total loss 1.41823542\n",
      "Trained batch 585 batch loss 1.45195329 epoch total loss 1.41829312\n",
      "Trained batch 586 batch loss 1.40989518 epoch total loss 1.41827881\n",
      "Trained batch 587 batch loss 1.39814377 epoch total loss 1.41824448\n",
      "Trained batch 588 batch loss 1.44764149 epoch total loss 1.41829443\n",
      "Trained batch 589 batch loss 1.43309319 epoch total loss 1.41831958\n",
      "Trained batch 590 batch loss 1.39654434 epoch total loss 1.41828275\n",
      "Trained batch 591 batch loss 1.34235859 epoch total loss 1.41815424\n",
      "Trained batch 592 batch loss 1.32213569 epoch total loss 1.417992\n",
      "Trained batch 593 batch loss 1.32519412 epoch total loss 1.41783559\n",
      "Trained batch 594 batch loss 1.41881919 epoch total loss 1.41783726\n",
      "Trained batch 595 batch loss 1.40045846 epoch total loss 1.41780806\n",
      "Trained batch 596 batch loss 1.33945894 epoch total loss 1.41767657\n",
      "Trained batch 597 batch loss 1.34213066 epoch total loss 1.41755\n",
      "Trained batch 598 batch loss 1.30830073 epoch total loss 1.41736722\n",
      "Trained batch 599 batch loss 1.35790229 epoch total loss 1.41726804\n",
      "Trained batch 600 batch loss 1.35239565 epoch total loss 1.41715991\n",
      "Trained batch 601 batch loss 1.34822249 epoch total loss 1.41704524\n",
      "Trained batch 602 batch loss 1.33925569 epoch total loss 1.41691589\n",
      "Trained batch 603 batch loss 1.30716419 epoch total loss 1.41673398\n",
      "Trained batch 604 batch loss 1.36260819 epoch total loss 1.41664433\n",
      "Trained batch 605 batch loss 1.32229137 epoch total loss 1.41648841\n",
      "Trained batch 606 batch loss 1.34726715 epoch total loss 1.41637421\n",
      "Trained batch 607 batch loss 1.45343316 epoch total loss 1.41643524\n",
      "Trained batch 608 batch loss 1.3615545 epoch total loss 1.416345\n",
      "Trained batch 609 batch loss 1.28842461 epoch total loss 1.41613495\n",
      "Trained batch 610 batch loss 1.38537788 epoch total loss 1.41608453\n",
      "Trained batch 611 batch loss 1.22816181 epoch total loss 1.41577697\n",
      "Trained batch 612 batch loss 1.30643058 epoch total loss 1.41559839\n",
      "Trained batch 613 batch loss 1.37510133 epoch total loss 1.41553235\n",
      "Trained batch 614 batch loss 1.30223107 epoch total loss 1.41534781\n",
      "Trained batch 615 batch loss 1.22621655 epoch total loss 1.41504025\n",
      "Trained batch 616 batch loss 1.32623684 epoch total loss 1.41489613\n",
      "Trained batch 617 batch loss 1.23390436 epoch total loss 1.41460276\n",
      "Trained batch 618 batch loss 1.24172211 epoch total loss 1.41432297\n",
      "Trained batch 619 batch loss 1.20922649 epoch total loss 1.41399157\n",
      "Trained batch 620 batch loss 1.25958872 epoch total loss 1.41374254\n",
      "Trained batch 621 batch loss 1.35447013 epoch total loss 1.41364717\n",
      "Trained batch 622 batch loss 1.34035015 epoch total loss 1.41352928\n",
      "Trained batch 623 batch loss 1.27859235 epoch total loss 1.41331267\n",
      "Trained batch 624 batch loss 1.32580948 epoch total loss 1.41317236\n",
      "Trained batch 625 batch loss 1.41867363 epoch total loss 1.4131813\n",
      "Trained batch 626 batch loss 1.47565603 epoch total loss 1.41328108\n",
      "Trained batch 627 batch loss 1.50972641 epoch total loss 1.41343486\n",
      "Trained batch 628 batch loss 1.38071823 epoch total loss 1.41338277\n",
      "Trained batch 629 batch loss 1.34324574 epoch total loss 1.41327131\n",
      "Trained batch 630 batch loss 1.33691072 epoch total loss 1.41315007\n",
      "Trained batch 631 batch loss 1.36679459 epoch total loss 1.41307664\n",
      "Trained batch 632 batch loss 1.31533992 epoch total loss 1.41292202\n",
      "Trained batch 633 batch loss 1.15580308 epoch total loss 1.41251588\n",
      "Trained batch 634 batch loss 1.24259472 epoch total loss 1.4122479\n",
      "Trained batch 635 batch loss 1.29170656 epoch total loss 1.412058\n",
      "Trained batch 636 batch loss 1.44453454 epoch total loss 1.41210914\n",
      "Trained batch 637 batch loss 1.33586359 epoch total loss 1.41198945\n",
      "Trained batch 638 batch loss 1.33153963 epoch total loss 1.41186333\n",
      "Trained batch 639 batch loss 1.32191467 epoch total loss 1.41172254\n",
      "Trained batch 640 batch loss 1.34235287 epoch total loss 1.41161418\n",
      "Trained batch 641 batch loss 1.45792365 epoch total loss 1.41168642\n",
      "Trained batch 642 batch loss 1.3861171 epoch total loss 1.4116466\n",
      "Trained batch 643 batch loss 1.3625319 epoch total loss 1.41157019\n",
      "Trained batch 644 batch loss 1.30451035 epoch total loss 1.41140401\n",
      "Trained batch 645 batch loss 1.35896564 epoch total loss 1.41132259\n",
      "Trained batch 646 batch loss 1.26682246 epoch total loss 1.41109896\n",
      "Trained batch 647 batch loss 1.20742869 epoch total loss 1.41078424\n",
      "Trained batch 648 batch loss 1.38000607 epoch total loss 1.4107368\n",
      "Trained batch 649 batch loss 1.31987011 epoch total loss 1.41059673\n",
      "Trained batch 650 batch loss 1.38236558 epoch total loss 1.41055334\n",
      "Trained batch 651 batch loss 1.2674129 epoch total loss 1.4103334\n",
      "Trained batch 652 batch loss 1.47131753 epoch total loss 1.41042697\n",
      "Trained batch 653 batch loss 1.29685092 epoch total loss 1.41025305\n",
      "Trained batch 654 batch loss 1.36430848 epoch total loss 1.41018283\n",
      "Trained batch 655 batch loss 1.37859356 epoch total loss 1.41013467\n",
      "Trained batch 656 batch loss 1.28877234 epoch total loss 1.40994966\n",
      "Trained batch 657 batch loss 1.32832837 epoch total loss 1.40982533\n",
      "Trained batch 658 batch loss 1.43110299 epoch total loss 1.40985763\n",
      "Trained batch 659 batch loss 1.39900815 epoch total loss 1.40984118\n",
      "Trained batch 660 batch loss 1.34088922 epoch total loss 1.40973663\n",
      "Trained batch 661 batch loss 1.2616663 epoch total loss 1.40951264\n",
      "Trained batch 662 batch loss 1.26402736 epoch total loss 1.40929294\n",
      "Trained batch 663 batch loss 1.28976035 epoch total loss 1.40911257\n",
      "Trained batch 664 batch loss 1.20931149 epoch total loss 1.40881169\n",
      "Trained batch 665 batch loss 1.14063978 epoch total loss 1.4084084\n",
      "Trained batch 666 batch loss 1.26635027 epoch total loss 1.40819502\n",
      "Trained batch 667 batch loss 1.28725934 epoch total loss 1.4080137\n",
      "Trained batch 668 batch loss 1.4087975 epoch total loss 1.40801489\n",
      "Trained batch 669 batch loss 1.34885812 epoch total loss 1.40792656\n",
      "Trained batch 670 batch loss 1.44157541 epoch total loss 1.40797675\n",
      "Trained batch 671 batch loss 1.42667556 epoch total loss 1.40800464\n",
      "Trained batch 672 batch loss 1.35356152 epoch total loss 1.4079237\n",
      "Trained batch 673 batch loss 1.45974517 epoch total loss 1.40800059\n",
      "Trained batch 674 batch loss 1.36616731 epoch total loss 1.40793848\n",
      "Trained batch 675 batch loss 1.36210465 epoch total loss 1.40787065\n",
      "Trained batch 676 batch loss 1.5614152 epoch total loss 1.40809774\n",
      "Trained batch 677 batch loss 1.22485471 epoch total loss 1.40782714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 678 batch loss 1.26921535 epoch total loss 1.40762269\n",
      "Trained batch 679 batch loss 1.33345342 epoch total loss 1.40751338\n",
      "Trained batch 680 batch loss 1.28691196 epoch total loss 1.40733612\n",
      "Trained batch 681 batch loss 1.30220282 epoch total loss 1.40718162\n",
      "Trained batch 682 batch loss 1.2623539 epoch total loss 1.40696931\n",
      "Trained batch 683 batch loss 1.24901605 epoch total loss 1.40673804\n",
      "Trained batch 684 batch loss 1.33169568 epoch total loss 1.40662837\n",
      "Trained batch 685 batch loss 1.28890157 epoch total loss 1.40645647\n",
      "Trained batch 686 batch loss 1.34634042 epoch total loss 1.40636873\n",
      "Trained batch 687 batch loss 1.25354803 epoch total loss 1.40614629\n",
      "Trained batch 688 batch loss 1.36826015 epoch total loss 1.40609133\n",
      "Trained batch 689 batch loss 1.41327405 epoch total loss 1.4061017\n",
      "Trained batch 690 batch loss 1.4755702 epoch total loss 1.40620244\n",
      "Trained batch 691 batch loss 1.47712517 epoch total loss 1.40630507\n",
      "Trained batch 692 batch loss 1.45458746 epoch total loss 1.40637481\n",
      "Trained batch 693 batch loss 1.45770049 epoch total loss 1.40644884\n",
      "Trained batch 694 batch loss 1.33183837 epoch total loss 1.40634143\n",
      "Trained batch 695 batch loss 1.34464073 epoch total loss 1.40625262\n",
      "Trained batch 696 batch loss 1.28092599 epoch total loss 1.40607262\n",
      "Trained batch 697 batch loss 1.20309556 epoch total loss 1.40578139\n",
      "Trained batch 698 batch loss 1.22968221 epoch total loss 1.40552914\n",
      "Trained batch 699 batch loss 1.32688749 epoch total loss 1.40541661\n",
      "Trained batch 700 batch loss 1.23403978 epoch total loss 1.40517187\n",
      "Trained batch 701 batch loss 1.45140147 epoch total loss 1.40523779\n",
      "Trained batch 702 batch loss 1.31966758 epoch total loss 1.40511584\n",
      "Trained batch 703 batch loss 1.44430149 epoch total loss 1.40517163\n",
      "Trained batch 704 batch loss 1.35725403 epoch total loss 1.40510356\n",
      "Trained batch 705 batch loss 1.29314733 epoch total loss 1.40494466\n",
      "Trained batch 706 batch loss 1.29693401 epoch total loss 1.40479171\n",
      "Trained batch 707 batch loss 1.35471416 epoch total loss 1.4047209\n",
      "Trained batch 708 batch loss 1.40417624 epoch total loss 1.40472019\n",
      "Trained batch 709 batch loss 1.22911119 epoch total loss 1.40447247\n",
      "Trained batch 710 batch loss 1.04176426 epoch total loss 1.40396166\n",
      "Trained batch 711 batch loss 1.1357708 epoch total loss 1.40358436\n",
      "Trained batch 712 batch loss 1.31834114 epoch total loss 1.40346467\n",
      "Trained batch 713 batch loss 1.47466099 epoch total loss 1.40356457\n",
      "Trained batch 714 batch loss 1.53890467 epoch total loss 1.403754\n",
      "Trained batch 715 batch loss 1.5473876 epoch total loss 1.40395486\n",
      "Trained batch 716 batch loss 1.52167988 epoch total loss 1.40411925\n",
      "Trained batch 717 batch loss 1.42973733 epoch total loss 1.40415502\n",
      "Trained batch 718 batch loss 1.3685751 epoch total loss 1.40410554\n",
      "Trained batch 719 batch loss 1.23239911 epoch total loss 1.40386677\n",
      "Trained batch 720 batch loss 1.24328947 epoch total loss 1.40364373\n",
      "Trained batch 721 batch loss 1.41690278 epoch total loss 1.40366209\n",
      "Trained batch 722 batch loss 1.44519687 epoch total loss 1.40371966\n",
      "Trained batch 723 batch loss 1.44041348 epoch total loss 1.40377045\n",
      "Trained batch 724 batch loss 1.43339062 epoch total loss 1.40381134\n",
      "Trained batch 725 batch loss 1.37238693 epoch total loss 1.40376806\n",
      "Trained batch 726 batch loss 1.32453823 epoch total loss 1.40365887\n",
      "Trained batch 727 batch loss 1.33241725 epoch total loss 1.40356088\n",
      "Trained batch 728 batch loss 1.39615536 epoch total loss 1.40355074\n",
      "Trained batch 729 batch loss 1.34929919 epoch total loss 1.40347624\n",
      "Trained batch 730 batch loss 1.31493533 epoch total loss 1.40335488\n",
      "Trained batch 731 batch loss 1.34759629 epoch total loss 1.40327871\n",
      "Trained batch 732 batch loss 1.36749482 epoch total loss 1.40323\n",
      "Trained batch 733 batch loss 1.34977043 epoch total loss 1.403157\n",
      "Trained batch 734 batch loss 1.32829833 epoch total loss 1.40305483\n",
      "Trained batch 735 batch loss 1.41866612 epoch total loss 1.40307617\n",
      "Trained batch 736 batch loss 1.40749395 epoch total loss 1.40308213\n",
      "Trained batch 737 batch loss 1.39657176 epoch total loss 1.40307331\n",
      "Trained batch 738 batch loss 1.48511517 epoch total loss 1.40318453\n",
      "Trained batch 739 batch loss 1.38161182 epoch total loss 1.40315533\n",
      "Trained batch 740 batch loss 1.34169865 epoch total loss 1.40307224\n",
      "Trained batch 741 batch loss 1.40886736 epoch total loss 1.40308\n",
      "Trained batch 742 batch loss 1.45480633 epoch total loss 1.40314972\n",
      "Trained batch 743 batch loss 1.4753933 epoch total loss 1.40324688\n",
      "Trained batch 744 batch loss 1.30092096 epoch total loss 1.40310931\n",
      "Trained batch 745 batch loss 1.36164832 epoch total loss 1.40305376\n",
      "Trained batch 746 batch loss 1.28125775 epoch total loss 1.40289044\n",
      "Trained batch 747 batch loss 1.37171292 epoch total loss 1.40284872\n",
      "Trained batch 748 batch loss 1.45469046 epoch total loss 1.40291798\n",
      "Trained batch 749 batch loss 1.35573459 epoch total loss 1.40285504\n",
      "Trained batch 750 batch loss 1.42434669 epoch total loss 1.40288365\n",
      "Trained batch 751 batch loss 1.44755864 epoch total loss 1.40294302\n",
      "Trained batch 752 batch loss 1.43749499 epoch total loss 1.40298903\n",
      "Trained batch 753 batch loss 1.4969871 epoch total loss 1.40311372\n",
      "Trained batch 754 batch loss 1.41683841 epoch total loss 1.40313208\n",
      "Trained batch 755 batch loss 1.27594244 epoch total loss 1.40296364\n",
      "Trained batch 756 batch loss 1.38420153 epoch total loss 1.40293872\n",
      "Trained batch 757 batch loss 1.41679132 epoch total loss 1.40295696\n",
      "Trained batch 758 batch loss 1.33418083 epoch total loss 1.40286636\n",
      "Trained batch 759 batch loss 1.34346807 epoch total loss 1.40278816\n",
      "Trained batch 760 batch loss 1.3369112 epoch total loss 1.4027015\n",
      "Trained batch 761 batch loss 1.41390908 epoch total loss 1.40271616\n",
      "Trained batch 762 batch loss 1.51834679 epoch total loss 1.40286791\n",
      "Trained batch 763 batch loss 1.30594742 epoch total loss 1.40274084\n",
      "Trained batch 764 batch loss 1.31776118 epoch total loss 1.40262961\n",
      "Trained batch 765 batch loss 1.29760635 epoch total loss 1.40249228\n",
      "Trained batch 766 batch loss 1.34012663 epoch total loss 1.40241086\n",
      "Trained batch 767 batch loss 1.37907481 epoch total loss 1.40238035\n",
      "Trained batch 768 batch loss 1.42798555 epoch total loss 1.40241373\n",
      "Trained batch 769 batch loss 1.41381931 epoch total loss 1.40242851\n",
      "Trained batch 770 batch loss 1.41970849 epoch total loss 1.40245092\n",
      "Trained batch 771 batch loss 1.37047791 epoch total loss 1.40240943\n",
      "Trained batch 772 batch loss 1.3358736 epoch total loss 1.40232325\n",
      "Trained batch 773 batch loss 1.28711843 epoch total loss 1.40217412\n",
      "Trained batch 774 batch loss 1.45321441 epoch total loss 1.40224016\n",
      "Trained batch 775 batch loss 1.53746331 epoch total loss 1.40241468\n",
      "Trained batch 776 batch loss 1.50782692 epoch total loss 1.40255046\n",
      "Trained batch 777 batch loss 1.34157836 epoch total loss 1.4024719\n",
      "Trained batch 778 batch loss 1.32510471 epoch total loss 1.40237248\n",
      "Trained batch 779 batch loss 1.26557052 epoch total loss 1.40219688\n",
      "Trained batch 780 batch loss 1.38520324 epoch total loss 1.40217519\n",
      "Trained batch 781 batch loss 1.24755716 epoch total loss 1.40197718\n",
      "Trained batch 782 batch loss 1.24155688 epoch total loss 1.40177214\n",
      "Trained batch 783 batch loss 1.24762893 epoch total loss 1.40157533\n",
      "Trained batch 784 batch loss 1.40750265 epoch total loss 1.40158284\n",
      "Trained batch 785 batch loss 1.44667077 epoch total loss 1.4016403\n",
      "Trained batch 786 batch loss 1.46072972 epoch total loss 1.4017154\n",
      "Trained batch 787 batch loss 1.47433627 epoch total loss 1.40180767\n",
      "Trained batch 788 batch loss 1.49986339 epoch total loss 1.40193212\n",
      "Trained batch 789 batch loss 1.43504548 epoch total loss 1.40197408\n",
      "Trained batch 790 batch loss 1.44185448 epoch total loss 1.40202463\n",
      "Trained batch 791 batch loss 1.48559809 epoch total loss 1.40213037\n",
      "Trained batch 792 batch loss 1.42341566 epoch total loss 1.40215731\n",
      "Trained batch 793 batch loss 1.38105774 epoch total loss 1.40213072\n",
      "Trained batch 794 batch loss 1.37077284 epoch total loss 1.40209115\n",
      "Trained batch 795 batch loss 1.26781499 epoch total loss 1.40192223\n",
      "Trained batch 796 batch loss 1.26446867 epoch total loss 1.40174961\n",
      "Trained batch 797 batch loss 1.2473985 epoch total loss 1.40155602\n",
      "Trained batch 798 batch loss 1.45885742 epoch total loss 1.4016279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 799 batch loss 1.36642122 epoch total loss 1.40158379\n",
      "Trained batch 800 batch loss 1.34988964 epoch total loss 1.40151918\n",
      "Trained batch 801 batch loss 1.47719812 epoch total loss 1.40161359\n",
      "Trained batch 802 batch loss 1.35014927 epoch total loss 1.40154934\n",
      "Trained batch 803 batch loss 1.30681467 epoch total loss 1.40143132\n",
      "Trained batch 804 batch loss 1.54411125 epoch total loss 1.40160871\n",
      "Trained batch 805 batch loss 1.46279037 epoch total loss 1.40168476\n",
      "Trained batch 806 batch loss 1.43344688 epoch total loss 1.40172422\n",
      "Trained batch 807 batch loss 1.54844546 epoch total loss 1.40190601\n",
      "Trained batch 808 batch loss 1.38843989 epoch total loss 1.40188932\n",
      "Trained batch 809 batch loss 1.39759374 epoch total loss 1.40188396\n",
      "Trained batch 810 batch loss 1.37917137 epoch total loss 1.40185595\n",
      "Trained batch 811 batch loss 1.41252589 epoch total loss 1.40186906\n",
      "Trained batch 812 batch loss 1.35933256 epoch total loss 1.40181673\n",
      "Trained batch 813 batch loss 1.37026715 epoch total loss 1.40177786\n",
      "Trained batch 814 batch loss 1.34241986 epoch total loss 1.40170491\n",
      "Trained batch 815 batch loss 1.38983309 epoch total loss 1.40169036\n",
      "Trained batch 816 batch loss 1.28675079 epoch total loss 1.40154958\n",
      "Trained batch 817 batch loss 1.28610325 epoch total loss 1.40140831\n",
      "Trained batch 818 batch loss 1.20834339 epoch total loss 1.40117228\n",
      "Trained batch 819 batch loss 1.20795238 epoch total loss 1.40093648\n",
      "Trained batch 820 batch loss 1.29311764 epoch total loss 1.40080488\n",
      "Trained batch 821 batch loss 1.41278124 epoch total loss 1.40081954\n",
      "Trained batch 822 batch loss 1.40805984 epoch total loss 1.40082836\n",
      "Trained batch 823 batch loss 1.64034927 epoch total loss 1.40111947\n",
      "Trained batch 824 batch loss 1.3997761 epoch total loss 1.40111792\n",
      "Trained batch 825 batch loss 1.36796105 epoch total loss 1.40107763\n",
      "Trained batch 826 batch loss 1.29873979 epoch total loss 1.40095365\n",
      "Trained batch 827 batch loss 1.36974454 epoch total loss 1.40091598\n",
      "Trained batch 828 batch loss 1.32335865 epoch total loss 1.40082228\n",
      "Trained batch 829 batch loss 1.42182732 epoch total loss 1.40084767\n",
      "Trained batch 830 batch loss 1.43441057 epoch total loss 1.4008882\n",
      "Trained batch 831 batch loss 1.41917086 epoch total loss 1.40091014\n",
      "Trained batch 832 batch loss 1.2600944 epoch total loss 1.40074098\n",
      "Trained batch 833 batch loss 1.25430369 epoch total loss 1.40056515\n",
      "Trained batch 834 batch loss 1.32751763 epoch total loss 1.40047753\n",
      "Trained batch 835 batch loss 1.24167776 epoch total loss 1.40028739\n",
      "Trained batch 836 batch loss 1.35402071 epoch total loss 1.40023208\n",
      "Trained batch 837 batch loss 1.40803933 epoch total loss 1.40024137\n",
      "Trained batch 838 batch loss 1.4279027 epoch total loss 1.4002744\n",
      "Trained batch 839 batch loss 1.39393115 epoch total loss 1.40026677\n",
      "Trained batch 840 batch loss 1.32943153 epoch total loss 1.40018249\n",
      "Trained batch 841 batch loss 1.30494273 epoch total loss 1.40006924\n",
      "Trained batch 842 batch loss 1.3667233 epoch total loss 1.40002966\n",
      "Trained batch 843 batch loss 1.32369435 epoch total loss 1.39993918\n",
      "Trained batch 844 batch loss 1.38603818 epoch total loss 1.39992261\n",
      "Trained batch 845 batch loss 1.36393213 epoch total loss 1.39987993\n",
      "Trained batch 846 batch loss 1.45492029 epoch total loss 1.39994502\n",
      "Trained batch 847 batch loss 1.4051764 epoch total loss 1.39995122\n",
      "Trained batch 848 batch loss 1.37627697 epoch total loss 1.39992321\n",
      "Trained batch 849 batch loss 1.36610055 epoch total loss 1.39988339\n",
      "Trained batch 850 batch loss 1.39645326 epoch total loss 1.39987934\n",
      "Trained batch 851 batch loss 1.31026125 epoch total loss 1.39977407\n",
      "Trained batch 852 batch loss 1.29675412 epoch total loss 1.3996532\n",
      "Trained batch 853 batch loss 1.23730087 epoch total loss 1.39946282\n",
      "Trained batch 854 batch loss 1.4217968 epoch total loss 1.39948893\n",
      "Trained batch 855 batch loss 1.33139503 epoch total loss 1.39940929\n",
      "Trained batch 856 batch loss 1.27735913 epoch total loss 1.39926672\n",
      "Trained batch 857 batch loss 1.36661983 epoch total loss 1.39922857\n",
      "Trained batch 858 batch loss 1.36323261 epoch total loss 1.39918673\n",
      "Trained batch 859 batch loss 1.33628225 epoch total loss 1.39911354\n",
      "Trained batch 860 batch loss 1.32665622 epoch total loss 1.39902925\n",
      "Trained batch 861 batch loss 1.43166912 epoch total loss 1.39906716\n",
      "Trained batch 862 batch loss 1.31502318 epoch total loss 1.39896965\n",
      "Trained batch 863 batch loss 1.34693587 epoch total loss 1.39890933\n",
      "Trained batch 864 batch loss 1.38325524 epoch total loss 1.39889133\n",
      "Trained batch 865 batch loss 1.20352507 epoch total loss 1.39866543\n",
      "Trained batch 866 batch loss 1.2730552 epoch total loss 1.39852035\n",
      "Trained batch 867 batch loss 1.4044764 epoch total loss 1.39852715\n",
      "Trained batch 868 batch loss 1.41617966 epoch total loss 1.39854753\n",
      "Trained batch 869 batch loss 1.41938984 epoch total loss 1.39857149\n",
      "Trained batch 870 batch loss 1.32001877 epoch total loss 1.39848125\n",
      "Trained batch 871 batch loss 1.19069231 epoch total loss 1.39824271\n",
      "Trained batch 872 batch loss 1.22405934 epoch total loss 1.39804292\n",
      "Trained batch 873 batch loss 1.28045368 epoch total loss 1.39790809\n",
      "Trained batch 874 batch loss 1.30267417 epoch total loss 1.39779925\n",
      "Trained batch 875 batch loss 1.23881698 epoch total loss 1.39761746\n",
      "Trained batch 876 batch loss 1.29638863 epoch total loss 1.39750195\n",
      "Trained batch 877 batch loss 1.24722624 epoch total loss 1.39733052\n",
      "Trained batch 878 batch loss 1.40443134 epoch total loss 1.39733863\n",
      "Trained batch 879 batch loss 1.38481987 epoch total loss 1.39732432\n",
      "Trained batch 880 batch loss 1.4628346 epoch total loss 1.39739883\n",
      "Trained batch 881 batch loss 1.28057134 epoch total loss 1.39726615\n",
      "Trained batch 882 batch loss 1.28790319 epoch total loss 1.39714217\n",
      "Trained batch 883 batch loss 1.27481937 epoch total loss 1.39700365\n",
      "Trained batch 884 batch loss 1.3291347 epoch total loss 1.39692676\n",
      "Trained batch 885 batch loss 1.41017532 epoch total loss 1.39694178\n",
      "Trained batch 886 batch loss 1.49740815 epoch total loss 1.39705515\n",
      "Trained batch 887 batch loss 1.39446211 epoch total loss 1.39705217\n",
      "Trained batch 888 batch loss 1.25829458 epoch total loss 1.396896\n",
      "Trained batch 889 batch loss 1.32594037 epoch total loss 1.39681613\n",
      "Trained batch 890 batch loss 1.31792784 epoch total loss 1.39672744\n",
      "Trained batch 891 batch loss 1.36826038 epoch total loss 1.39669549\n",
      "Trained batch 892 batch loss 1.33366013 epoch total loss 1.3966248\n",
      "Trained batch 893 batch loss 1.29532933 epoch total loss 1.39651132\n",
      "Trained batch 894 batch loss 1.33600342 epoch total loss 1.39644372\n",
      "Trained batch 895 batch loss 1.38759637 epoch total loss 1.39643383\n",
      "Trained batch 896 batch loss 1.37943292 epoch total loss 1.39641476\n",
      "Trained batch 897 batch loss 1.47094584 epoch total loss 1.39649785\n",
      "Trained batch 898 batch loss 1.33839059 epoch total loss 1.39643312\n",
      "Trained batch 899 batch loss 1.23105121 epoch total loss 1.39624918\n",
      "Trained batch 900 batch loss 1.38234842 epoch total loss 1.39623368\n",
      "Trained batch 901 batch loss 1.28879702 epoch total loss 1.39611447\n",
      "Trained batch 902 batch loss 1.32043552 epoch total loss 1.39603055\n",
      "Trained batch 903 batch loss 1.29506993 epoch total loss 1.39591873\n",
      "Trained batch 904 batch loss 1.36079741 epoch total loss 1.39588\n",
      "Trained batch 905 batch loss 1.30601668 epoch total loss 1.39578068\n",
      "Trained batch 906 batch loss 1.42207217 epoch total loss 1.39580977\n",
      "Trained batch 907 batch loss 1.46912551 epoch total loss 1.39589059\n",
      "Trained batch 908 batch loss 1.43486714 epoch total loss 1.39593339\n",
      "Trained batch 909 batch loss 1.3487879 epoch total loss 1.39588153\n",
      "Trained batch 910 batch loss 1.36251855 epoch total loss 1.39584494\n",
      "Trained batch 911 batch loss 1.3246901 epoch total loss 1.39576685\n",
      "Trained batch 912 batch loss 1.26632524 epoch total loss 1.39562488\n",
      "Trained batch 913 batch loss 1.37509906 epoch total loss 1.39560246\n",
      "Trained batch 914 batch loss 1.30160165 epoch total loss 1.39549971\n",
      "Trained batch 915 batch loss 1.29639447 epoch total loss 1.39539135\n",
      "Trained batch 916 batch loss 1.24840045 epoch total loss 1.39523089\n",
      "Trained batch 917 batch loss 1.33227015 epoch total loss 1.39516222\n",
      "Trained batch 918 batch loss 1.2861383 epoch total loss 1.39504349\n",
      "Trained batch 919 batch loss 1.2197 epoch total loss 1.39485264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 920 batch loss 1.10148668 epoch total loss 1.39453375\n",
      "Trained batch 921 batch loss 1.05901456 epoch total loss 1.39416945\n",
      "Trained batch 922 batch loss 1.20944285 epoch total loss 1.39396906\n",
      "Trained batch 923 batch loss 1.48772764 epoch total loss 1.39407063\n",
      "Trained batch 924 batch loss 1.45422912 epoch total loss 1.39413571\n",
      "Trained batch 925 batch loss 1.41944659 epoch total loss 1.39416301\n",
      "Trained batch 926 batch loss 1.34890592 epoch total loss 1.39411414\n",
      "Trained batch 927 batch loss 1.25492096 epoch total loss 1.39396393\n",
      "Trained batch 928 batch loss 1.30895 epoch total loss 1.39387238\n",
      "Trained batch 929 batch loss 1.2463392 epoch total loss 1.39371359\n",
      "Trained batch 930 batch loss 1.20804739 epoch total loss 1.39351392\n",
      "Trained batch 931 batch loss 1.40982473 epoch total loss 1.39353132\n",
      "Trained batch 932 batch loss 1.31428409 epoch total loss 1.39344633\n",
      "Trained batch 933 batch loss 1.33827615 epoch total loss 1.3933872\n",
      "Trained batch 934 batch loss 1.37165809 epoch total loss 1.39336395\n",
      "Trained batch 935 batch loss 1.32442605 epoch total loss 1.39329028\n",
      "Trained batch 936 batch loss 1.35749936 epoch total loss 1.39325213\n",
      "Trained batch 937 batch loss 1.33778286 epoch total loss 1.39319289\n",
      "Trained batch 938 batch loss 1.31985319 epoch total loss 1.39311469\n",
      "Trained batch 939 batch loss 1.39454854 epoch total loss 1.39311624\n",
      "Trained batch 940 batch loss 1.35972786 epoch total loss 1.39308071\n",
      "Trained batch 941 batch loss 1.3617202 epoch total loss 1.39304733\n",
      "Trained batch 942 batch loss 1.35929966 epoch total loss 1.39301145\n",
      "Trained batch 943 batch loss 1.44774771 epoch total loss 1.39306951\n",
      "Trained batch 944 batch loss 1.38170278 epoch total loss 1.39305747\n",
      "Trained batch 945 batch loss 1.47505891 epoch total loss 1.39314425\n",
      "Trained batch 946 batch loss 1.37870419 epoch total loss 1.39312899\n",
      "Trained batch 947 batch loss 1.56798196 epoch total loss 1.39331365\n",
      "Trained batch 948 batch loss 1.35536885 epoch total loss 1.39327359\n",
      "Trained batch 949 batch loss 1.28278041 epoch total loss 1.39315724\n",
      "Trained batch 950 batch loss 1.32817125 epoch total loss 1.39308882\n",
      "Trained batch 951 batch loss 1.28282809 epoch total loss 1.39297283\n",
      "Trained batch 952 batch loss 1.24047542 epoch total loss 1.39281261\n",
      "Trained batch 953 batch loss 1.31782854 epoch total loss 1.39273405\n",
      "Trained batch 954 batch loss 1.30556619 epoch total loss 1.39264262\n",
      "Trained batch 955 batch loss 1.30445254 epoch total loss 1.39255023\n",
      "Trained batch 956 batch loss 1.1716125 epoch total loss 1.3923192\n",
      "Trained batch 957 batch loss 1.36854279 epoch total loss 1.39229429\n",
      "Trained batch 958 batch loss 1.41022921 epoch total loss 1.39231312\n",
      "Trained batch 959 batch loss 1.44778466 epoch total loss 1.39237094\n",
      "Trained batch 960 batch loss 1.3466723 epoch total loss 1.39232326\n",
      "Trained batch 961 batch loss 1.38525462 epoch total loss 1.39231598\n",
      "Trained batch 962 batch loss 1.27293754 epoch total loss 1.39219189\n",
      "Trained batch 963 batch loss 1.34950876 epoch total loss 1.39214754\n",
      "Trained batch 964 batch loss 1.39721668 epoch total loss 1.39215279\n",
      "Trained batch 965 batch loss 1.25297868 epoch total loss 1.39200854\n",
      "Trained batch 966 batch loss 1.25531328 epoch total loss 1.39186704\n",
      "Trained batch 967 batch loss 1.27483416 epoch total loss 1.39174592\n",
      "Trained batch 968 batch loss 1.34635401 epoch total loss 1.39169908\n",
      "Trained batch 969 batch loss 1.40355492 epoch total loss 1.39171124\n",
      "Trained batch 970 batch loss 1.35187531 epoch total loss 1.39167023\n",
      "Trained batch 971 batch loss 1.41339135 epoch total loss 1.39169276\n",
      "Trained batch 972 batch loss 1.25878179 epoch total loss 1.39155602\n",
      "Trained batch 973 batch loss 1.18445158 epoch total loss 1.39134312\n",
      "Trained batch 974 batch loss 1.15555513 epoch total loss 1.391101\n",
      "Trained batch 975 batch loss 1.21383119 epoch total loss 1.39091921\n",
      "Trained batch 976 batch loss 1.38527966 epoch total loss 1.39091337\n",
      "Trained batch 977 batch loss 1.48280239 epoch total loss 1.39100742\n",
      "Trained batch 978 batch loss 1.5299654 epoch total loss 1.39114952\n",
      "Trained batch 979 batch loss 1.34120405 epoch total loss 1.3910985\n",
      "Trained batch 980 batch loss 1.56620932 epoch total loss 1.39127707\n",
      "Trained batch 981 batch loss 1.39992845 epoch total loss 1.3912859\n",
      "Trained batch 982 batch loss 1.33717191 epoch total loss 1.3912307\n",
      "Trained batch 983 batch loss 1.32943332 epoch total loss 1.39116788\n",
      "Trained batch 984 batch loss 1.34279943 epoch total loss 1.39111876\n",
      "Trained batch 985 batch loss 1.28068268 epoch total loss 1.39100659\n",
      "Trained batch 986 batch loss 1.39148426 epoch total loss 1.39100707\n",
      "Trained batch 987 batch loss 1.31052542 epoch total loss 1.39092553\n",
      "Trained batch 988 batch loss 1.53206372 epoch total loss 1.39106846\n",
      "Trained batch 989 batch loss 1.46902227 epoch total loss 1.39114726\n",
      "Trained batch 990 batch loss 1.35993862 epoch total loss 1.39111578\n",
      "Trained batch 991 batch loss 1.47206 epoch total loss 1.39119744\n",
      "Trained batch 992 batch loss 1.33251739 epoch total loss 1.39113832\n",
      "Trained batch 993 batch loss 1.35571051 epoch total loss 1.39110255\n",
      "Trained batch 994 batch loss 1.32643569 epoch total loss 1.39103746\n",
      "Trained batch 995 batch loss 1.31831551 epoch total loss 1.39096451\n",
      "Trained batch 996 batch loss 1.37950826 epoch total loss 1.39095294\n",
      "Trained batch 997 batch loss 1.30195737 epoch total loss 1.39086378\n",
      "Trained batch 998 batch loss 1.28315151 epoch total loss 1.39075589\n",
      "Trained batch 999 batch loss 1.30190039 epoch total loss 1.39066696\n",
      "Trained batch 1000 batch loss 1.40437496 epoch total loss 1.39068067\n",
      "Trained batch 1001 batch loss 1.37911642 epoch total loss 1.39066911\n",
      "Trained batch 1002 batch loss 1.25495458 epoch total loss 1.3905338\n",
      "Trained batch 1003 batch loss 1.33101952 epoch total loss 1.39047444\n",
      "Trained batch 1004 batch loss 1.25759792 epoch total loss 1.39034212\n",
      "Trained batch 1005 batch loss 1.37719774 epoch total loss 1.390329\n",
      "Trained batch 1006 batch loss 1.42941964 epoch total loss 1.39036787\n",
      "Trained batch 1007 batch loss 1.21303582 epoch total loss 1.39019179\n",
      "Trained batch 1008 batch loss 1.2071172 epoch total loss 1.39001012\n",
      "Trained batch 1009 batch loss 1.26327288 epoch total loss 1.38988459\n",
      "Trained batch 1010 batch loss 1.25440013 epoch total loss 1.38975048\n",
      "Trained batch 1011 batch loss 1.31438792 epoch total loss 1.38967586\n",
      "Trained batch 1012 batch loss 1.20594358 epoch total loss 1.3894943\n",
      "Trained batch 1013 batch loss 1.33737874 epoch total loss 1.3894428\n",
      "Trained batch 1014 batch loss 1.39020514 epoch total loss 1.38944364\n",
      "Trained batch 1015 batch loss 1.40458751 epoch total loss 1.38945854\n",
      "Trained batch 1016 batch loss 1.32360661 epoch total loss 1.38939369\n",
      "Trained batch 1017 batch loss 1.49007475 epoch total loss 1.38949275\n",
      "Trained batch 1018 batch loss 1.56951952 epoch total loss 1.38966966\n",
      "Trained batch 1019 batch loss 1.53659546 epoch total loss 1.3898139\n",
      "Trained batch 1020 batch loss 1.33680916 epoch total loss 1.38976192\n",
      "Trained batch 1021 batch loss 1.35492802 epoch total loss 1.38972783\n",
      "Trained batch 1022 batch loss 1.39859796 epoch total loss 1.38973641\n",
      "Trained batch 1023 batch loss 1.35254014 epoch total loss 1.3897\n",
      "Trained batch 1024 batch loss 1.49006188 epoch total loss 1.38979816\n",
      "Trained batch 1025 batch loss 1.45739031 epoch total loss 1.38986409\n",
      "Trained batch 1026 batch loss 1.40202951 epoch total loss 1.38987589\n",
      "Trained batch 1027 batch loss 1.37532187 epoch total loss 1.38986182\n",
      "Trained batch 1028 batch loss 1.34629309 epoch total loss 1.38981938\n",
      "Trained batch 1029 batch loss 1.35737026 epoch total loss 1.38978791\n",
      "Trained batch 1030 batch loss 1.4167347 epoch total loss 1.38981414\n",
      "Trained batch 1031 batch loss 1.41453707 epoch total loss 1.3898381\n",
      "Trained batch 1032 batch loss 1.5071665 epoch total loss 1.38995183\n",
      "Trained batch 1033 batch loss 1.53724408 epoch total loss 1.3900944\n",
      "Trained batch 1034 batch loss 1.44107437 epoch total loss 1.39014363\n",
      "Trained batch 1035 batch loss 1.51328683 epoch total loss 1.39026272\n",
      "Trained batch 1036 batch loss 1.46457481 epoch total loss 1.39033449\n",
      "Trained batch 1037 batch loss 1.44877708 epoch total loss 1.39039075\n",
      "Trained batch 1038 batch loss 1.50330091 epoch total loss 1.39049947\n",
      "Trained batch 1039 batch loss 1.52701616 epoch total loss 1.39063084\n",
      "Trained batch 1040 batch loss 1.29080606 epoch total loss 1.39053488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1041 batch loss 1.30172133 epoch total loss 1.39044952\n",
      "Trained batch 1042 batch loss 1.34138107 epoch total loss 1.39040256\n",
      "Trained batch 1043 batch loss 1.34346926 epoch total loss 1.39035761\n",
      "Trained batch 1044 batch loss 1.41951013 epoch total loss 1.39038551\n",
      "Trained batch 1045 batch loss 1.3677702 epoch total loss 1.39036393\n",
      "Trained batch 1046 batch loss 1.42885637 epoch total loss 1.39040065\n",
      "Trained batch 1047 batch loss 1.38524449 epoch total loss 1.39039576\n",
      "Trained batch 1048 batch loss 1.23689675 epoch total loss 1.39024937\n",
      "Trained batch 1049 batch loss 1.30853736 epoch total loss 1.39017153\n",
      "Trained batch 1050 batch loss 1.21622646 epoch total loss 1.39000583\n",
      "Trained batch 1051 batch loss 1.24399483 epoch total loss 1.38986695\n",
      "Trained batch 1052 batch loss 1.27706861 epoch total loss 1.38975966\n",
      "Trained batch 1053 batch loss 1.30008006 epoch total loss 1.38967454\n",
      "Trained batch 1054 batch loss 1.29644918 epoch total loss 1.38958609\n",
      "Trained batch 1055 batch loss 1.21517885 epoch total loss 1.38942087\n",
      "Trained batch 1056 batch loss 1.26455033 epoch total loss 1.38930261\n",
      "Trained batch 1057 batch loss 1.30030632 epoch total loss 1.38921833\n",
      "Trained batch 1058 batch loss 1.25605249 epoch total loss 1.38909256\n",
      "Trained batch 1059 batch loss 1.35222483 epoch total loss 1.38905764\n",
      "Trained batch 1060 batch loss 1.38131 epoch total loss 1.38905036\n",
      "Trained batch 1061 batch loss 1.43687701 epoch total loss 1.38909554\n",
      "Trained batch 1062 batch loss 1.3020221 epoch total loss 1.38901353\n",
      "Trained batch 1063 batch loss 1.27042925 epoch total loss 1.38890195\n",
      "Trained batch 1064 batch loss 1.22340631 epoch total loss 1.38874638\n",
      "Trained batch 1065 batch loss 1.25626 epoch total loss 1.38862193\n",
      "Trained batch 1066 batch loss 1.39960647 epoch total loss 1.3886323\n",
      "Trained batch 1067 batch loss 1.30127108 epoch total loss 1.3885504\n",
      "Trained batch 1068 batch loss 1.40118754 epoch total loss 1.38856232\n",
      "Trained batch 1069 batch loss 1.4621762 epoch total loss 1.38863111\n",
      "Trained batch 1070 batch loss 1.40271258 epoch total loss 1.38864422\n",
      "Trained batch 1071 batch loss 1.38931179 epoch total loss 1.38864481\n",
      "Trained batch 1072 batch loss 1.51505876 epoch total loss 1.38876271\n",
      "Trained batch 1073 batch loss 1.41450298 epoch total loss 1.38878679\n",
      "Trained batch 1074 batch loss 1.28845739 epoch total loss 1.38869333\n",
      "Trained batch 1075 batch loss 1.31331956 epoch total loss 1.38862324\n",
      "Trained batch 1076 batch loss 1.41456 epoch total loss 1.38864732\n",
      "Trained batch 1077 batch loss 1.28465581 epoch total loss 1.38855088\n",
      "Trained batch 1078 batch loss 1.31940556 epoch total loss 1.38848674\n",
      "Trained batch 1079 batch loss 1.24602199 epoch total loss 1.38835466\n",
      "Trained batch 1080 batch loss 1.27991521 epoch total loss 1.38825428\n",
      "Trained batch 1081 batch loss 1.29698753 epoch total loss 1.38816977\n",
      "Trained batch 1082 batch loss 1.28254509 epoch total loss 1.38807225\n",
      "Trained batch 1083 batch loss 1.35668957 epoch total loss 1.38804328\n",
      "Trained batch 1084 batch loss 1.34020102 epoch total loss 1.38799918\n",
      "Trained batch 1085 batch loss 1.29785681 epoch total loss 1.38791609\n",
      "Trained batch 1086 batch loss 1.37419796 epoch total loss 1.38790333\n",
      "Trained batch 1087 batch loss 1.23487663 epoch total loss 1.38776255\n",
      "Trained batch 1088 batch loss 1.19284964 epoch total loss 1.38758349\n",
      "Trained batch 1089 batch loss 1.16152835 epoch total loss 1.38737583\n",
      "Trained batch 1090 batch loss 1.21014285 epoch total loss 1.38721323\n",
      "Trained batch 1091 batch loss 1.7796818 epoch total loss 1.38757288\n",
      "Trained batch 1092 batch loss 1.53623366 epoch total loss 1.38770902\n",
      "Trained batch 1093 batch loss 1.33018 epoch total loss 1.38765645\n",
      "Trained batch 1094 batch loss 1.31056845 epoch total loss 1.387586\n",
      "Trained batch 1095 batch loss 1.44987464 epoch total loss 1.38764286\n",
      "Trained batch 1096 batch loss 1.25533271 epoch total loss 1.3875221\n",
      "Trained batch 1097 batch loss 1.27054942 epoch total loss 1.38741541\n",
      "Trained batch 1098 batch loss 1.25448906 epoch total loss 1.38729441\n",
      "Trained batch 1099 batch loss 1.38368356 epoch total loss 1.38729107\n",
      "Trained batch 1100 batch loss 1.38948107 epoch total loss 1.3872931\n",
      "Trained batch 1101 batch loss 1.32299352 epoch total loss 1.38723481\n",
      "Trained batch 1102 batch loss 1.36786103 epoch total loss 1.38721716\n",
      "Trained batch 1103 batch loss 1.28571463 epoch total loss 1.38712525\n",
      "Trained batch 1104 batch loss 1.29772234 epoch total loss 1.38704431\n",
      "Trained batch 1105 batch loss 1.34908855 epoch total loss 1.38701\n",
      "Trained batch 1106 batch loss 1.4113977 epoch total loss 1.38703203\n",
      "Trained batch 1107 batch loss 1.44400716 epoch total loss 1.38708341\n",
      "Trained batch 1108 batch loss 1.42576456 epoch total loss 1.38711834\n",
      "Trained batch 1109 batch loss 1.42206383 epoch total loss 1.38714993\n",
      "Trained batch 1110 batch loss 1.41822565 epoch total loss 1.38717794\n",
      "Trained batch 1111 batch loss 1.47939 epoch total loss 1.38726091\n",
      "Trained batch 1112 batch loss 1.4250083 epoch total loss 1.38729489\n",
      "Trained batch 1113 batch loss 1.27387941 epoch total loss 1.38719296\n",
      "Trained batch 1114 batch loss 1.34754252 epoch total loss 1.38715744\n",
      "Trained batch 1115 batch loss 1.2167803 epoch total loss 1.38700461\n",
      "Trained batch 1116 batch loss 1.21039581 epoch total loss 1.38684642\n",
      "Trained batch 1117 batch loss 1.18784583 epoch total loss 1.38666821\n",
      "Trained batch 1118 batch loss 1.24585164 epoch total loss 1.38654232\n",
      "Trained batch 1119 batch loss 1.14749527 epoch total loss 1.3863287\n",
      "Trained batch 1120 batch loss 1.09315443 epoch total loss 1.38606691\n",
      "Trained batch 1121 batch loss 1.06901491 epoch total loss 1.38578403\n",
      "Trained batch 1122 batch loss 1.03474391 epoch total loss 1.38547122\n",
      "Trained batch 1123 batch loss 1.32152498 epoch total loss 1.38541424\n",
      "Trained batch 1124 batch loss 1.48696613 epoch total loss 1.3855046\n",
      "Trained batch 1125 batch loss 1.37636256 epoch total loss 1.38549638\n",
      "Trained batch 1126 batch loss 1.35599017 epoch total loss 1.38547015\n",
      "Trained batch 1127 batch loss 1.30267823 epoch total loss 1.38539672\n",
      "Trained batch 1128 batch loss 1.44224536 epoch total loss 1.38544714\n",
      "Trained batch 1129 batch loss 1.41093457 epoch total loss 1.38546968\n",
      "Trained batch 1130 batch loss 1.30149186 epoch total loss 1.38539541\n",
      "Trained batch 1131 batch loss 1.37645125 epoch total loss 1.38538754\n",
      "Trained batch 1132 batch loss 1.29350245 epoch total loss 1.38530636\n",
      "Trained batch 1133 batch loss 1.42291379 epoch total loss 1.38533962\n",
      "Trained batch 1134 batch loss 1.39223647 epoch total loss 1.38534558\n",
      "Trained batch 1135 batch loss 1.38624954 epoch total loss 1.38534641\n",
      "Trained batch 1136 batch loss 1.37800074 epoch total loss 1.38534\n",
      "Trained batch 1137 batch loss 1.44569397 epoch total loss 1.38539302\n",
      "Trained batch 1138 batch loss 1.4757483 epoch total loss 1.38547242\n",
      "Trained batch 1139 batch loss 1.44865191 epoch total loss 1.38552785\n",
      "Trained batch 1140 batch loss 1.37515306 epoch total loss 1.38551867\n",
      "Trained batch 1141 batch loss 1.42087305 epoch total loss 1.38554966\n",
      "Trained batch 1142 batch loss 1.27549148 epoch total loss 1.38545334\n",
      "Trained batch 1143 batch loss 1.37029314 epoch total loss 1.38544011\n",
      "Trained batch 1144 batch loss 1.39679408 epoch total loss 1.38545\n",
      "Trained batch 1145 batch loss 1.30698347 epoch total loss 1.38538158\n",
      "Trained batch 1146 batch loss 1.36050141 epoch total loss 1.38535976\n",
      "Trained batch 1147 batch loss 1.35895014 epoch total loss 1.38533676\n",
      "Trained batch 1148 batch loss 1.37106597 epoch total loss 1.38532436\n",
      "Trained batch 1149 batch loss 1.26629913 epoch total loss 1.38522089\n",
      "Trained batch 1150 batch loss 1.36726069 epoch total loss 1.38520527\n",
      "Trained batch 1151 batch loss 1.32149 epoch total loss 1.38515\n",
      "Trained batch 1152 batch loss 1.32042229 epoch total loss 1.38509381\n",
      "Trained batch 1153 batch loss 1.27926862 epoch total loss 1.38500202\n",
      "Trained batch 1154 batch loss 1.33381319 epoch total loss 1.38495767\n",
      "Trained batch 1155 batch loss 1.28411746 epoch total loss 1.38487041\n",
      "Trained batch 1156 batch loss 1.20026135 epoch total loss 1.38471067\n",
      "Trained batch 1157 batch loss 1.22413993 epoch total loss 1.38457191\n",
      "Trained batch 1158 batch loss 1.30397058 epoch total loss 1.38450229\n",
      "Trained batch 1159 batch loss 1.20035195 epoch total loss 1.38434339\n",
      "Trained batch 1160 batch loss 1.28408217 epoch total loss 1.38425696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1161 batch loss 1.20726585 epoch total loss 1.38410449\n",
      "Trained batch 1162 batch loss 1.26672 epoch total loss 1.38400352\n",
      "Trained batch 1163 batch loss 1.45684826 epoch total loss 1.3840661\n",
      "Trained batch 1164 batch loss 1.52525568 epoch total loss 1.38418746\n",
      "Trained batch 1165 batch loss 1.41512752 epoch total loss 1.38421404\n",
      "Trained batch 1166 batch loss 1.27862608 epoch total loss 1.38412356\n",
      "Trained batch 1167 batch loss 1.57602787 epoch total loss 1.38428807\n",
      "Trained batch 1168 batch loss 1.5148437 epoch total loss 1.38439989\n",
      "Trained batch 1169 batch loss 1.47073781 epoch total loss 1.38447368\n",
      "Trained batch 1170 batch loss 1.299878 epoch total loss 1.38440144\n",
      "Trained batch 1171 batch loss 1.33805549 epoch total loss 1.38436174\n",
      "Trained batch 1172 batch loss 1.30142629 epoch total loss 1.38429093\n",
      "Trained batch 1173 batch loss 1.40224397 epoch total loss 1.38430631\n",
      "Trained batch 1174 batch loss 1.49375856 epoch total loss 1.38439953\n",
      "Trained batch 1175 batch loss 1.42117262 epoch total loss 1.38443077\n",
      "Trained batch 1176 batch loss 1.40603554 epoch total loss 1.38444912\n",
      "Trained batch 1177 batch loss 1.41075945 epoch total loss 1.38447154\n",
      "Trained batch 1178 batch loss 1.38063097 epoch total loss 1.3844682\n",
      "Trained batch 1179 batch loss 1.32214737 epoch total loss 1.38441539\n",
      "Trained batch 1180 batch loss 1.35515177 epoch total loss 1.38439047\n",
      "Trained batch 1181 batch loss 1.53808904 epoch total loss 1.38452065\n",
      "Trained batch 1182 batch loss 1.46591854 epoch total loss 1.38458955\n",
      "Trained batch 1183 batch loss 1.34458184 epoch total loss 1.3845557\n",
      "Trained batch 1184 batch loss 1.39055026 epoch total loss 1.38456082\n",
      "Trained batch 1185 batch loss 1.41547894 epoch total loss 1.38458693\n",
      "Trained batch 1186 batch loss 1.39236903 epoch total loss 1.38459349\n",
      "Trained batch 1187 batch loss 1.38874781 epoch total loss 1.38459694\n",
      "Trained batch 1188 batch loss 1.40257072 epoch total loss 1.38461208\n",
      "Trained batch 1189 batch loss 1.55048823 epoch total loss 1.38475168\n",
      "Trained batch 1190 batch loss 1.4021666 epoch total loss 1.38476634\n",
      "Trained batch 1191 batch loss 1.22042716 epoch total loss 1.38462842\n",
      "Trained batch 1192 batch loss 1.27965367 epoch total loss 1.38454032\n",
      "Trained batch 1193 batch loss 1.33858132 epoch total loss 1.38450181\n",
      "Trained batch 1194 batch loss 1.33016944 epoch total loss 1.3844564\n",
      "Trained batch 1195 batch loss 1.3195678 epoch total loss 1.38440204\n",
      "Trained batch 1196 batch loss 1.37733936 epoch total loss 1.3843962\n",
      "Trained batch 1197 batch loss 1.44938767 epoch total loss 1.38445044\n",
      "Trained batch 1198 batch loss 1.37354863 epoch total loss 1.38444126\n",
      "Trained batch 1199 batch loss 1.4203018 epoch total loss 1.38447118\n",
      "Trained batch 1200 batch loss 1.35268259 epoch total loss 1.38444471\n",
      "Trained batch 1201 batch loss 1.47697186 epoch total loss 1.38452172\n",
      "Trained batch 1202 batch loss 1.34034824 epoch total loss 1.38448489\n",
      "Trained batch 1203 batch loss 1.30425334 epoch total loss 1.38441813\n",
      "Trained batch 1204 batch loss 1.31015325 epoch total loss 1.3843565\n",
      "Trained batch 1205 batch loss 1.42052376 epoch total loss 1.38438654\n",
      "Trained batch 1206 batch loss 1.3148458 epoch total loss 1.38432884\n",
      "Trained batch 1207 batch loss 1.27616405 epoch total loss 1.3842392\n",
      "Trained batch 1208 batch loss 1.26766467 epoch total loss 1.38414276\n",
      "Trained batch 1209 batch loss 1.15565777 epoch total loss 1.38395369\n",
      "Trained batch 1210 batch loss 1.21842146 epoch total loss 1.38381696\n",
      "Trained batch 1211 batch loss 1.32217169 epoch total loss 1.38376594\n",
      "Trained batch 1212 batch loss 1.25332725 epoch total loss 1.38365829\n",
      "Trained batch 1213 batch loss 1.11583042 epoch total loss 1.38343751\n",
      "Trained batch 1214 batch loss 1.24017608 epoch total loss 1.38331962\n",
      "Trained batch 1215 batch loss 1.32166219 epoch total loss 1.38326883\n",
      "Trained batch 1216 batch loss 1.27101123 epoch total loss 1.38317645\n",
      "Trained batch 1217 batch loss 1.35803246 epoch total loss 1.38315582\n",
      "Trained batch 1218 batch loss 1.3527025 epoch total loss 1.38313079\n",
      "Trained batch 1219 batch loss 1.36091042 epoch total loss 1.38311267\n",
      "Trained batch 1220 batch loss 1.32265425 epoch total loss 1.38306308\n",
      "Trained batch 1221 batch loss 1.32787061 epoch total loss 1.3830179\n",
      "Trained batch 1222 batch loss 1.27537417 epoch total loss 1.3829298\n",
      "Trained batch 1223 batch loss 1.26711559 epoch total loss 1.38283503\n",
      "Trained batch 1224 batch loss 1.39060187 epoch total loss 1.38284147\n",
      "Trained batch 1225 batch loss 1.41158211 epoch total loss 1.38286495\n",
      "Trained batch 1226 batch loss 1.48873389 epoch total loss 1.38295126\n",
      "Trained batch 1227 batch loss 1.40715277 epoch total loss 1.38297093\n",
      "Trained batch 1228 batch loss 1.32588089 epoch total loss 1.38292456\n",
      "Trained batch 1229 batch loss 1.39118862 epoch total loss 1.38293135\n",
      "Trained batch 1230 batch loss 1.39984357 epoch total loss 1.38294506\n",
      "Trained batch 1231 batch loss 1.39898562 epoch total loss 1.38295805\n",
      "Trained batch 1232 batch loss 1.35093307 epoch total loss 1.38293207\n",
      "Trained batch 1233 batch loss 1.30299854 epoch total loss 1.38286722\n",
      "Trained batch 1234 batch loss 1.24596667 epoch total loss 1.38275635\n",
      "Trained batch 1235 batch loss 1.16487718 epoch total loss 1.38257992\n",
      "Trained batch 1236 batch loss 1.27350914 epoch total loss 1.38249171\n",
      "Trained batch 1237 batch loss 1.30918765 epoch total loss 1.38243246\n",
      "Trained batch 1238 batch loss 1.2286644 epoch total loss 1.38230824\n",
      "Trained batch 1239 batch loss 1.29933023 epoch total loss 1.38224125\n",
      "Trained batch 1240 batch loss 1.37878287 epoch total loss 1.38223851\n",
      "Trained batch 1241 batch loss 1.2094276 epoch total loss 1.38209927\n",
      "Trained batch 1242 batch loss 1.31933641 epoch total loss 1.38204873\n",
      "Trained batch 1243 batch loss 1.24069047 epoch total loss 1.381935\n",
      "Trained batch 1244 batch loss 1.24063134 epoch total loss 1.38182139\n",
      "Trained batch 1245 batch loss 1.20794523 epoch total loss 1.38168168\n",
      "Trained batch 1246 batch loss 1.42912829 epoch total loss 1.38171971\n",
      "Trained batch 1247 batch loss 1.3028028 epoch total loss 1.38165653\n",
      "Trained batch 1248 batch loss 1.16830266 epoch total loss 1.38148558\n",
      "Trained batch 1249 batch loss 1.28250742 epoch total loss 1.38140631\n",
      "Trained batch 1250 batch loss 1.29648829 epoch total loss 1.38133836\n",
      "Trained batch 1251 batch loss 1.38630271 epoch total loss 1.38134241\n",
      "Trained batch 1252 batch loss 1.36498511 epoch total loss 1.3813293\n",
      "Trained batch 1253 batch loss 1.3574456 epoch total loss 1.38131022\n",
      "Trained batch 1254 batch loss 1.23295534 epoch total loss 1.38119185\n",
      "Trained batch 1255 batch loss 1.24247801 epoch total loss 1.38108134\n",
      "Trained batch 1256 batch loss 1.21494484 epoch total loss 1.38094914\n",
      "Trained batch 1257 batch loss 1.32017279 epoch total loss 1.38090074\n",
      "Trained batch 1258 batch loss 1.40120339 epoch total loss 1.38091695\n",
      "Trained batch 1259 batch loss 1.4357444 epoch total loss 1.38096046\n",
      "Trained batch 1260 batch loss 1.49137139 epoch total loss 1.38104808\n",
      "Trained batch 1261 batch loss 1.30816507 epoch total loss 1.38099027\n",
      "Trained batch 1262 batch loss 1.27665901 epoch total loss 1.38090754\n",
      "Trained batch 1263 batch loss 1.24692953 epoch total loss 1.38080144\n",
      "Trained batch 1264 batch loss 1.25910747 epoch total loss 1.38070524\n",
      "Trained batch 1265 batch loss 1.33855259 epoch total loss 1.38067186\n",
      "Trained batch 1266 batch loss 1.3497622 epoch total loss 1.38064742\n",
      "Trained batch 1267 batch loss 1.3187387 epoch total loss 1.38059855\n",
      "Trained batch 1268 batch loss 1.48857152 epoch total loss 1.38068366\n",
      "Trained batch 1269 batch loss 1.47983336 epoch total loss 1.38076186\n",
      "Trained batch 1270 batch loss 1.47553313 epoch total loss 1.38083649\n",
      "Trained batch 1271 batch loss 1.31871104 epoch total loss 1.38078761\n",
      "Trained batch 1272 batch loss 1.32500505 epoch total loss 1.38074374\n",
      "Trained batch 1273 batch loss 1.29921556 epoch total loss 1.38067973\n",
      "Trained batch 1274 batch loss 1.3484745 epoch total loss 1.38065445\n",
      "Trained batch 1275 batch loss 1.31744933 epoch total loss 1.38060486\n",
      "Trained batch 1276 batch loss 1.27953827 epoch total loss 1.38052571\n",
      "Trained batch 1277 batch loss 1.23159444 epoch total loss 1.380409\n",
      "Trained batch 1278 batch loss 1.24754059 epoch total loss 1.38030505\n",
      "Trained batch 1279 batch loss 1.32757211 epoch total loss 1.38026381\n",
      "Trained batch 1280 batch loss 1.29989755 epoch total loss 1.3802011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1281 batch loss 1.39404726 epoch total loss 1.38021183\n",
      "Trained batch 1282 batch loss 1.31679416 epoch total loss 1.38016236\n",
      "Trained batch 1283 batch loss 1.2846899 epoch total loss 1.38008797\n",
      "Trained batch 1284 batch loss 1.42605627 epoch total loss 1.38012373\n",
      "Trained batch 1285 batch loss 1.37957072 epoch total loss 1.38012326\n",
      "Trained batch 1286 batch loss 1.28403449 epoch total loss 1.38004851\n",
      "Trained batch 1287 batch loss 1.41074467 epoch total loss 1.38007247\n",
      "Trained batch 1288 batch loss 1.21648049 epoch total loss 1.3799454\n",
      "Trained batch 1289 batch loss 1.24477 epoch total loss 1.37984049\n",
      "Trained batch 1290 batch loss 1.26046348 epoch total loss 1.37974799\n",
      "Trained batch 1291 batch loss 1.25589621 epoch total loss 1.37965202\n",
      "Trained batch 1292 batch loss 1.3164655 epoch total loss 1.37960303\n",
      "Trained batch 1293 batch loss 1.51770949 epoch total loss 1.37970984\n",
      "Trained batch 1294 batch loss 1.52899635 epoch total loss 1.37982523\n",
      "Trained batch 1295 batch loss 1.52532208 epoch total loss 1.37993753\n",
      "Trained batch 1296 batch loss 1.53547072 epoch total loss 1.38005757\n",
      "Trained batch 1297 batch loss 1.27337062 epoch total loss 1.37997532\n",
      "Trained batch 1298 batch loss 1.28232992 epoch total loss 1.3799001\n",
      "Trained batch 1299 batch loss 1.09215534 epoch total loss 1.37967861\n",
      "Trained batch 1300 batch loss 1.09655607 epoch total loss 1.37946081\n",
      "Trained batch 1301 batch loss 1.13211739 epoch total loss 1.37927067\n",
      "Trained batch 1302 batch loss 1.27386391 epoch total loss 1.37918973\n",
      "Trained batch 1303 batch loss 1.31020617 epoch total loss 1.37913668\n",
      "Trained batch 1304 batch loss 1.29738426 epoch total loss 1.37907398\n",
      "Trained batch 1305 batch loss 1.3063786 epoch total loss 1.37901831\n",
      "Trained batch 1306 batch loss 1.19145107 epoch total loss 1.37887466\n",
      "Trained batch 1307 batch loss 1.19651 epoch total loss 1.37873518\n",
      "Trained batch 1308 batch loss 1.14783871 epoch total loss 1.37855864\n",
      "Trained batch 1309 batch loss 1.18881893 epoch total loss 1.37841368\n",
      "Trained batch 1310 batch loss 1.28195691 epoch total loss 1.37834013\n",
      "Trained batch 1311 batch loss 1.26929867 epoch total loss 1.37825692\n",
      "Trained batch 1312 batch loss 1.38241196 epoch total loss 1.37826014\n",
      "Trained batch 1313 batch loss 1.18225312 epoch total loss 1.37811077\n",
      "Trained batch 1314 batch loss 1.29110336 epoch total loss 1.37804461\n",
      "Trained batch 1315 batch loss 1.37793934 epoch total loss 1.37804449\n",
      "Trained batch 1316 batch loss 1.35172093 epoch total loss 1.37802446\n",
      "Trained batch 1317 batch loss 1.44292927 epoch total loss 1.37807369\n",
      "Trained batch 1318 batch loss 1.44766951 epoch total loss 1.3781265\n",
      "Trained batch 1319 batch loss 1.25106204 epoch total loss 1.37803018\n",
      "Trained batch 1320 batch loss 1.38602829 epoch total loss 1.37803626\n",
      "Trained batch 1321 batch loss 1.35399771 epoch total loss 1.37801802\n",
      "Trained batch 1322 batch loss 1.30460012 epoch total loss 1.37796247\n",
      "Trained batch 1323 batch loss 1.3183434 epoch total loss 1.37791741\n",
      "Trained batch 1324 batch loss 1.37874711 epoch total loss 1.37791812\n",
      "Trained batch 1325 batch loss 1.32501686 epoch total loss 1.37787819\n",
      "Trained batch 1326 batch loss 1.32209301 epoch total loss 1.37783611\n",
      "Trained batch 1327 batch loss 1.24861586 epoch total loss 1.37773883\n",
      "Trained batch 1328 batch loss 1.24376512 epoch total loss 1.37763798\n",
      "Trained batch 1329 batch loss 1.38414729 epoch total loss 1.37764287\n",
      "Trained batch 1330 batch loss 1.3068949 epoch total loss 1.37758958\n",
      "Trained batch 1331 batch loss 1.31997478 epoch total loss 1.37754631\n",
      "Trained batch 1332 batch loss 1.33453286 epoch total loss 1.377514\n",
      "Trained batch 1333 batch loss 1.31693137 epoch total loss 1.37746847\n",
      "Trained batch 1334 batch loss 1.3439455 epoch total loss 1.37744343\n",
      "Trained batch 1335 batch loss 1.24129 epoch total loss 1.37734151\n",
      "Trained batch 1336 batch loss 1.29789519 epoch total loss 1.37728202\n",
      "Trained batch 1337 batch loss 1.29497755 epoch total loss 1.37722039\n",
      "Trained batch 1338 batch loss 1.26169455 epoch total loss 1.37713408\n",
      "Trained batch 1339 batch loss 1.25269723 epoch total loss 1.3770411\n",
      "Trained batch 1340 batch loss 1.35269141 epoch total loss 1.37702286\n",
      "Trained batch 1341 batch loss 1.34319 epoch total loss 1.37699759\n",
      "Trained batch 1342 batch loss 1.30498075 epoch total loss 1.37694395\n",
      "Trained batch 1343 batch loss 1.2414794 epoch total loss 1.37684309\n",
      "Trained batch 1344 batch loss 1.2306174 epoch total loss 1.37673426\n",
      "Trained batch 1345 batch loss 1.26249552 epoch total loss 1.37664926\n",
      "Trained batch 1346 batch loss 1.36543155 epoch total loss 1.37664092\n",
      "Trained batch 1347 batch loss 1.35989094 epoch total loss 1.37662852\n",
      "Trained batch 1348 batch loss 1.45195532 epoch total loss 1.37668431\n",
      "Trained batch 1349 batch loss 1.3467865 epoch total loss 1.37666225\n",
      "Trained batch 1350 batch loss 1.13755989 epoch total loss 1.37648511\n",
      "Trained batch 1351 batch loss 1.10175943 epoch total loss 1.37628174\n",
      "Trained batch 1352 batch loss 1.36282229 epoch total loss 1.37627184\n",
      "Trained batch 1353 batch loss 1.38849831 epoch total loss 1.3762809\n",
      "Trained batch 1354 batch loss 1.34242141 epoch total loss 1.37625587\n",
      "Trained batch 1355 batch loss 1.46337938 epoch total loss 1.37632012\n",
      "Trained batch 1356 batch loss 1.4108907 epoch total loss 1.37634563\n",
      "Trained batch 1357 batch loss 1.40678549 epoch total loss 1.37636805\n",
      "Trained batch 1358 batch loss 1.35343611 epoch total loss 1.37635112\n",
      "Trained batch 1359 batch loss 1.42848301 epoch total loss 1.3763895\n",
      "Trained batch 1360 batch loss 1.3905704 epoch total loss 1.3764\n",
      "Trained batch 1361 batch loss 1.37172389 epoch total loss 1.37639654\n",
      "Trained batch 1362 batch loss 1.30166364 epoch total loss 1.37634158\n",
      "Trained batch 1363 batch loss 1.41244161 epoch total loss 1.37636817\n",
      "Trained batch 1364 batch loss 1.39542627 epoch total loss 1.37638211\n",
      "Trained batch 1365 batch loss 1.43810284 epoch total loss 1.37642729\n",
      "Trained batch 1366 batch loss 1.31039929 epoch total loss 1.37637901\n",
      "Trained batch 1367 batch loss 1.27440763 epoch total loss 1.37630439\n",
      "Trained batch 1368 batch loss 1.23795664 epoch total loss 1.37620318\n",
      "Trained batch 1369 batch loss 1.27087331 epoch total loss 1.37612629\n",
      "Trained batch 1370 batch loss 1.27476323 epoch total loss 1.37605226\n",
      "Trained batch 1371 batch loss 1.28234613 epoch total loss 1.37598395\n",
      "Trained batch 1372 batch loss 1.27672589 epoch total loss 1.37591159\n",
      "Trained batch 1373 batch loss 1.3768599 epoch total loss 1.37591231\n",
      "Trained batch 1374 batch loss 1.37693906 epoch total loss 1.37591302\n",
      "Trained batch 1375 batch loss 1.39297318 epoch total loss 1.37592542\n",
      "Trained batch 1376 batch loss 1.34922767 epoch total loss 1.37590599\n",
      "Trained batch 1377 batch loss 1.33501816 epoch total loss 1.37587631\n",
      "Trained batch 1378 batch loss 1.2489984 epoch total loss 1.37578428\n",
      "Trained batch 1379 batch loss 1.19506073 epoch total loss 1.37565315\n",
      "Trained batch 1380 batch loss 1.2114985 epoch total loss 1.3755343\n",
      "Trained batch 1381 batch loss 1.30458939 epoch total loss 1.37548292\n",
      "Trained batch 1382 batch loss 1.46386898 epoch total loss 1.37554681\n",
      "Trained batch 1383 batch loss 1.35641408 epoch total loss 1.37553298\n",
      "Trained batch 1384 batch loss 1.2703743 epoch total loss 1.37545705\n",
      "Trained batch 1385 batch loss 1.30689132 epoch total loss 1.37540758\n",
      "Trained batch 1386 batch loss 1.2582891 epoch total loss 1.37532306\n",
      "Trained batch 1387 batch loss 1.14436018 epoch total loss 1.37515652\n",
      "Trained batch 1388 batch loss 1.24395263 epoch total loss 1.37506199\n",
      "Epoch 2 train loss 1.3750619888305664\n",
      "Validated batch 1 batch loss 1.33140349\n",
      "Validated batch 2 batch loss 1.19621181\n",
      "Validated batch 3 batch loss 1.34406054\n",
      "Validated batch 4 batch loss 1.24374866\n",
      "Validated batch 5 batch loss 1.34386563\n",
      "Validated batch 6 batch loss 1.36078584\n",
      "Validated batch 7 batch loss 1.37339211\n",
      "Validated batch 8 batch loss 1.46877718\n",
      "Validated batch 9 batch loss 1.42454302\n",
      "Validated batch 10 batch loss 1.37107682\n",
      "Validated batch 11 batch loss 1.35208225\n",
      "Validated batch 12 batch loss 1.42756224\n",
      "Validated batch 13 batch loss 1.42417336\n",
      "Validated batch 14 batch loss 1.42017913\n",
      "Validated batch 15 batch loss 1.42382693\n",
      "Validated batch 16 batch loss 1.41035378\n",
      "Validated batch 17 batch loss 1.37906504\n",
      "Validated batch 18 batch loss 1.23992193\n",
      "Validated batch 19 batch loss 1.31149089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 20 batch loss 1.38090563\n",
      "Validated batch 21 batch loss 1.35661793\n",
      "Validated batch 22 batch loss 1.38749623\n",
      "Validated batch 23 batch loss 1.31820154\n",
      "Validated batch 24 batch loss 1.331038\n",
      "Validated batch 25 batch loss 1.372931\n",
      "Validated batch 26 batch loss 1.23100567\n",
      "Validated batch 27 batch loss 1.39439321\n",
      "Validated batch 28 batch loss 1.36232567\n",
      "Validated batch 29 batch loss 1.35804176\n",
      "Validated batch 30 batch loss 1.45214748\n",
      "Validated batch 31 batch loss 1.39438379\n",
      "Validated batch 32 batch loss 1.39966047\n",
      "Validated batch 33 batch loss 1.35993862\n",
      "Validated batch 34 batch loss 1.42638552\n",
      "Validated batch 35 batch loss 1.34632051\n",
      "Validated batch 36 batch loss 1.30395007\n",
      "Validated batch 37 batch loss 1.42641294\n",
      "Validated batch 38 batch loss 1.41528106\n",
      "Validated batch 39 batch loss 1.3277477\n",
      "Validated batch 40 batch loss 1.52216959\n",
      "Validated batch 41 batch loss 1.15463972\n",
      "Validated batch 42 batch loss 1.41742599\n",
      "Validated batch 43 batch loss 1.16645622\n",
      "Validated batch 44 batch loss 1.35889387\n",
      "Validated batch 45 batch loss 1.44168055\n",
      "Validated batch 46 batch loss 1.26191044\n",
      "Validated batch 47 batch loss 1.2318151\n",
      "Validated batch 48 batch loss 1.40806973\n",
      "Validated batch 49 batch loss 1.39153469\n",
      "Validated batch 50 batch loss 1.27789509\n",
      "Validated batch 51 batch loss 1.42102325\n",
      "Validated batch 52 batch loss 1.47477865\n",
      "Validated batch 53 batch loss 1.18597651\n",
      "Validated batch 54 batch loss 1.41451275\n",
      "Validated batch 55 batch loss 1.3155992\n",
      "Validated batch 56 batch loss 1.3983984\n",
      "Validated batch 57 batch loss 1.40164185\n",
      "Validated batch 58 batch loss 1.20676494\n",
      "Validated batch 59 batch loss 1.17505217\n",
      "Validated batch 60 batch loss 1.31446052\n",
      "Validated batch 61 batch loss 1.31512189\n",
      "Validated batch 62 batch loss 1.25716186\n",
      "Validated batch 63 batch loss 1.34818947\n",
      "Validated batch 64 batch loss 1.30566943\n",
      "Validated batch 65 batch loss 1.3823669\n",
      "Validated batch 66 batch loss 1.42252362\n",
      "Validated batch 67 batch loss 1.37782145\n",
      "Validated batch 68 batch loss 1.36538887\n",
      "Validated batch 69 batch loss 1.23342681\n",
      "Validated batch 70 batch loss 1.37061\n",
      "Validated batch 71 batch loss 1.4250555\n",
      "Validated batch 72 batch loss 1.23666799\n",
      "Validated batch 73 batch loss 1.25442505\n",
      "Validated batch 74 batch loss 1.37679708\n",
      "Validated batch 75 batch loss 1.30449402\n",
      "Validated batch 76 batch loss 1.30314219\n",
      "Validated batch 77 batch loss 1.30511832\n",
      "Validated batch 78 batch loss 1.27700591\n",
      "Validated batch 79 batch loss 1.45461237\n",
      "Validated batch 80 batch loss 1.22363734\n",
      "Validated batch 81 batch loss 1.19700789\n",
      "Validated batch 82 batch loss 1.39504719\n",
      "Validated batch 83 batch loss 1.42660379\n",
      "Validated batch 84 batch loss 1.43139374\n",
      "Validated batch 85 batch loss 1.49591255\n",
      "Validated batch 86 batch loss 1.24695742\n",
      "Validated batch 87 batch loss 1.51088822\n",
      "Validated batch 88 batch loss 1.31289697\n",
      "Validated batch 89 batch loss 1.40935183\n",
      "Validated batch 90 batch loss 1.40171468\n",
      "Validated batch 91 batch loss 1.12585437\n",
      "Validated batch 92 batch loss 1.37949634\n",
      "Validated batch 93 batch loss 1.37992835\n",
      "Validated batch 94 batch loss 1.16818821\n",
      "Validated batch 95 batch loss 1.33436775\n",
      "Validated batch 96 batch loss 1.24367905\n",
      "Validated batch 97 batch loss 1.30263531\n",
      "Validated batch 98 batch loss 1.35733616\n",
      "Validated batch 99 batch loss 1.35500145\n",
      "Validated batch 100 batch loss 1.33183098\n",
      "Validated batch 101 batch loss 1.34356487\n",
      "Validated batch 102 batch loss 1.29180527\n",
      "Validated batch 103 batch loss 1.43493235\n",
      "Validated batch 104 batch loss 1.34936011\n",
      "Validated batch 105 batch loss 1.28182065\n",
      "Validated batch 106 batch loss 1.27495027\n",
      "Validated batch 107 batch loss 1.3934226\n",
      "Validated batch 108 batch loss 1.30143178\n",
      "Validated batch 109 batch loss 1.47826\n",
      "Validated batch 110 batch loss 1.36116874\n",
      "Validated batch 111 batch loss 1.29911458\n",
      "Validated batch 112 batch loss 1.37776136\n",
      "Validated batch 113 batch loss 1.30659366\n",
      "Validated batch 114 batch loss 1.28993118\n",
      "Validated batch 115 batch loss 1.30168247\n",
      "Validated batch 116 batch loss 1.32656336\n",
      "Validated batch 117 batch loss 1.37114739\n",
      "Validated batch 118 batch loss 1.3134861\n",
      "Validated batch 119 batch loss 1.19593883\n",
      "Validated batch 120 batch loss 1.2529037\n",
      "Validated batch 121 batch loss 1.45462751\n",
      "Validated batch 122 batch loss 1.24889433\n",
      "Validated batch 123 batch loss 1.23059666\n",
      "Validated batch 124 batch loss 1.31763422\n",
      "Validated batch 125 batch loss 1.33517075\n",
      "Validated batch 126 batch loss 1.26748872\n",
      "Validated batch 127 batch loss 1.34747767\n",
      "Validated batch 128 batch loss 1.29212213\n",
      "Validated batch 129 batch loss 1.267308\n",
      "Validated batch 130 batch loss 1.39897192\n",
      "Validated batch 131 batch loss 1.4535296\n",
      "Validated batch 132 batch loss 1.26748204\n",
      "Validated batch 133 batch loss 1.48475182\n",
      "Validated batch 134 batch loss 1.17453229\n",
      "Validated batch 135 batch loss 1.24984384\n",
      "Validated batch 136 batch loss 1.2869556\n",
      "Validated batch 137 batch loss 1.35510373\n",
      "Validated batch 138 batch loss 1.41478586\n",
      "Validated batch 139 batch loss 1.41770434\n",
      "Validated batch 140 batch loss 1.33734417\n",
      "Validated batch 141 batch loss 1.3526957\n",
      "Validated batch 142 batch loss 1.30708933\n",
      "Validated batch 143 batch loss 1.3370235\n",
      "Validated batch 144 batch loss 1.50490355\n",
      "Validated batch 145 batch loss 1.27065945\n",
      "Validated batch 146 batch loss 1.41197431\n",
      "Validated batch 147 batch loss 1.31398487\n",
      "Validated batch 148 batch loss 1.41041374\n",
      "Validated batch 149 batch loss 1.31851459\n",
      "Validated batch 150 batch loss 1.26141834\n",
      "Validated batch 151 batch loss 1.37272072\n",
      "Validated batch 152 batch loss 1.39740443\n",
      "Validated batch 153 batch loss 1.40833426\n",
      "Validated batch 154 batch loss 1.31607091\n",
      "Validated batch 155 batch loss 1.35400581\n",
      "Validated batch 156 batch loss 1.32369184\n",
      "Validated batch 157 batch loss 1.2467227\n",
      "Validated batch 158 batch loss 1.32866728\n",
      "Validated batch 159 batch loss 1.27825379\n",
      "Validated batch 160 batch loss 1.32893479\n",
      "Validated batch 161 batch loss 1.35868192\n",
      "Validated batch 162 batch loss 1.50681376\n",
      "Validated batch 163 batch loss 1.32733476\n",
      "Validated batch 164 batch loss 1.35964799\n",
      "Validated batch 165 batch loss 1.32896042\n",
      "Validated batch 166 batch loss 1.24693334\n",
      "Validated batch 167 batch loss 1.40353334\n",
      "Validated batch 168 batch loss 1.36695039\n",
      "Validated batch 169 batch loss 1.25536025\n",
      "Validated batch 170 batch loss 1.24534917\n",
      "Validated batch 171 batch loss 1.36229789\n",
      "Validated batch 172 batch loss 1.33541989\n",
      "Validated batch 173 batch loss 1.4219265\n",
      "Validated batch 174 batch loss 1.33808184\n",
      "Validated batch 175 batch loss 1.26223588\n",
      "Validated batch 176 batch loss 1.32628286\n",
      "Validated batch 177 batch loss 1.33442664\n",
      "Validated batch 178 batch loss 1.33396339\n",
      "Validated batch 179 batch loss 1.37539625\n",
      "Validated batch 180 batch loss 1.4196806\n",
      "Validated batch 181 batch loss 1.57025146\n",
      "Validated batch 182 batch loss 1.5266012\n",
      "Validated batch 183 batch loss 1.37176096\n",
      "Validated batch 184 batch loss 1.25207603\n",
      "Validated batch 185 batch loss 1.1980114\n",
      "Epoch 2 val loss 1.3421480655670166\n",
      "Model /aiffel/aiffel/mpii/my_models/model-epoch-2-loss-1.3421.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.26549613 epoch total loss 1.26549613\n",
      "Trained batch 2 batch loss 1.24534214 epoch total loss 1.25541914\n",
      "Trained batch 3 batch loss 1.21359146 epoch total loss 1.24147654\n",
      "Trained batch 4 batch loss 1.29916573 epoch total loss 1.25589883\n",
      "Trained batch 5 batch loss 1.40267706 epoch total loss 1.28525448\n",
      "Trained batch 6 batch loss 1.38331044 epoch total loss 1.30159712\n",
      "Trained batch 7 batch loss 1.35882449 epoch total loss 1.30977249\n",
      "Trained batch 8 batch loss 1.35126519 epoch total loss 1.31495905\n",
      "Trained batch 9 batch loss 1.4318465 epoch total loss 1.32794654\n",
      "Trained batch 10 batch loss 1.31959391 epoch total loss 1.32711124\n",
      "Trained batch 11 batch loss 1.38261783 epoch total loss 1.33215725\n",
      "Trained batch 12 batch loss 1.30337167 epoch total loss 1.32975852\n",
      "Trained batch 13 batch loss 1.28500772 epoch total loss 1.32631612\n",
      "Trained batch 14 batch loss 1.20815599 epoch total loss 1.3178761\n",
      "Trained batch 15 batch loss 1.37836683 epoch total loss 1.32190883\n",
      "Trained batch 16 batch loss 1.31542146 epoch total loss 1.3215034\n",
      "Trained batch 17 batch loss 1.43355918 epoch total loss 1.32809496\n",
      "Trained batch 18 batch loss 1.36208081 epoch total loss 1.32998312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 19 batch loss 1.36958146 epoch total loss 1.33206713\n",
      "Trained batch 20 batch loss 1.28870034 epoch total loss 1.32989883\n",
      "Trained batch 21 batch loss 1.33737087 epoch total loss 1.33025467\n",
      "Trained batch 22 batch loss 1.31773102 epoch total loss 1.32968545\n",
      "Trained batch 23 batch loss 1.2166183 epoch total loss 1.3247695\n",
      "Trained batch 24 batch loss 1.27658892 epoch total loss 1.32276189\n",
      "Trained batch 25 batch loss 1.21640658 epoch total loss 1.31850767\n",
      "Trained batch 26 batch loss 1.26509154 epoch total loss 1.31645322\n",
      "Trained batch 27 batch loss 1.27298391 epoch total loss 1.31484318\n",
      "Trained batch 28 batch loss 1.23505044 epoch total loss 1.31199348\n",
      "Trained batch 29 batch loss 1.13507092 epoch total loss 1.30589271\n",
      "Trained batch 30 batch loss 1.21413672 epoch total loss 1.30283415\n",
      "Trained batch 31 batch loss 1.23776174 epoch total loss 1.30073512\n",
      "Trained batch 32 batch loss 1.23057723 epoch total loss 1.29854262\n",
      "Trained batch 33 batch loss 1.24426913 epoch total loss 1.29689801\n",
      "Trained batch 34 batch loss 1.14235699 epoch total loss 1.29235268\n",
      "Trained batch 35 batch loss 1.20595884 epoch total loss 1.28988433\n",
      "Trained batch 36 batch loss 1.26351964 epoch total loss 1.28915191\n",
      "Trained batch 37 batch loss 1.28071642 epoch total loss 1.28892398\n",
      "Trained batch 38 batch loss 1.28782821 epoch total loss 1.28889501\n",
      "Trained batch 39 batch loss 1.34172177 epoch total loss 1.29024959\n",
      "Trained batch 40 batch loss 1.38892579 epoch total loss 1.2927165\n",
      "Trained batch 41 batch loss 1.40385222 epoch total loss 1.29542708\n",
      "Trained batch 42 batch loss 1.38587868 epoch total loss 1.29758072\n",
      "Trained batch 43 batch loss 1.35236073 epoch total loss 1.29885471\n",
      "Trained batch 44 batch loss 1.29527462 epoch total loss 1.29877329\n",
      "Trained batch 45 batch loss 1.36750102 epoch total loss 1.30030048\n",
      "Trained batch 46 batch loss 1.29947209 epoch total loss 1.30028248\n",
      "Trained batch 47 batch loss 1.37557101 epoch total loss 1.30188441\n",
      "Trained batch 48 batch loss 1.36332655 epoch total loss 1.30316448\n",
      "Trained batch 49 batch loss 1.34882104 epoch total loss 1.30409622\n",
      "Trained batch 50 batch loss 1.34644723 epoch total loss 1.3049432\n",
      "Trained batch 51 batch loss 1.41841054 epoch total loss 1.30716813\n",
      "Trained batch 52 batch loss 1.34816396 epoch total loss 1.30795658\n",
      "Trained batch 53 batch loss 1.32159734 epoch total loss 1.30821383\n",
      "Trained batch 54 batch loss 1.41146135 epoch total loss 1.31012583\n",
      "Trained batch 55 batch loss 1.39003992 epoch total loss 1.31157875\n",
      "Trained batch 56 batch loss 1.34469581 epoch total loss 1.31217015\n",
      "Trained batch 57 batch loss 1.43048155 epoch total loss 1.31424582\n",
      "Trained batch 58 batch loss 1.27836275 epoch total loss 1.31362712\n",
      "Trained batch 59 batch loss 1.36624527 epoch total loss 1.31451905\n",
      "Trained batch 60 batch loss 1.31079578 epoch total loss 1.31445706\n",
      "Trained batch 61 batch loss 1.30831695 epoch total loss 1.31435645\n",
      "Trained batch 62 batch loss 1.32758129 epoch total loss 1.31456983\n",
      "Trained batch 63 batch loss 1.21878207 epoch total loss 1.31304932\n",
      "Trained batch 64 batch loss 1.22590983 epoch total loss 1.31168771\n",
      "Trained batch 65 batch loss 1.22087264 epoch total loss 1.31029058\n",
      "Trained batch 66 batch loss 1.27449775 epoch total loss 1.30974817\n",
      "Trained batch 67 batch loss 1.29913449 epoch total loss 1.30958974\n",
      "Trained batch 68 batch loss 1.22239864 epoch total loss 1.30830753\n",
      "Trained batch 69 batch loss 1.17459488 epoch total loss 1.30636978\n",
      "Trained batch 70 batch loss 1.13532543 epoch total loss 1.30392623\n",
      "Trained batch 71 batch loss 1.32318354 epoch total loss 1.30419743\n",
      "Trained batch 72 batch loss 1.26199269 epoch total loss 1.30361128\n",
      "Trained batch 73 batch loss 1.34975171 epoch total loss 1.30424333\n",
      "Trained batch 74 batch loss 1.32460093 epoch total loss 1.30451846\n",
      "Trained batch 75 batch loss 1.24081779 epoch total loss 1.30366898\n",
      "Trained batch 76 batch loss 1.28158069 epoch total loss 1.30337834\n",
      "Trained batch 77 batch loss 1.29515028 epoch total loss 1.30327153\n",
      "Trained batch 78 batch loss 1.35174942 epoch total loss 1.30389309\n",
      "Trained batch 79 batch loss 1.3157661 epoch total loss 1.30404329\n",
      "Trained batch 80 batch loss 1.30695939 epoch total loss 1.30407977\n",
      "Trained batch 81 batch loss 1.28966761 epoch total loss 1.30390191\n",
      "Trained batch 82 batch loss 1.1808 epoch total loss 1.30240059\n",
      "Trained batch 83 batch loss 1.25717854 epoch total loss 1.3018558\n",
      "Trained batch 84 batch loss 1.24443173 epoch total loss 1.30117214\n",
      "Trained batch 85 batch loss 1.29918504 epoch total loss 1.30114877\n",
      "Trained batch 86 batch loss 1.3081758 epoch total loss 1.30123055\n",
      "Trained batch 87 batch loss 1.37765801 epoch total loss 1.30210888\n",
      "Trained batch 88 batch loss 1.29715705 epoch total loss 1.30205262\n",
      "Trained batch 89 batch loss 1.30583608 epoch total loss 1.30209517\n",
      "Trained batch 90 batch loss 1.30303836 epoch total loss 1.30210567\n",
      "Trained batch 91 batch loss 1.38723421 epoch total loss 1.30304122\n",
      "Trained batch 92 batch loss 1.12849307 epoch total loss 1.301144\n",
      "Trained batch 93 batch loss 1.23681474 epoch total loss 1.30045223\n",
      "Trained batch 94 batch loss 1.32407558 epoch total loss 1.30070353\n",
      "Trained batch 95 batch loss 1.29630375 epoch total loss 1.30065727\n",
      "Trained batch 96 batch loss 1.35347748 epoch total loss 1.30120742\n",
      "Trained batch 97 batch loss 1.21533465 epoch total loss 1.30032218\n",
      "Trained batch 98 batch loss 1.36007345 epoch total loss 1.30093193\n",
      "Trained batch 99 batch loss 1.29799879 epoch total loss 1.30090225\n",
      "Trained batch 100 batch loss 1.19109225 epoch total loss 1.29980409\n",
      "Trained batch 101 batch loss 1.26983476 epoch total loss 1.29950738\n",
      "Trained batch 102 batch loss 1.47472322 epoch total loss 1.30122507\n",
      "Trained batch 103 batch loss 1.27862561 epoch total loss 1.30100572\n",
      "Trained batch 104 batch loss 1.31308675 epoch total loss 1.30112183\n",
      "Trained batch 105 batch loss 1.31261373 epoch total loss 1.30123115\n",
      "Trained batch 106 batch loss 1.17898142 epoch total loss 1.30007792\n",
      "Trained batch 107 batch loss 1.23821378 epoch total loss 1.29949975\n",
      "Trained batch 108 batch loss 1.38942122 epoch total loss 1.30033243\n",
      "Trained batch 109 batch loss 1.24040699 epoch total loss 1.29978251\n",
      "Trained batch 110 batch loss 1.27367544 epoch total loss 1.29954529\n",
      "Trained batch 111 batch loss 1.32680297 epoch total loss 1.29979086\n",
      "Trained batch 112 batch loss 1.24593651 epoch total loss 1.29931\n",
      "Trained batch 113 batch loss 1.20316148 epoch total loss 1.29845905\n",
      "Trained batch 114 batch loss 1.2830838 epoch total loss 1.29832423\n",
      "Trained batch 115 batch loss 1.32326078 epoch total loss 1.29854095\n",
      "Trained batch 116 batch loss 1.41124415 epoch total loss 1.29951251\n",
      "Trained batch 117 batch loss 1.18134785 epoch total loss 1.29850256\n",
      "Trained batch 118 batch loss 1.26023221 epoch total loss 1.29817832\n",
      "Trained batch 119 batch loss 1.4919045 epoch total loss 1.29980624\n",
      "Trained batch 120 batch loss 1.26436877 epoch total loss 1.29951096\n",
      "Trained batch 121 batch loss 1.29896045 epoch total loss 1.29950643\n",
      "Trained batch 122 batch loss 1.28084183 epoch total loss 1.29935348\n",
      "Trained batch 123 batch loss 1.30298936 epoch total loss 1.29938304\n",
      "Trained batch 124 batch loss 1.33951175 epoch total loss 1.29970658\n",
      "Trained batch 125 batch loss 1.42333043 epoch total loss 1.30069554\n",
      "Trained batch 126 batch loss 1.32256126 epoch total loss 1.30086911\n",
      "Trained batch 127 batch loss 1.34140408 epoch total loss 1.30118823\n",
      "Trained batch 128 batch loss 1.2355938 epoch total loss 1.30067575\n",
      "Trained batch 129 batch loss 1.33438444 epoch total loss 1.30093706\n",
      "Trained batch 130 batch loss 1.25895345 epoch total loss 1.30061412\n",
      "Trained batch 131 batch loss 1.28737426 epoch total loss 1.30051303\n",
      "Trained batch 132 batch loss 1.21742153 epoch total loss 1.29988348\n",
      "Trained batch 133 batch loss 1.32407284 epoch total loss 1.3000654\n",
      "Trained batch 134 batch loss 1.32921362 epoch total loss 1.30028284\n",
      "Trained batch 135 batch loss 1.37079692 epoch total loss 1.30080521\n",
      "Trained batch 136 batch loss 1.31481862 epoch total loss 1.30090821\n",
      "Trained batch 137 batch loss 1.23772621 epoch total loss 1.30044711\n",
      "Trained batch 138 batch loss 1.30910325 epoch total loss 1.30050981\n",
      "Trained batch 139 batch loss 1.28748238 epoch total loss 1.30041599\n",
      "Trained batch 140 batch loss 1.23414207 epoch total loss 1.29994261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 141 batch loss 1.24430084 epoch total loss 1.29954803\n",
      "Trained batch 142 batch loss 1.15024638 epoch total loss 1.2984966\n",
      "Trained batch 143 batch loss 1.26942742 epoch total loss 1.29829335\n",
      "Trained batch 144 batch loss 1.33885038 epoch total loss 1.29857492\n",
      "Trained batch 145 batch loss 1.28184831 epoch total loss 1.29845965\n",
      "Trained batch 146 batch loss 1.35541177 epoch total loss 1.2988497\n",
      "Trained batch 147 batch loss 1.25859761 epoch total loss 1.29857576\n",
      "Trained batch 148 batch loss 1.19986653 epoch total loss 1.29790878\n",
      "Trained batch 149 batch loss 1.15386057 epoch total loss 1.296942\n",
      "Trained batch 150 batch loss 1.09030604 epoch total loss 1.29556441\n",
      "Trained batch 151 batch loss 1.22328913 epoch total loss 1.29508567\n",
      "Trained batch 152 batch loss 1.13171303 epoch total loss 1.29401088\n",
      "Trained batch 153 batch loss 1.33465552 epoch total loss 1.29427648\n",
      "Trained batch 154 batch loss 1.32782471 epoch total loss 1.29449439\n",
      "Trained batch 155 batch loss 1.32336092 epoch total loss 1.2946806\n",
      "Trained batch 156 batch loss 1.4778142 epoch total loss 1.29585457\n",
      "Trained batch 157 batch loss 1.64154053 epoch total loss 1.29805636\n",
      "Trained batch 158 batch loss 1.4414165 epoch total loss 1.29896367\n",
      "Trained batch 159 batch loss 1.28142679 epoch total loss 1.29885352\n",
      "Trained batch 160 batch loss 1.33665919 epoch total loss 1.29908967\n",
      "Trained batch 161 batch loss 1.13478374 epoch total loss 1.29806912\n",
      "Trained batch 162 batch loss 1.19274831 epoch total loss 1.29741907\n",
      "Trained batch 163 batch loss 1.22504818 epoch total loss 1.29697502\n",
      "Trained batch 164 batch loss 1.21915245 epoch total loss 1.29650056\n",
      "Trained batch 165 batch loss 0.995722294 epoch total loss 1.29467762\n",
      "Trained batch 166 batch loss 1.07260334 epoch total loss 1.29333985\n",
      "Trained batch 167 batch loss 1.0994432 epoch total loss 1.29217875\n",
      "Trained batch 168 batch loss 1.19124579 epoch total loss 1.29157794\n",
      "Trained batch 169 batch loss 1.24849975 epoch total loss 1.29132307\n",
      "Trained batch 170 batch loss 1.39847052 epoch total loss 1.29195333\n",
      "Trained batch 171 batch loss 1.33265615 epoch total loss 1.29219139\n",
      "Trained batch 172 batch loss 1.33482528 epoch total loss 1.29243922\n",
      "Trained batch 173 batch loss 1.38695657 epoch total loss 1.29298556\n",
      "Trained batch 174 batch loss 1.34047008 epoch total loss 1.29325855\n",
      "Trained batch 175 batch loss 1.29577017 epoch total loss 1.29327285\n",
      "Trained batch 176 batch loss 1.26591 epoch total loss 1.2931174\n",
      "Trained batch 177 batch loss 1.40605283 epoch total loss 1.29375553\n",
      "Trained batch 178 batch loss 1.42827344 epoch total loss 1.2945112\n",
      "Trained batch 179 batch loss 1.37285411 epoch total loss 1.29494882\n",
      "Trained batch 180 batch loss 1.43134165 epoch total loss 1.29570651\n",
      "Trained batch 181 batch loss 1.28829932 epoch total loss 1.29566562\n",
      "Trained batch 182 batch loss 1.28727567 epoch total loss 1.29561949\n",
      "Trained batch 183 batch loss 1.26323819 epoch total loss 1.29544258\n",
      "Trained batch 184 batch loss 1.34911489 epoch total loss 1.29573429\n",
      "Trained batch 185 batch loss 1.37559533 epoch total loss 1.29616594\n",
      "Trained batch 186 batch loss 1.39498746 epoch total loss 1.29669726\n",
      "Trained batch 187 batch loss 1.64308476 epoch total loss 1.29854965\n",
      "Trained batch 188 batch loss 1.46048403 epoch total loss 1.29941094\n",
      "Trained batch 189 batch loss 1.34434545 epoch total loss 1.29964876\n",
      "Trained batch 190 batch loss 1.39604568 epoch total loss 1.300156\n",
      "Trained batch 191 batch loss 1.36712253 epoch total loss 1.30050671\n",
      "Trained batch 192 batch loss 1.37180221 epoch total loss 1.30087793\n",
      "Trained batch 193 batch loss 1.42066789 epoch total loss 1.30149865\n",
      "Trained batch 194 batch loss 1.31788087 epoch total loss 1.30158317\n",
      "Trained batch 195 batch loss 1.1867013 epoch total loss 1.30099404\n",
      "Trained batch 196 batch loss 1.12937832 epoch total loss 1.30011845\n",
      "Trained batch 197 batch loss 1.28590238 epoch total loss 1.30004621\n",
      "Trained batch 198 batch loss 1.2221036 epoch total loss 1.29965258\n",
      "Trained batch 199 batch loss 1.38324118 epoch total loss 1.30007255\n",
      "Trained batch 200 batch loss 1.31925035 epoch total loss 1.30016851\n",
      "Trained batch 201 batch loss 1.479509 epoch total loss 1.3010608\n",
      "Trained batch 202 batch loss 1.2916646 epoch total loss 1.30101418\n",
      "Trained batch 203 batch loss 1.26642728 epoch total loss 1.30084383\n",
      "Trained batch 204 batch loss 1.24399948 epoch total loss 1.30056512\n",
      "Trained batch 205 batch loss 1.29897 epoch total loss 1.30055737\n",
      "Trained batch 206 batch loss 1.35143852 epoch total loss 1.30080438\n",
      "Trained batch 207 batch loss 1.3203001 epoch total loss 1.30089855\n",
      "Trained batch 208 batch loss 1.36878717 epoch total loss 1.30122495\n",
      "Trained batch 209 batch loss 1.4259088 epoch total loss 1.30182147\n",
      "Trained batch 210 batch loss 1.39100444 epoch total loss 1.30224609\n",
      "Trained batch 211 batch loss 1.41795576 epoch total loss 1.30279458\n",
      "Trained batch 212 batch loss 1.37891 epoch total loss 1.30315351\n",
      "Trained batch 213 batch loss 1.49546313 epoch total loss 1.30405641\n",
      "Trained batch 214 batch loss 1.3606894 epoch total loss 1.30432105\n",
      "Trained batch 215 batch loss 1.47168887 epoch total loss 1.30509937\n",
      "Trained batch 216 batch loss 1.35026181 epoch total loss 1.30530846\n",
      "Trained batch 217 batch loss 1.38946807 epoch total loss 1.30569625\n",
      "Trained batch 218 batch loss 1.3378576 epoch total loss 1.30584383\n",
      "Trained batch 219 batch loss 1.1869458 epoch total loss 1.30530095\n",
      "Trained batch 220 batch loss 1.30456519 epoch total loss 1.30529761\n",
      "Trained batch 221 batch loss 1.28495181 epoch total loss 1.30520546\n",
      "Trained batch 222 batch loss 1.20880115 epoch total loss 1.30477118\n",
      "Trained batch 223 batch loss 1.26814437 epoch total loss 1.30460703\n",
      "Trained batch 224 batch loss 1.18869805 epoch total loss 1.30408955\n",
      "Trained batch 225 batch loss 1.29003286 epoch total loss 1.30402708\n",
      "Trained batch 226 batch loss 1.30309057 epoch total loss 1.30402303\n",
      "Trained batch 227 batch loss 1.32499361 epoch total loss 1.3041153\n",
      "Trained batch 228 batch loss 1.2245754 epoch total loss 1.30376649\n",
      "Trained batch 229 batch loss 1.18948686 epoch total loss 1.30326748\n",
      "Trained batch 230 batch loss 1.21903884 epoch total loss 1.30290115\n",
      "Trained batch 231 batch loss 1.33341146 epoch total loss 1.30303323\n",
      "Trained batch 232 batch loss 1.30975056 epoch total loss 1.3030622\n",
      "Trained batch 233 batch loss 1.3152355 epoch total loss 1.30311441\n",
      "Trained batch 234 batch loss 1.29451776 epoch total loss 1.30307782\n",
      "Trained batch 235 batch loss 1.26559675 epoch total loss 1.30291831\n",
      "Trained batch 236 batch loss 1.16364324 epoch total loss 1.30232811\n",
      "Trained batch 237 batch loss 1.30047154 epoch total loss 1.30232024\n",
      "Trained batch 238 batch loss 1.1344496 epoch total loss 1.301615\n",
      "Trained batch 239 batch loss 1.27935803 epoch total loss 1.3015219\n",
      "Trained batch 240 batch loss 1.37025154 epoch total loss 1.30180812\n",
      "Trained batch 241 batch loss 1.21340394 epoch total loss 1.30144131\n",
      "Trained batch 242 batch loss 1.12493396 epoch total loss 1.30071199\n",
      "Trained batch 243 batch loss 1.13070381 epoch total loss 1.30001235\n",
      "Trained batch 244 batch loss 1.19937611 epoch total loss 1.29959989\n",
      "Trained batch 245 batch loss 1.32598472 epoch total loss 1.29970765\n",
      "Trained batch 246 batch loss 1.38017976 epoch total loss 1.30003476\n",
      "Trained batch 247 batch loss 1.5142138 epoch total loss 1.30090201\n",
      "Trained batch 248 batch loss 1.32421041 epoch total loss 1.30099595\n",
      "Trained batch 249 batch loss 1.37903023 epoch total loss 1.30130935\n",
      "Trained batch 250 batch loss 1.1596874 epoch total loss 1.30074286\n",
      "Trained batch 251 batch loss 1.46161675 epoch total loss 1.30138385\n",
      "Trained batch 252 batch loss 1.33573627 epoch total loss 1.30152011\n",
      "Trained batch 253 batch loss 1.36524487 epoch total loss 1.30177188\n",
      "Trained batch 254 batch loss 1.41408467 epoch total loss 1.30221415\n",
      "Trained batch 255 batch loss 1.50342441 epoch total loss 1.30300319\n",
      "Trained batch 256 batch loss 1.3929944 epoch total loss 1.30335474\n",
      "Trained batch 257 batch loss 1.37475944 epoch total loss 1.30363262\n",
      "Trained batch 258 batch loss 1.31502616 epoch total loss 1.30367672\n",
      "Trained batch 259 batch loss 1.3011682 epoch total loss 1.30366707\n",
      "Trained batch 260 batch loss 1.19069266 epoch total loss 1.30323267\n",
      "Trained batch 261 batch loss 1.29297435 epoch total loss 1.30319333\n",
      "Trained batch 262 batch loss 1.20953298 epoch total loss 1.30283582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 263 batch loss 1.19131339 epoch total loss 1.30241179\n",
      "Trained batch 264 batch loss 1.28108025 epoch total loss 1.30233097\n",
      "Trained batch 265 batch loss 1.24599266 epoch total loss 1.30211842\n",
      "Trained batch 266 batch loss 1.28159571 epoch total loss 1.30204117\n",
      "Trained batch 267 batch loss 1.29941928 epoch total loss 1.30203128\n",
      "Trained batch 268 batch loss 1.14912415 epoch total loss 1.30146086\n",
      "Trained batch 269 batch loss 1.18232977 epoch total loss 1.301018\n",
      "Trained batch 270 batch loss 1.02555037 epoch total loss 1.29999769\n",
      "Trained batch 271 batch loss 1.07812619 epoch total loss 1.29917908\n",
      "Trained batch 272 batch loss 1.37474656 epoch total loss 1.29945683\n",
      "Trained batch 273 batch loss 1.36953199 epoch total loss 1.29971361\n",
      "Trained batch 274 batch loss 1.32065642 epoch total loss 1.29979\n",
      "Trained batch 275 batch loss 1.30643201 epoch total loss 1.29981411\n",
      "Trained batch 276 batch loss 1.29033113 epoch total loss 1.29977977\n",
      "Trained batch 277 batch loss 1.1387651 epoch total loss 1.29919851\n",
      "Trained batch 278 batch loss 1.21176636 epoch total loss 1.29888403\n",
      "Trained batch 279 batch loss 1.19506931 epoch total loss 1.29851186\n",
      "Trained batch 280 batch loss 1.33168459 epoch total loss 1.29863036\n",
      "Trained batch 281 batch loss 1.24606681 epoch total loss 1.29844332\n",
      "Trained batch 282 batch loss 1.28537774 epoch total loss 1.29839694\n",
      "Trained batch 283 batch loss 1.30743468 epoch total loss 1.29842889\n",
      "Trained batch 284 batch loss 1.27697778 epoch total loss 1.29835343\n",
      "Trained batch 285 batch loss 1.27236128 epoch total loss 1.29826224\n",
      "Trained batch 286 batch loss 1.29162419 epoch total loss 1.29823899\n",
      "Trained batch 287 batch loss 1.416587 epoch total loss 1.29865146\n",
      "Trained batch 288 batch loss 1.40145254 epoch total loss 1.29900837\n",
      "Trained batch 289 batch loss 1.5876739 epoch total loss 1.30000722\n",
      "Trained batch 290 batch loss 1.37347651 epoch total loss 1.30026054\n",
      "Trained batch 291 batch loss 1.39020777 epoch total loss 1.30056965\n",
      "Trained batch 292 batch loss 1.49621022 epoch total loss 1.30123961\n",
      "Trained batch 293 batch loss 1.36213279 epoch total loss 1.30144739\n",
      "Trained batch 294 batch loss 1.41500294 epoch total loss 1.30183375\n",
      "Trained batch 295 batch loss 1.33989906 epoch total loss 1.30196273\n",
      "Trained batch 296 batch loss 1.38004661 epoch total loss 1.30222654\n",
      "Trained batch 297 batch loss 1.23163176 epoch total loss 1.30198884\n",
      "Trained batch 298 batch loss 1.31048882 epoch total loss 1.30201733\n",
      "Trained batch 299 batch loss 1.32399714 epoch total loss 1.30209088\n",
      "Trained batch 300 batch loss 1.35019147 epoch total loss 1.30225122\n",
      "Trained batch 301 batch loss 1.28154063 epoch total loss 1.30218244\n",
      "Trained batch 302 batch loss 1.29493868 epoch total loss 1.30215847\n",
      "Trained batch 303 batch loss 1.22881901 epoch total loss 1.30191648\n",
      "Trained batch 304 batch loss 1.45667386 epoch total loss 1.3024255\n",
      "Trained batch 305 batch loss 1.41652417 epoch total loss 1.30279958\n",
      "Trained batch 306 batch loss 1.3856076 epoch total loss 1.30307031\n",
      "Trained batch 307 batch loss 1.42147708 epoch total loss 1.30345595\n",
      "Trained batch 308 batch loss 1.43670702 epoch total loss 1.30388856\n",
      "Trained batch 309 batch loss 1.34914851 epoch total loss 1.30403507\n",
      "Trained batch 310 batch loss 1.31734383 epoch total loss 1.30407798\n",
      "Trained batch 311 batch loss 1.32357359 epoch total loss 1.30414069\n",
      "Trained batch 312 batch loss 1.30084467 epoch total loss 1.3041302\n",
      "Trained batch 313 batch loss 1.28880346 epoch total loss 1.3040812\n",
      "Trained batch 314 batch loss 1.36291766 epoch total loss 1.3042686\n",
      "Trained batch 315 batch loss 1.36278582 epoch total loss 1.30445445\n",
      "Trained batch 316 batch loss 1.36836219 epoch total loss 1.30465662\n",
      "Trained batch 317 batch loss 1.40053105 epoch total loss 1.30495906\n",
      "Trained batch 318 batch loss 1.33212256 epoch total loss 1.30504453\n",
      "Trained batch 319 batch loss 1.40005302 epoch total loss 1.30534232\n",
      "Trained batch 320 batch loss 1.26452136 epoch total loss 1.30521476\n",
      "Trained batch 321 batch loss 1.24823296 epoch total loss 1.30503726\n",
      "Trained batch 322 batch loss 1.20473111 epoch total loss 1.30472577\n",
      "Trained batch 323 batch loss 1.40546203 epoch total loss 1.30503762\n",
      "Trained batch 324 batch loss 1.28675067 epoch total loss 1.30498123\n",
      "Trained batch 325 batch loss 1.25786078 epoch total loss 1.30483627\n",
      "Trained batch 326 batch loss 1.2591778 epoch total loss 1.3046962\n",
      "Trained batch 327 batch loss 1.25519395 epoch total loss 1.30454481\n",
      "Trained batch 328 batch loss 1.28833771 epoch total loss 1.30449533\n",
      "Trained batch 329 batch loss 1.14175 epoch total loss 1.30400074\n",
      "Trained batch 330 batch loss 1.22002614 epoch total loss 1.30374622\n",
      "Trained batch 331 batch loss 1.22947121 epoch total loss 1.30352187\n",
      "Trained batch 332 batch loss 1.24461508 epoch total loss 1.30334449\n",
      "Trained batch 333 batch loss 1.22628129 epoch total loss 1.3031131\n",
      "Trained batch 334 batch loss 1.2443732 epoch total loss 1.30293727\n",
      "Trained batch 335 batch loss 1.30205154 epoch total loss 1.30293465\n",
      "Trained batch 336 batch loss 1.30205703 epoch total loss 1.30293202\n",
      "Trained batch 337 batch loss 1.35756207 epoch total loss 1.30309415\n",
      "Trained batch 338 batch loss 1.40120173 epoch total loss 1.30338442\n",
      "Trained batch 339 batch loss 1.28340375 epoch total loss 1.30332553\n",
      "Trained batch 340 batch loss 1.2175746 epoch total loss 1.30307329\n",
      "Trained batch 341 batch loss 1.2609266 epoch total loss 1.30294967\n",
      "Trained batch 342 batch loss 1.34522855 epoch total loss 1.30307329\n",
      "Trained batch 343 batch loss 1.35744441 epoch total loss 1.30323184\n",
      "Trained batch 344 batch loss 1.41080201 epoch total loss 1.30354452\n",
      "Trained batch 345 batch loss 1.29974365 epoch total loss 1.30353343\n",
      "Trained batch 346 batch loss 1.26210129 epoch total loss 1.30341375\n",
      "Trained batch 347 batch loss 1.24455166 epoch total loss 1.30324411\n",
      "Trained batch 348 batch loss 1.12117362 epoch total loss 1.3027209\n",
      "Trained batch 349 batch loss 1.20446312 epoch total loss 1.30243945\n",
      "Trained batch 350 batch loss 1.22754025 epoch total loss 1.30222547\n",
      "Trained batch 351 batch loss 1.28001237 epoch total loss 1.30216205\n",
      "Trained batch 352 batch loss 1.35058296 epoch total loss 1.30229962\n",
      "Trained batch 353 batch loss 1.38223433 epoch total loss 1.30252612\n",
      "Trained batch 354 batch loss 1.32099783 epoch total loss 1.30257821\n",
      "Trained batch 355 batch loss 1.41806769 epoch total loss 1.30290353\n",
      "Trained batch 356 batch loss 1.26921928 epoch total loss 1.302809\n",
      "Trained batch 357 batch loss 1.35863519 epoch total loss 1.3029654\n",
      "Trained batch 358 batch loss 1.41521645 epoch total loss 1.30327892\n",
      "Trained batch 359 batch loss 1.28444684 epoch total loss 1.30322647\n",
      "Trained batch 360 batch loss 1.28032851 epoch total loss 1.30316293\n",
      "Trained batch 361 batch loss 1.39571941 epoch total loss 1.30341923\n",
      "Trained batch 362 batch loss 1.3806622 epoch total loss 1.30363274\n",
      "Trained batch 363 batch loss 1.36317897 epoch total loss 1.30379677\n",
      "Trained batch 364 batch loss 1.34506047 epoch total loss 1.30391014\n",
      "Trained batch 365 batch loss 1.42135096 epoch total loss 1.30423188\n",
      "Trained batch 366 batch loss 1.4926827 epoch total loss 1.30474675\n",
      "Trained batch 367 batch loss 1.37412393 epoch total loss 1.30493581\n",
      "Trained batch 368 batch loss 1.29477882 epoch total loss 1.30490816\n",
      "Trained batch 369 batch loss 1.29476 epoch total loss 1.30488074\n",
      "Trained batch 370 batch loss 1.24196923 epoch total loss 1.30471063\n",
      "Trained batch 371 batch loss 1.33171082 epoch total loss 1.30478346\n",
      "Trained batch 372 batch loss 1.24913025 epoch total loss 1.30463386\n",
      "Trained batch 373 batch loss 1.24030399 epoch total loss 1.30446136\n",
      "Trained batch 374 batch loss 1.20148861 epoch total loss 1.30418599\n",
      "Trained batch 375 batch loss 1.15251923 epoch total loss 1.30378163\n",
      "Trained batch 376 batch loss 1.20689929 epoch total loss 1.3035239\n",
      "Trained batch 377 batch loss 1.17894351 epoch total loss 1.30319357\n",
      "Trained batch 378 batch loss 1.23148608 epoch total loss 1.30300379\n",
      "Trained batch 379 batch loss 1.19732976 epoch total loss 1.30272496\n",
      "Trained batch 380 batch loss 1.14812911 epoch total loss 1.3023181\n",
      "Trained batch 381 batch loss 1.25869679 epoch total loss 1.30220366\n",
      "Trained batch 382 batch loss 1.2199645 epoch total loss 1.30198836\n",
      "Trained batch 383 batch loss 1.37417352 epoch total loss 1.30217683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 384 batch loss 1.31721342 epoch total loss 1.30221593\n",
      "Trained batch 385 batch loss 1.40134072 epoch total loss 1.30247343\n",
      "Trained batch 386 batch loss 1.2818085 epoch total loss 1.3024199\n",
      "Trained batch 387 batch loss 1.38391304 epoch total loss 1.30263042\n",
      "Trained batch 388 batch loss 1.31736183 epoch total loss 1.30266833\n",
      "Trained batch 389 batch loss 1.23571169 epoch total loss 1.30249631\n",
      "Trained batch 390 batch loss 1.23926282 epoch total loss 1.30233407\n",
      "Trained batch 391 batch loss 1.27878153 epoch total loss 1.30227387\n",
      "Trained batch 392 batch loss 1.18965816 epoch total loss 1.30198658\n",
      "Trained batch 393 batch loss 1.14689517 epoch total loss 1.30159199\n",
      "Trained batch 394 batch loss 1.13568389 epoch total loss 1.30117083\n",
      "Trained batch 395 batch loss 1.20574939 epoch total loss 1.30092931\n",
      "Trained batch 396 batch loss 1.24861765 epoch total loss 1.3007971\n",
      "Trained batch 397 batch loss 1.25230908 epoch total loss 1.30067503\n",
      "Trained batch 398 batch loss 1.35877824 epoch total loss 1.30082095\n",
      "Trained batch 399 batch loss 1.36808 epoch total loss 1.30098963\n",
      "Trained batch 400 batch loss 1.2481699 epoch total loss 1.30085754\n",
      "Trained batch 401 batch loss 1.27838731 epoch total loss 1.30080152\n",
      "Trained batch 402 batch loss 1.2490741 epoch total loss 1.30067289\n",
      "Trained batch 403 batch loss 1.26133966 epoch total loss 1.30057526\n",
      "Trained batch 404 batch loss 1.30182517 epoch total loss 1.30057836\n",
      "Trained batch 405 batch loss 1.21011186 epoch total loss 1.30035496\n",
      "Trained batch 406 batch loss 0.965181 epoch total loss 1.29952943\n",
      "Trained batch 407 batch loss 1.00474739 epoch total loss 1.29880524\n",
      "Trained batch 408 batch loss 1.24668157 epoch total loss 1.29867744\n",
      "Trained batch 409 batch loss 1.38696432 epoch total loss 1.29889333\n",
      "Trained batch 410 batch loss 1.4319768 epoch total loss 1.29921806\n",
      "Trained batch 411 batch loss 1.47768855 epoch total loss 1.29965222\n",
      "Trained batch 412 batch loss 1.37105453 epoch total loss 1.29982543\n",
      "Trained batch 413 batch loss 1.31874228 epoch total loss 1.29987121\n",
      "Trained batch 414 batch loss 1.32943356 epoch total loss 1.29994249\n",
      "Trained batch 415 batch loss 1.36910892 epoch total loss 1.30010915\n",
      "Trained batch 416 batch loss 1.27142811 epoch total loss 1.30004013\n",
      "Trained batch 417 batch loss 1.26263952 epoch total loss 1.29995048\n",
      "Trained batch 418 batch loss 1.35355544 epoch total loss 1.30007875\n",
      "Trained batch 419 batch loss 1.19269705 epoch total loss 1.29982245\n",
      "Trained batch 420 batch loss 1.26004469 epoch total loss 1.2997278\n",
      "Trained batch 421 batch loss 1.34760213 epoch total loss 1.29984152\n",
      "Trained batch 422 batch loss 1.28792524 epoch total loss 1.29981327\n",
      "Trained batch 423 batch loss 1.25794911 epoch total loss 1.29971421\n",
      "Trained batch 424 batch loss 1.25655305 epoch total loss 1.2996124\n",
      "Trained batch 425 batch loss 1.19985282 epoch total loss 1.29937756\n",
      "Trained batch 426 batch loss 1.28595841 epoch total loss 1.29934609\n",
      "Trained batch 427 batch loss 1.28204715 epoch total loss 1.29930556\n",
      "Trained batch 428 batch loss 1.25616896 epoch total loss 1.29920471\n",
      "Trained batch 429 batch loss 1.22761118 epoch total loss 1.29903781\n",
      "Trained batch 430 batch loss 1.24590015 epoch total loss 1.29891431\n",
      "Trained batch 431 batch loss 1.17396927 epoch total loss 1.2986244\n",
      "Trained batch 432 batch loss 1.38191187 epoch total loss 1.29881716\n",
      "Trained batch 433 batch loss 1.27688789 epoch total loss 1.29876649\n",
      "Trained batch 434 batch loss 1.33687878 epoch total loss 1.29885423\n",
      "Trained batch 435 batch loss 1.36904991 epoch total loss 1.29901576\n",
      "Trained batch 436 batch loss 1.5020752 epoch total loss 1.29948151\n",
      "Trained batch 437 batch loss 1.50298 epoch total loss 1.29994714\n",
      "Trained batch 438 batch loss 1.58821523 epoch total loss 1.3006053\n",
      "Trained batch 439 batch loss 1.3598628 epoch total loss 1.30074024\n",
      "Trained batch 440 batch loss 1.2378602 epoch total loss 1.30059731\n",
      "Trained batch 441 batch loss 1.28952217 epoch total loss 1.30057228\n",
      "Trained batch 442 batch loss 1.25955701 epoch total loss 1.30047953\n",
      "Trained batch 443 batch loss 1.32286811 epoch total loss 1.30053008\n",
      "Trained batch 444 batch loss 1.33266211 epoch total loss 1.30060244\n",
      "Trained batch 445 batch loss 1.28677726 epoch total loss 1.30057144\n",
      "Trained batch 446 batch loss 1.23629689 epoch total loss 1.3004272\n",
      "Trained batch 447 batch loss 1.13334417 epoch total loss 1.30005348\n",
      "Trained batch 448 batch loss 1.13932967 epoch total loss 1.29969478\n",
      "Trained batch 449 batch loss 1.29368114 epoch total loss 1.29968143\n",
      "Trained batch 450 batch loss 1.50090647 epoch total loss 1.30012858\n",
      "Trained batch 451 batch loss 1.55181718 epoch total loss 1.3006866\n",
      "Trained batch 452 batch loss 1.49602723 epoch total loss 1.30111885\n",
      "Trained batch 453 batch loss 1.44234073 epoch total loss 1.30143058\n",
      "Trained batch 454 batch loss 1.45583689 epoch total loss 1.30177057\n",
      "Trained batch 455 batch loss 1.41398847 epoch total loss 1.30201721\n",
      "Trained batch 456 batch loss 1.42782438 epoch total loss 1.30229306\n",
      "Trained batch 457 batch loss 1.30878592 epoch total loss 1.30230725\n",
      "Trained batch 458 batch loss 1.28055334 epoch total loss 1.3022598\n",
      "Trained batch 459 batch loss 1.19221926 epoch total loss 1.30202007\n",
      "Trained batch 460 batch loss 1.09642124 epoch total loss 1.30157316\n",
      "Trained batch 461 batch loss 1.16623 epoch total loss 1.30127954\n",
      "Trained batch 462 batch loss 1.4827075 epoch total loss 1.30167234\n",
      "Trained batch 463 batch loss 1.36580014 epoch total loss 1.30181086\n",
      "Trained batch 464 batch loss 1.35059595 epoch total loss 1.30191588\n",
      "Trained batch 465 batch loss 1.26491642 epoch total loss 1.30183625\n",
      "Trained batch 466 batch loss 1.30789137 epoch total loss 1.30184925\n",
      "Trained batch 467 batch loss 1.14925206 epoch total loss 1.30152237\n",
      "Trained batch 468 batch loss 1.23729396 epoch total loss 1.30138516\n",
      "Trained batch 469 batch loss 1.32031655 epoch total loss 1.30142558\n",
      "Trained batch 470 batch loss 1.25625074 epoch total loss 1.30132937\n",
      "Trained batch 471 batch loss 1.25887406 epoch total loss 1.30123925\n",
      "Trained batch 472 batch loss 1.29491949 epoch total loss 1.30122578\n",
      "Trained batch 473 batch loss 1.3108474 epoch total loss 1.30124617\n",
      "Trained batch 474 batch loss 1.26030302 epoch total loss 1.30115986\n",
      "Trained batch 475 batch loss 1.26260352 epoch total loss 1.30107856\n",
      "Trained batch 476 batch loss 1.17829061 epoch total loss 1.30082059\n",
      "Trained batch 477 batch loss 1.23978794 epoch total loss 1.30069268\n",
      "Trained batch 478 batch loss 1.24086571 epoch total loss 1.30056751\n",
      "Trained batch 479 batch loss 1.25953138 epoch total loss 1.3004818\n",
      "Trained batch 480 batch loss 1.206617 epoch total loss 1.30028617\n",
      "Trained batch 481 batch loss 1.20795274 epoch total loss 1.30009425\n",
      "Trained batch 482 batch loss 1.23593807 epoch total loss 1.29996121\n",
      "Trained batch 483 batch loss 1.13004947 epoch total loss 1.29960942\n",
      "Trained batch 484 batch loss 1.16754484 epoch total loss 1.29933655\n",
      "Trained batch 485 batch loss 1.20162201 epoch total loss 1.29913509\n",
      "Trained batch 486 batch loss 1.22265172 epoch total loss 1.29897773\n",
      "Trained batch 487 batch loss 1.18242669 epoch total loss 1.29873836\n",
      "Trained batch 488 batch loss 1.2248174 epoch total loss 1.29858685\n",
      "Trained batch 489 batch loss 1.20071149 epoch total loss 1.29838669\n",
      "Trained batch 490 batch loss 1.27045941 epoch total loss 1.29832959\n",
      "Trained batch 491 batch loss 1.42010617 epoch total loss 1.29857767\n",
      "Trained batch 492 batch loss 1.40389454 epoch total loss 1.29879165\n",
      "Trained batch 493 batch loss 1.34751892 epoch total loss 1.29889047\n",
      "Trained batch 494 batch loss 1.34480429 epoch total loss 1.29898345\n",
      "Trained batch 495 batch loss 1.57701135 epoch total loss 1.29954517\n",
      "Trained batch 496 batch loss 1.39727366 epoch total loss 1.29974222\n",
      "Trained batch 497 batch loss 1.35759354 epoch total loss 1.29985857\n",
      "Trained batch 498 batch loss 1.21384609 epoch total loss 1.29968596\n",
      "Trained batch 499 batch loss 1.26226139 epoch total loss 1.29961097\n",
      "Trained batch 500 batch loss 1.26616812 epoch total loss 1.2995441\n",
      "Trained batch 501 batch loss 1.26716149 epoch total loss 1.29947937\n",
      "Trained batch 502 batch loss 1.36166883 epoch total loss 1.29960334\n",
      "Trained batch 503 batch loss 1.22797322 epoch total loss 1.29946089\n",
      "Trained batch 504 batch loss 1.26439095 epoch total loss 1.29939139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 505 batch loss 1.44091535 epoch total loss 1.29967165\n",
      "Trained batch 506 batch loss 1.31654668 epoch total loss 1.29970491\n",
      "Trained batch 507 batch loss 1.27563095 epoch total loss 1.29965746\n",
      "Trained batch 508 batch loss 1.29330468 epoch total loss 1.29964507\n",
      "Trained batch 509 batch loss 1.37132251 epoch total loss 1.29978585\n",
      "Trained batch 510 batch loss 1.29179049 epoch total loss 1.29977024\n",
      "Trained batch 511 batch loss 1.29530895 epoch total loss 1.29976141\n",
      "Trained batch 512 batch loss 1.31688142 epoch total loss 1.29979491\n",
      "Trained batch 513 batch loss 1.25610089 epoch total loss 1.2997098\n",
      "Trained batch 514 batch loss 1.33202064 epoch total loss 1.29977262\n",
      "Trained batch 515 batch loss 1.28457665 epoch total loss 1.29974318\n",
      "Trained batch 516 batch loss 1.38601065 epoch total loss 1.29991031\n",
      "Trained batch 517 batch loss 1.3694458 epoch total loss 1.30004478\n",
      "Trained batch 518 batch loss 1.42709184 epoch total loss 1.30029\n",
      "Trained batch 519 batch loss 1.27228379 epoch total loss 1.30023611\n",
      "Trained batch 520 batch loss 1.28046834 epoch total loss 1.30019796\n",
      "Trained batch 521 batch loss 1.30458474 epoch total loss 1.30020642\n",
      "Trained batch 522 batch loss 1.30864453 epoch total loss 1.30022264\n",
      "Trained batch 523 batch loss 1.38947618 epoch total loss 1.30039322\n",
      "Trained batch 524 batch loss 1.37311018 epoch total loss 1.30053198\n",
      "Trained batch 525 batch loss 1.26954103 epoch total loss 1.30047297\n",
      "Trained batch 526 batch loss 1.42031765 epoch total loss 1.30070066\n",
      "Trained batch 527 batch loss 1.32199299 epoch total loss 1.3007412\n",
      "Trained batch 528 batch loss 1.24866927 epoch total loss 1.30064249\n",
      "Trained batch 529 batch loss 1.28004265 epoch total loss 1.30060351\n",
      "Trained batch 530 batch loss 1.41077757 epoch total loss 1.30081141\n",
      "Trained batch 531 batch loss 1.48132205 epoch total loss 1.30115139\n",
      "Trained batch 532 batch loss 1.32748723 epoch total loss 1.30120087\n",
      "Trained batch 533 batch loss 1.26511204 epoch total loss 1.30113328\n",
      "Trained batch 534 batch loss 1.30807459 epoch total loss 1.30114615\n",
      "Trained batch 535 batch loss 1.40295291 epoch total loss 1.30133653\n",
      "Trained batch 536 batch loss 1.28607154 epoch total loss 1.30130804\n",
      "Trained batch 537 batch loss 1.344432 epoch total loss 1.30138826\n",
      "Trained batch 538 batch loss 1.46011674 epoch total loss 1.30168343\n",
      "Trained batch 539 batch loss 1.43159306 epoch total loss 1.30192435\n",
      "Trained batch 540 batch loss 1.28440273 epoch total loss 1.30189192\n",
      "Trained batch 541 batch loss 1.17139399 epoch total loss 1.30165076\n",
      "Trained batch 542 batch loss 1.26985168 epoch total loss 1.30159199\n",
      "Trained batch 543 batch loss 1.2454946 epoch total loss 1.30148876\n",
      "Trained batch 544 batch loss 1.32918334 epoch total loss 1.30153954\n",
      "Trained batch 545 batch loss 1.29949856 epoch total loss 1.30153584\n",
      "Trained batch 546 batch loss 1.31274486 epoch total loss 1.30155635\n",
      "Trained batch 547 batch loss 1.4030745 epoch total loss 1.30174196\n",
      "Trained batch 548 batch loss 1.47383451 epoch total loss 1.30205595\n",
      "Trained batch 549 batch loss 1.34467566 epoch total loss 1.30213356\n",
      "Trained batch 550 batch loss 1.32839549 epoch total loss 1.30218124\n",
      "Trained batch 551 batch loss 1.30436563 epoch total loss 1.3021853\n",
      "Trained batch 552 batch loss 1.28130841 epoch total loss 1.30214751\n",
      "Trained batch 553 batch loss 1.2867682 epoch total loss 1.30211961\n",
      "Trained batch 554 batch loss 1.38660812 epoch total loss 1.30227208\n",
      "Trained batch 555 batch loss 1.42120409 epoch total loss 1.30248642\n",
      "Trained batch 556 batch loss 1.36240602 epoch total loss 1.30259418\n",
      "Trained batch 557 batch loss 1.45764112 epoch total loss 1.30287254\n",
      "Trained batch 558 batch loss 1.49133992 epoch total loss 1.30321026\n",
      "Trained batch 559 batch loss 1.40686369 epoch total loss 1.30339575\n",
      "Trained batch 560 batch loss 1.41682017 epoch total loss 1.30359828\n",
      "Trained batch 561 batch loss 1.40962899 epoch total loss 1.30378723\n",
      "Trained batch 562 batch loss 1.42738211 epoch total loss 1.30400705\n",
      "Trained batch 563 batch loss 1.34675956 epoch total loss 1.30408299\n",
      "Trained batch 564 batch loss 1.54586732 epoch total loss 1.30451167\n",
      "Trained batch 565 batch loss 1.52707791 epoch total loss 1.30490565\n",
      "Trained batch 566 batch loss 1.5038569 epoch total loss 1.30525708\n",
      "Trained batch 567 batch loss 1.27584171 epoch total loss 1.30520523\n",
      "Trained batch 568 batch loss 1.21744418 epoch total loss 1.30505073\n",
      "Trained batch 569 batch loss 1.24392295 epoch total loss 1.3049432\n",
      "Trained batch 570 batch loss 1.30010235 epoch total loss 1.30493474\n",
      "Trained batch 571 batch loss 1.33832884 epoch total loss 1.30499315\n",
      "Trained batch 572 batch loss 1.35709226 epoch total loss 1.30508435\n",
      "Trained batch 573 batch loss 1.37850761 epoch total loss 1.30521238\n",
      "Trained batch 574 batch loss 1.34304357 epoch total loss 1.3052783\n",
      "Trained batch 575 batch loss 1.4222188 epoch total loss 1.30548167\n",
      "Trained batch 576 batch loss 1.43911743 epoch total loss 1.30571365\n",
      "Trained batch 577 batch loss 1.35276735 epoch total loss 1.30579519\n",
      "Trained batch 578 batch loss 1.26435661 epoch total loss 1.30572355\n",
      "Trained batch 579 batch loss 1.24830747 epoch total loss 1.30562437\n",
      "Trained batch 580 batch loss 1.27868795 epoch total loss 1.30557787\n",
      "Trained batch 581 batch loss 1.33318734 epoch total loss 1.30562544\n",
      "Trained batch 582 batch loss 1.248281 epoch total loss 1.30552685\n",
      "Trained batch 583 batch loss 1.21464396 epoch total loss 1.30537105\n",
      "Trained batch 584 batch loss 1.20390713 epoch total loss 1.30519736\n",
      "Trained batch 585 batch loss 1.30105615 epoch total loss 1.30519032\n",
      "Trained batch 586 batch loss 1.33389568 epoch total loss 1.30523932\n",
      "Trained batch 587 batch loss 1.17302656 epoch total loss 1.30501413\n",
      "Trained batch 588 batch loss 1.27546573 epoch total loss 1.30496383\n",
      "Trained batch 589 batch loss 1.34415913 epoch total loss 1.30503035\n",
      "Trained batch 590 batch loss 1.35787988 epoch total loss 1.30512\n",
      "Trained batch 591 batch loss 1.28608489 epoch total loss 1.3050878\n",
      "Trained batch 592 batch loss 1.28080547 epoch total loss 1.3050468\n",
      "Trained batch 593 batch loss 1.28754985 epoch total loss 1.30501723\n",
      "Trained batch 594 batch loss 1.19539928 epoch total loss 1.3048327\n",
      "Trained batch 595 batch loss 1.26613545 epoch total loss 1.30476761\n",
      "Trained batch 596 batch loss 1.30003858 epoch total loss 1.30475974\n",
      "Trained batch 597 batch loss 1.33921945 epoch total loss 1.30481744\n",
      "Trained batch 598 batch loss 1.33198023 epoch total loss 1.30486286\n",
      "Trained batch 599 batch loss 1.36207974 epoch total loss 1.30495834\n",
      "Trained batch 600 batch loss 1.3048234 epoch total loss 1.30495811\n",
      "Trained batch 601 batch loss 1.33136797 epoch total loss 1.30500197\n",
      "Trained batch 602 batch loss 1.31500769 epoch total loss 1.30501866\n",
      "Trained batch 603 batch loss 1.23415172 epoch total loss 1.30490112\n",
      "Trained batch 604 batch loss 1.26620317 epoch total loss 1.30483699\n",
      "Trained batch 605 batch loss 1.31427574 epoch total loss 1.3048526\n",
      "Trained batch 606 batch loss 1.1947298 epoch total loss 1.30467081\n",
      "Trained batch 607 batch loss 1.34330511 epoch total loss 1.30473447\n",
      "Trained batch 608 batch loss 1.42984641 epoch total loss 1.30494022\n",
      "Trained batch 609 batch loss 1.44243324 epoch total loss 1.30516601\n",
      "Trained batch 610 batch loss 1.35008359 epoch total loss 1.30523968\n",
      "Trained batch 611 batch loss 1.37539768 epoch total loss 1.3053546\n",
      "Trained batch 612 batch loss 1.35539043 epoch total loss 1.30543637\n",
      "Trained batch 613 batch loss 1.3791225 epoch total loss 1.30555665\n",
      "Trained batch 614 batch loss 1.24296069 epoch total loss 1.30545473\n",
      "Trained batch 615 batch loss 1.26902342 epoch total loss 1.30539548\n",
      "Trained batch 616 batch loss 1.1879468 epoch total loss 1.30520487\n",
      "Trained batch 617 batch loss 1.17488372 epoch total loss 1.30499351\n",
      "Trained batch 618 batch loss 1.06210542 epoch total loss 1.3046006\n",
      "Trained batch 619 batch loss 1.1796 epoch total loss 1.30439866\n",
      "Trained batch 620 batch loss 1.13698661 epoch total loss 1.30412865\n",
      "Trained batch 621 batch loss 1.04947615 epoch total loss 1.30371857\n",
      "Trained batch 622 batch loss 1.04246187 epoch total loss 1.30329859\n",
      "Trained batch 623 batch loss 0.960706472 epoch total loss 1.30274868\n",
      "Trained batch 624 batch loss 1.10516095 epoch total loss 1.30243206\n",
      "Trained batch 625 batch loss 1.33553135 epoch total loss 1.30248499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 626 batch loss 1.32782984 epoch total loss 1.3025254\n",
      "Trained batch 627 batch loss 1.31463909 epoch total loss 1.30254471\n",
      "Trained batch 628 batch loss 1.2143867 epoch total loss 1.3024044\n",
      "Trained batch 629 batch loss 1.49069417 epoch total loss 1.30270386\n",
      "Trained batch 630 batch loss 1.40320039 epoch total loss 1.30286336\n",
      "Trained batch 631 batch loss 1.38638985 epoch total loss 1.30299568\n",
      "Trained batch 632 batch loss 1.27012646 epoch total loss 1.30294371\n",
      "Trained batch 633 batch loss 1.27658296 epoch total loss 1.3029021\n",
      "Trained batch 634 batch loss 1.2082696 epoch total loss 1.30275285\n",
      "Trained batch 635 batch loss 1.36263049 epoch total loss 1.30284715\n",
      "Trained batch 636 batch loss 1.18352556 epoch total loss 1.30265951\n",
      "Trained batch 637 batch loss 1.19106185 epoch total loss 1.30248427\n",
      "Trained batch 638 batch loss 1.16766131 epoch total loss 1.30227304\n",
      "Trained batch 639 batch loss 1.12237608 epoch total loss 1.30199146\n",
      "Trained batch 640 batch loss 1.19252455 epoch total loss 1.3018204\n",
      "Trained batch 641 batch loss 1.08352768 epoch total loss 1.30147982\n",
      "Trained batch 642 batch loss 1.23397207 epoch total loss 1.30137467\n",
      "Trained batch 643 batch loss 1.24409747 epoch total loss 1.30128562\n",
      "Trained batch 644 batch loss 1.34806132 epoch total loss 1.30135822\n",
      "Trained batch 645 batch loss 1.25349021 epoch total loss 1.30128396\n",
      "Trained batch 646 batch loss 1.28479791 epoch total loss 1.30125844\n",
      "Trained batch 647 batch loss 1.2419591 epoch total loss 1.30116677\n",
      "Trained batch 648 batch loss 1.33414865 epoch total loss 1.30121768\n",
      "Trained batch 649 batch loss 1.29916859 epoch total loss 1.30121458\n",
      "Trained batch 650 batch loss 1.26176357 epoch total loss 1.3011539\n",
      "Trained batch 651 batch loss 1.22305989 epoch total loss 1.30103397\n",
      "Trained batch 652 batch loss 1.09874 epoch total loss 1.30072379\n",
      "Trained batch 653 batch loss 1.18412364 epoch total loss 1.30054522\n",
      "Trained batch 654 batch loss 1.37577844 epoch total loss 1.30066025\n",
      "Trained batch 655 batch loss 1.36531103 epoch total loss 1.30075896\n",
      "Trained batch 656 batch loss 1.38303316 epoch total loss 1.30088449\n",
      "Trained batch 657 batch loss 1.39456439 epoch total loss 1.30102706\n",
      "Trained batch 658 batch loss 1.40657163 epoch total loss 1.3011874\n",
      "Trained batch 659 batch loss 1.36572647 epoch total loss 1.30128539\n",
      "Trained batch 660 batch loss 1.31269503 epoch total loss 1.30130267\n",
      "Trained batch 661 batch loss 1.43545485 epoch total loss 1.30150557\n",
      "Trained batch 662 batch loss 1.30333614 epoch total loss 1.30150831\n",
      "Trained batch 663 batch loss 1.31593943 epoch total loss 1.30153\n",
      "Trained batch 664 batch loss 1.17006993 epoch total loss 1.301332\n",
      "Trained batch 665 batch loss 1.11513686 epoch total loss 1.30105197\n",
      "Trained batch 666 batch loss 1.19526899 epoch total loss 1.30089319\n",
      "Trained batch 667 batch loss 1.23869801 epoch total loss 1.3008\n",
      "Trained batch 668 batch loss 1.27180839 epoch total loss 1.30075645\n",
      "Trained batch 669 batch loss 1.29270983 epoch total loss 1.30074453\n",
      "Trained batch 670 batch loss 1.38243055 epoch total loss 1.30086648\n",
      "Trained batch 671 batch loss 1.32494974 epoch total loss 1.30090237\n",
      "Trained batch 672 batch loss 1.40517235 epoch total loss 1.30105746\n",
      "Trained batch 673 batch loss 1.33282566 epoch total loss 1.30110466\n",
      "Trained batch 674 batch loss 1.33792973 epoch total loss 1.30115938\n",
      "Trained batch 675 batch loss 1.26621032 epoch total loss 1.30110753\n",
      "Trained batch 676 batch loss 1.37628555 epoch total loss 1.30121875\n",
      "Trained batch 677 batch loss 1.26784408 epoch total loss 1.30116951\n",
      "Trained batch 678 batch loss 1.34652948 epoch total loss 1.30123639\n",
      "Trained batch 679 batch loss 1.30219722 epoch total loss 1.30123782\n",
      "Trained batch 680 batch loss 1.30472422 epoch total loss 1.30124295\n",
      "Trained batch 681 batch loss 1.189996 epoch total loss 1.30107963\n",
      "Trained batch 682 batch loss 1.22657931 epoch total loss 1.30097032\n",
      "Trained batch 683 batch loss 1.35868919 epoch total loss 1.30105484\n",
      "Trained batch 684 batch loss 1.25106478 epoch total loss 1.30098176\n",
      "Trained batch 685 batch loss 1.41251922 epoch total loss 1.3011446\n",
      "Trained batch 686 batch loss 1.27518201 epoch total loss 1.30110681\n",
      "Trained batch 687 batch loss 1.34744549 epoch total loss 1.30117428\n",
      "Trained batch 688 batch loss 1.22425413 epoch total loss 1.30106246\n",
      "Trained batch 689 batch loss 1.18067825 epoch total loss 1.3008877\n",
      "Trained batch 690 batch loss 1.22525561 epoch total loss 1.30077815\n",
      "Trained batch 691 batch loss 1.2938633 epoch total loss 1.30076814\n",
      "Trained batch 692 batch loss 1.40056694 epoch total loss 1.30091238\n",
      "Trained batch 693 batch loss 1.28473163 epoch total loss 1.30088902\n",
      "Trained batch 694 batch loss 1.36787283 epoch total loss 1.30098557\n",
      "Trained batch 695 batch loss 1.31039 epoch total loss 1.30099905\n",
      "Trained batch 696 batch loss 1.31855536 epoch total loss 1.3010242\n",
      "Trained batch 697 batch loss 1.21908271 epoch total loss 1.30090666\n",
      "Trained batch 698 batch loss 1.27824867 epoch total loss 1.30087423\n",
      "Trained batch 699 batch loss 1.28210902 epoch total loss 1.30084729\n",
      "Trained batch 700 batch loss 1.33650661 epoch total loss 1.30089831\n",
      "Trained batch 701 batch loss 1.25809288 epoch total loss 1.30083728\n",
      "Trained batch 702 batch loss 1.27417839 epoch total loss 1.30079925\n",
      "Trained batch 703 batch loss 1.3824482 epoch total loss 1.30091536\n",
      "Trained batch 704 batch loss 1.42228532 epoch total loss 1.30108786\n",
      "Trained batch 705 batch loss 1.4023385 epoch total loss 1.30123138\n",
      "Trained batch 706 batch loss 1.33108425 epoch total loss 1.3012737\n",
      "Trained batch 707 batch loss 1.27238369 epoch total loss 1.30123281\n",
      "Trained batch 708 batch loss 1.19195235 epoch total loss 1.30107844\n",
      "Trained batch 709 batch loss 1.32521796 epoch total loss 1.30111253\n",
      "Trained batch 710 batch loss 1.25376976 epoch total loss 1.30104589\n",
      "Trained batch 711 batch loss 1.24999213 epoch total loss 1.30097401\n",
      "Trained batch 712 batch loss 1.34806859 epoch total loss 1.30104017\n",
      "Trained batch 713 batch loss 1.33986437 epoch total loss 1.30109465\n",
      "Trained batch 714 batch loss 1.27543688 epoch total loss 1.30105877\n",
      "Trained batch 715 batch loss 1.34375525 epoch total loss 1.30111849\n",
      "Trained batch 716 batch loss 1.23274696 epoch total loss 1.30102289\n",
      "Trained batch 717 batch loss 1.22652149 epoch total loss 1.30091894\n",
      "Trained batch 718 batch loss 1.24109888 epoch total loss 1.30083561\n",
      "Trained batch 719 batch loss 1.18227816 epoch total loss 1.30067074\n",
      "Trained batch 720 batch loss 1.29770207 epoch total loss 1.30066669\n",
      "Trained batch 721 batch loss 1.33257866 epoch total loss 1.30071092\n",
      "Trained batch 722 batch loss 1.29248857 epoch total loss 1.30069947\n",
      "Trained batch 723 batch loss 1.38610721 epoch total loss 1.30081761\n",
      "Trained batch 724 batch loss 1.10471034 epoch total loss 1.30054677\n",
      "Trained batch 725 batch loss 1.17003131 epoch total loss 1.30036676\n",
      "Trained batch 726 batch loss 1.12898707 epoch total loss 1.30013072\n",
      "Trained batch 727 batch loss 1.28651989 epoch total loss 1.30011201\n",
      "Trained batch 728 batch loss 1.27746677 epoch total loss 1.3000809\n",
      "Trained batch 729 batch loss 1.31977201 epoch total loss 1.30010784\n",
      "Trained batch 730 batch loss 1.37184131 epoch total loss 1.30020607\n",
      "Trained batch 731 batch loss 1.22728 epoch total loss 1.30010641\n",
      "Trained batch 732 batch loss 1.2938292 epoch total loss 1.30009782\n",
      "Trained batch 733 batch loss 1.24477935 epoch total loss 1.30002224\n",
      "Trained batch 734 batch loss 1.29794562 epoch total loss 1.3000195\n",
      "Trained batch 735 batch loss 1.24674296 epoch total loss 1.29994702\n",
      "Trained batch 736 batch loss 1.34862709 epoch total loss 1.30001318\n",
      "Trained batch 737 batch loss 1.2769959 epoch total loss 1.29998195\n",
      "Trained batch 738 batch loss 1.21034956 epoch total loss 1.29986048\n",
      "Trained batch 739 batch loss 1.05930948 epoch total loss 1.29953492\n",
      "Trained batch 740 batch loss 1.10314965 epoch total loss 1.29926956\n",
      "Trained batch 741 batch loss 1.12568903 epoch total loss 1.29903531\n",
      "Trained batch 742 batch loss 1.24557972 epoch total loss 1.29896331\n",
      "Trained batch 743 batch loss 1.20428562 epoch total loss 1.29883587\n",
      "Trained batch 744 batch loss 1.2488544 epoch total loss 1.29876864\n",
      "Trained batch 745 batch loss 1.19083023 epoch total loss 1.2986238\n",
      "Trained batch 746 batch loss 1.27072453 epoch total loss 1.29858649\n",
      "Trained batch 747 batch loss 1.26931047 epoch total loss 1.29854727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 748 batch loss 1.22196388 epoch total loss 1.29844487\n",
      "Trained batch 749 batch loss 1.11435235 epoch total loss 1.29819906\n",
      "Trained batch 750 batch loss 1.23360145 epoch total loss 1.29811299\n",
      "Trained batch 751 batch loss 1.16174603 epoch total loss 1.29793131\n",
      "Trained batch 752 batch loss 1.25380707 epoch total loss 1.29787266\n",
      "Trained batch 753 batch loss 1.3338927 epoch total loss 1.29792047\n",
      "Trained batch 754 batch loss 1.56665862 epoch total loss 1.2982769\n",
      "Trained batch 755 batch loss 1.44712412 epoch total loss 1.29847407\n",
      "Trained batch 756 batch loss 1.30496573 epoch total loss 1.29848266\n",
      "Trained batch 757 batch loss 1.2617135 epoch total loss 1.29843414\n",
      "Trained batch 758 batch loss 1.22755861 epoch total loss 1.29834056\n",
      "Trained batch 759 batch loss 1.40372181 epoch total loss 1.29847944\n",
      "Trained batch 760 batch loss 1.11581016 epoch total loss 1.29823911\n",
      "Trained batch 761 batch loss 1.37338936 epoch total loss 1.29833782\n",
      "Trained batch 762 batch loss 1.43193 epoch total loss 1.29851317\n",
      "Trained batch 763 batch loss 1.2463913 epoch total loss 1.29844487\n",
      "Trained batch 764 batch loss 1.2591908 epoch total loss 1.29839349\n",
      "Trained batch 765 batch loss 1.29148376 epoch total loss 1.29838455\n",
      "Trained batch 766 batch loss 1.3128531 epoch total loss 1.29840338\n",
      "Trained batch 767 batch loss 1.29880869 epoch total loss 1.29840398\n",
      "Trained batch 768 batch loss 1.19561219 epoch total loss 1.29827011\n",
      "Trained batch 769 batch loss 1.26029348 epoch total loss 1.29822075\n",
      "Trained batch 770 batch loss 1.0815 epoch total loss 1.2979393\n",
      "Trained batch 771 batch loss 1.25089455 epoch total loss 1.29787827\n",
      "Trained batch 772 batch loss 1.22656131 epoch total loss 1.297786\n",
      "Trained batch 773 batch loss 1.17149043 epoch total loss 1.29762256\n",
      "Trained batch 774 batch loss 1.21702647 epoch total loss 1.29751849\n",
      "Trained batch 775 batch loss 1.27998281 epoch total loss 1.29749584\n",
      "Trained batch 776 batch loss 1.34678912 epoch total loss 1.29755938\n",
      "Trained batch 777 batch loss 1.3565042 epoch total loss 1.2976352\n",
      "Trained batch 778 batch loss 1.29963589 epoch total loss 1.29763782\n",
      "Trained batch 779 batch loss 1.43914938 epoch total loss 1.2978195\n",
      "Trained batch 780 batch loss 1.55124378 epoch total loss 1.29814434\n",
      "Trained batch 781 batch loss 1.37319946 epoch total loss 1.29824042\n",
      "Trained batch 782 batch loss 1.30902255 epoch total loss 1.29825425\n",
      "Trained batch 783 batch loss 1.4360801 epoch total loss 1.29843032\n",
      "Trained batch 784 batch loss 1.45439148 epoch total loss 1.29862916\n",
      "Trained batch 785 batch loss 1.22875357 epoch total loss 1.29854023\n",
      "Trained batch 786 batch loss 1.15137398 epoch total loss 1.29835296\n",
      "Trained batch 787 batch loss 1.18943715 epoch total loss 1.29821455\n",
      "Trained batch 788 batch loss 1.28789163 epoch total loss 1.29820156\n",
      "Trained batch 789 batch loss 1.33543348 epoch total loss 1.29824877\n",
      "Trained batch 790 batch loss 1.3839972 epoch total loss 1.29835725\n",
      "Trained batch 791 batch loss 1.24136257 epoch total loss 1.29828525\n",
      "Trained batch 792 batch loss 1.30120289 epoch total loss 1.29828882\n",
      "Trained batch 793 batch loss 1.26366 epoch total loss 1.29824519\n",
      "Trained batch 794 batch loss 1.20748234 epoch total loss 1.29813087\n",
      "Trained batch 795 batch loss 1.24686122 epoch total loss 1.29806638\n",
      "Trained batch 796 batch loss 1.24732471 epoch total loss 1.2980026\n",
      "Trained batch 797 batch loss 1.40472102 epoch total loss 1.29813647\n",
      "Trained batch 798 batch loss 1.44849265 epoch total loss 1.29832482\n",
      "Trained batch 799 batch loss 1.13952732 epoch total loss 1.2981261\n",
      "Trained batch 800 batch loss 1.251261 epoch total loss 1.29806745\n",
      "Trained batch 801 batch loss 1.17856669 epoch total loss 1.29791832\n",
      "Trained batch 802 batch loss 1.38145 epoch total loss 1.29802251\n",
      "Trained batch 803 batch loss 1.32482743 epoch total loss 1.29805589\n",
      "Trained batch 804 batch loss 1.30098164 epoch total loss 1.29805958\n",
      "Trained batch 805 batch loss 1.22418046 epoch total loss 1.29796767\n",
      "Trained batch 806 batch loss 1.35028458 epoch total loss 1.29803264\n",
      "Trained batch 807 batch loss 1.36948049 epoch total loss 1.29812121\n",
      "Trained batch 808 batch loss 1.20919204 epoch total loss 1.2980113\n",
      "Trained batch 809 batch loss 1.18592119 epoch total loss 1.29787266\n",
      "Trained batch 810 batch loss 1.12159014 epoch total loss 1.29765499\n",
      "Trained batch 811 batch loss 1.12194097 epoch total loss 1.29743838\n",
      "Trained batch 812 batch loss 1.32877755 epoch total loss 1.29747689\n",
      "Trained batch 813 batch loss 1.27958477 epoch total loss 1.29745483\n",
      "Trained batch 814 batch loss 1.31970465 epoch total loss 1.29748225\n",
      "Trained batch 815 batch loss 1.10228705 epoch total loss 1.29724276\n",
      "Trained batch 816 batch loss 1.22502875 epoch total loss 1.29715419\n",
      "Trained batch 817 batch loss 1.26632118 epoch total loss 1.2971164\n",
      "Trained batch 818 batch loss 1.35073578 epoch total loss 1.29718196\n",
      "Trained batch 819 batch loss 1.3402282 epoch total loss 1.29723454\n",
      "Trained batch 820 batch loss 1.29336333 epoch total loss 1.29722977\n",
      "Trained batch 821 batch loss 1.21407557 epoch total loss 1.29712856\n",
      "Trained batch 822 batch loss 1.02694082 epoch total loss 1.2967999\n",
      "Trained batch 823 batch loss 1.00470912 epoch total loss 1.29644501\n",
      "Trained batch 824 batch loss 1.03229904 epoch total loss 1.29612446\n",
      "Trained batch 825 batch loss 1.25556171 epoch total loss 1.29607534\n",
      "Trained batch 826 batch loss 1.25090361 epoch total loss 1.29602063\n",
      "Trained batch 827 batch loss 1.34285569 epoch total loss 1.29607737\n",
      "Trained batch 828 batch loss 1.32798409 epoch total loss 1.29611588\n",
      "Trained batch 829 batch loss 1.33705592 epoch total loss 1.29616523\n",
      "Trained batch 830 batch loss 1.26581764 epoch total loss 1.29612875\n",
      "Trained batch 831 batch loss 1.37056267 epoch total loss 1.2962184\n",
      "Trained batch 832 batch loss 1.37405372 epoch total loss 1.29631186\n",
      "Trained batch 833 batch loss 1.37669158 epoch total loss 1.29640841\n",
      "Trained batch 834 batch loss 1.40430176 epoch total loss 1.29653776\n",
      "Trained batch 835 batch loss 1.22872 epoch total loss 1.29645658\n",
      "Trained batch 836 batch loss 1.35051703 epoch total loss 1.29652119\n",
      "Trained batch 837 batch loss 1.35773849 epoch total loss 1.29659438\n",
      "Trained batch 838 batch loss 1.28944874 epoch total loss 1.2965858\n",
      "Trained batch 839 batch loss 1.37425947 epoch total loss 1.29667842\n",
      "Trained batch 840 batch loss 1.24814034 epoch total loss 1.29662073\n",
      "Trained batch 841 batch loss 1.29138947 epoch total loss 1.29661441\n",
      "Trained batch 842 batch loss 1.17060649 epoch total loss 1.29646492\n",
      "Trained batch 843 batch loss 1.30025125 epoch total loss 1.29646945\n",
      "Trained batch 844 batch loss 1.38331914 epoch total loss 1.29657233\n",
      "Trained batch 845 batch loss 1.36255753 epoch total loss 1.29665041\n",
      "Trained batch 846 batch loss 1.31725574 epoch total loss 1.29667473\n",
      "Trained batch 847 batch loss 1.27484405 epoch total loss 1.29664898\n",
      "Trained batch 848 batch loss 1.34617448 epoch total loss 1.29670739\n",
      "Trained batch 849 batch loss 1.17582011 epoch total loss 1.29656506\n",
      "Trained batch 850 batch loss 1.12895536 epoch total loss 1.29636776\n",
      "Trained batch 851 batch loss 1.31770492 epoch total loss 1.29639292\n",
      "Trained batch 852 batch loss 1.21410418 epoch total loss 1.29629636\n",
      "Trained batch 853 batch loss 1.2125113 epoch total loss 1.29619813\n",
      "Trained batch 854 batch loss 1.27806437 epoch total loss 1.29617691\n",
      "Trained batch 855 batch loss 1.3482002 epoch total loss 1.29623771\n",
      "Trained batch 856 batch loss 1.19419897 epoch total loss 1.2961185\n",
      "Trained batch 857 batch loss 1.29153419 epoch total loss 1.29611313\n",
      "Trained batch 858 batch loss 1.35643697 epoch total loss 1.29618347\n",
      "Trained batch 859 batch loss 1.38475716 epoch total loss 1.29628658\n",
      "Trained batch 860 batch loss 1.47547817 epoch total loss 1.29649484\n",
      "Trained batch 861 batch loss 1.57132638 epoch total loss 1.29681408\n",
      "Trained batch 862 batch loss 1.34519577 epoch total loss 1.29687023\n",
      "Trained batch 863 batch loss 1.17858541 epoch total loss 1.29673314\n",
      "Trained batch 864 batch loss 1.37931371 epoch total loss 1.29682863\n",
      "Trained batch 865 batch loss 1.32472348 epoch total loss 1.29686093\n",
      "Trained batch 866 batch loss 1.48847258 epoch total loss 1.29708219\n",
      "Trained batch 867 batch loss 1.39163542 epoch total loss 1.29719126\n",
      "Trained batch 868 batch loss 1.21964741 epoch total loss 1.29710186\n",
      "Trained batch 869 batch loss 1.16282535 epoch total loss 1.29694736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 870 batch loss 1.22309649 epoch total loss 1.29686248\n",
      "Trained batch 871 batch loss 1.29426718 epoch total loss 1.29685962\n",
      "Trained batch 872 batch loss 1.33716774 epoch total loss 1.29690576\n",
      "Trained batch 873 batch loss 1.31533217 epoch total loss 1.29692686\n",
      "Trained batch 874 batch loss 1.36401391 epoch total loss 1.29700363\n",
      "Trained batch 875 batch loss 1.34477353 epoch total loss 1.29705822\n",
      "Trained batch 876 batch loss 1.40596485 epoch total loss 1.29718256\n",
      "Trained batch 877 batch loss 1.41844904 epoch total loss 1.29732084\n",
      "Trained batch 878 batch loss 1.34042764 epoch total loss 1.29737\n",
      "Trained batch 879 batch loss 1.23668075 epoch total loss 1.29730093\n",
      "Trained batch 880 batch loss 1.31316066 epoch total loss 1.29731894\n",
      "Trained batch 881 batch loss 1.38165307 epoch total loss 1.29741466\n",
      "Trained batch 882 batch loss 1.27661896 epoch total loss 1.29739106\n",
      "Trained batch 883 batch loss 1.25753486 epoch total loss 1.297346\n",
      "Trained batch 884 batch loss 1.18922925 epoch total loss 1.29722369\n",
      "Trained batch 885 batch loss 1.17014933 epoch total loss 1.29708016\n",
      "Trained batch 886 batch loss 1.17484903 epoch total loss 1.29694211\n",
      "Trained batch 887 batch loss 1.21001291 epoch total loss 1.29684401\n",
      "Trained batch 888 batch loss 1.25611269 epoch total loss 1.29679811\n",
      "Trained batch 889 batch loss 1.22289157 epoch total loss 1.29671502\n",
      "Trained batch 890 batch loss 1.10043931 epoch total loss 1.29649448\n",
      "Trained batch 891 batch loss 1.05569088 epoch total loss 1.29622424\n",
      "Trained batch 892 batch loss 1.19208348 epoch total loss 1.29610753\n",
      "Trained batch 893 batch loss 1.29174495 epoch total loss 1.29610264\n",
      "Trained batch 894 batch loss 1.60436404 epoch total loss 1.29644752\n",
      "Trained batch 895 batch loss 1.51072764 epoch total loss 1.29668689\n",
      "Trained batch 896 batch loss 1.4205687 epoch total loss 1.29682517\n",
      "Trained batch 897 batch loss 1.35636079 epoch total loss 1.29689145\n",
      "Trained batch 898 batch loss 1.26351798 epoch total loss 1.29685438\n",
      "Trained batch 899 batch loss 1.32420874 epoch total loss 1.29688478\n",
      "Trained batch 900 batch loss 1.36339188 epoch total loss 1.29695868\n",
      "Trained batch 901 batch loss 1.25743079 epoch total loss 1.29691482\n",
      "Trained batch 902 batch loss 1.26975274 epoch total loss 1.29688478\n",
      "Trained batch 903 batch loss 1.40465534 epoch total loss 1.2970041\n",
      "Trained batch 904 batch loss 1.24326968 epoch total loss 1.29694462\n",
      "Trained batch 905 batch loss 1.26958489 epoch total loss 1.29691434\n",
      "Trained batch 906 batch loss 1.25314116 epoch total loss 1.29686606\n",
      "Trained batch 907 batch loss 1.21770883 epoch total loss 1.2967788\n",
      "Trained batch 908 batch loss 1.257581 epoch total loss 1.29673564\n",
      "Trained batch 909 batch loss 1.23779297 epoch total loss 1.29667079\n",
      "Trained batch 910 batch loss 1.20882535 epoch total loss 1.29657423\n",
      "Trained batch 911 batch loss 1.14800417 epoch total loss 1.29641116\n",
      "Trained batch 912 batch loss 1.26554978 epoch total loss 1.29637718\n",
      "Trained batch 913 batch loss 1.21766257 epoch total loss 1.29629099\n",
      "Trained batch 914 batch loss 1.15772045 epoch total loss 1.29613936\n",
      "Trained batch 915 batch loss 1.19912922 epoch total loss 1.29603326\n",
      "Trained batch 916 batch loss 1.3257525 epoch total loss 1.29606581\n",
      "Trained batch 917 batch loss 1.41863489 epoch total loss 1.29619944\n",
      "Trained batch 918 batch loss 1.27143145 epoch total loss 1.2961725\n",
      "Trained batch 919 batch loss 1.30146599 epoch total loss 1.29617834\n",
      "Trained batch 920 batch loss 1.30337274 epoch total loss 1.29618609\n",
      "Trained batch 921 batch loss 1.37138796 epoch total loss 1.29626775\n",
      "Trained batch 922 batch loss 1.2956357 epoch total loss 1.29626703\n",
      "Trained batch 923 batch loss 1.26844299 epoch total loss 1.29623687\n",
      "Trained batch 924 batch loss 1.24809766 epoch total loss 1.29618478\n",
      "Trained batch 925 batch loss 1.31681275 epoch total loss 1.29620695\n",
      "Trained batch 926 batch loss 1.11175478 epoch total loss 1.29600775\n",
      "Trained batch 927 batch loss 1.16117883 epoch total loss 1.2958622\n",
      "Trained batch 928 batch loss 1.37730575 epoch total loss 1.29595\n",
      "Trained batch 929 batch loss 1.21561682 epoch total loss 1.29586351\n",
      "Trained batch 930 batch loss 1.24382663 epoch total loss 1.29580748\n",
      "Trained batch 931 batch loss 1.16021252 epoch total loss 1.29566181\n",
      "Trained batch 932 batch loss 1.19133759 epoch total loss 1.29554975\n",
      "Trained batch 933 batch loss 1.35925686 epoch total loss 1.29561806\n",
      "Trained batch 934 batch loss 1.40592563 epoch total loss 1.29573607\n",
      "Trained batch 935 batch loss 1.19207847 epoch total loss 1.29562533\n",
      "Trained batch 936 batch loss 1.13807869 epoch total loss 1.29545701\n",
      "Trained batch 937 batch loss 1.24406075 epoch total loss 1.29540205\n",
      "Trained batch 938 batch loss 1.23605382 epoch total loss 1.29533887\n",
      "Trained batch 939 batch loss 1.39590585 epoch total loss 1.29544592\n",
      "Trained batch 940 batch loss 1.39750361 epoch total loss 1.2955544\n",
      "Trained batch 941 batch loss 1.40603185 epoch total loss 1.29567182\n",
      "Trained batch 942 batch loss 1.30646908 epoch total loss 1.29568326\n",
      "Trained batch 943 batch loss 1.31931567 epoch total loss 1.29570842\n",
      "Trained batch 944 batch loss 1.25279224 epoch total loss 1.295663\n",
      "Trained batch 945 batch loss 1.2951144 epoch total loss 1.2956624\n",
      "Trained batch 946 batch loss 1.19769788 epoch total loss 1.29555893\n",
      "Trained batch 947 batch loss 1.30485797 epoch total loss 1.2955687\n",
      "Trained batch 948 batch loss 1.26133561 epoch total loss 1.29553258\n",
      "Trained batch 949 batch loss 1.31415761 epoch total loss 1.29555225\n",
      "Trained batch 950 batch loss 1.29952538 epoch total loss 1.29555655\n",
      "Trained batch 951 batch loss 1.22884452 epoch total loss 1.29548645\n",
      "Trained batch 952 batch loss 1.3796978 epoch total loss 1.29557478\n",
      "Trained batch 953 batch loss 1.34199727 epoch total loss 1.29562354\n",
      "Trained batch 954 batch loss 1.41908514 epoch total loss 1.295753\n",
      "Trained batch 955 batch loss 1.35961711 epoch total loss 1.29581988\n",
      "Trained batch 956 batch loss 1.28011417 epoch total loss 1.29580343\n",
      "Trained batch 957 batch loss 1.30073571 epoch total loss 1.29580867\n",
      "Trained batch 958 batch loss 1.34785461 epoch total loss 1.29586303\n",
      "Trained batch 959 batch loss 1.35018921 epoch total loss 1.29591966\n",
      "Trained batch 960 batch loss 1.44464767 epoch total loss 1.29607463\n",
      "Trained batch 961 batch loss 1.28160095 epoch total loss 1.29605961\n",
      "Trained batch 962 batch loss 1.30033696 epoch total loss 1.29606402\n",
      "Trained batch 963 batch loss 1.30854571 epoch total loss 1.29607701\n",
      "Trained batch 964 batch loss 1.22551227 epoch total loss 1.29600382\n",
      "Trained batch 965 batch loss 1.36516881 epoch total loss 1.29607546\n",
      "Trained batch 966 batch loss 1.25444865 epoch total loss 1.29603231\n",
      "Trained batch 967 batch loss 1.36168647 epoch total loss 1.29610014\n",
      "Trained batch 968 batch loss 1.23537457 epoch total loss 1.29603744\n",
      "Trained batch 969 batch loss 1.19985962 epoch total loss 1.29593813\n",
      "Trained batch 970 batch loss 1.12965667 epoch total loss 1.29576671\n",
      "Trained batch 971 batch loss 1.13937223 epoch total loss 1.29560566\n",
      "Trained batch 972 batch loss 1.22393179 epoch total loss 1.29553187\n",
      "Trained batch 973 batch loss 1.26576805 epoch total loss 1.29550123\n",
      "Trained batch 974 batch loss 1.16013312 epoch total loss 1.29536223\n",
      "Trained batch 975 batch loss 1.12616038 epoch total loss 1.29518878\n",
      "Trained batch 976 batch loss 1.16218257 epoch total loss 1.29505253\n",
      "Trained batch 977 batch loss 1.2322557 epoch total loss 1.29498839\n",
      "Trained batch 978 batch loss 1.25565207 epoch total loss 1.2949481\n",
      "Trained batch 979 batch loss 1.4769361 epoch total loss 1.29513395\n",
      "Trained batch 980 batch loss 1.62571836 epoch total loss 1.29547131\n",
      "Trained batch 981 batch loss 1.53416085 epoch total loss 1.29571462\n",
      "Trained batch 982 batch loss 1.42984462 epoch total loss 1.29585123\n",
      "Trained batch 983 batch loss 1.34198141 epoch total loss 1.2958982\n",
      "Trained batch 984 batch loss 1.29551888 epoch total loss 1.29589784\n",
      "Trained batch 985 batch loss 1.18644619 epoch total loss 1.29578662\n",
      "Trained batch 986 batch loss 1.17297435 epoch total loss 1.29566205\n",
      "Trained batch 987 batch loss 1.29662466 epoch total loss 1.29566312\n",
      "Trained batch 988 batch loss 1.42029142 epoch total loss 1.29578924\n",
      "Trained batch 989 batch loss 1.2667309 epoch total loss 1.2957598\n",
      "Trained batch 990 batch loss 1.30171037 epoch total loss 1.29576588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 991 batch loss 1.23657858 epoch total loss 1.29570615\n",
      "Trained batch 992 batch loss 1.14849281 epoch total loss 1.29555774\n",
      "Trained batch 993 batch loss 1.14401078 epoch total loss 1.29540515\n",
      "Trained batch 994 batch loss 1.22315323 epoch total loss 1.29533243\n",
      "Trained batch 995 batch loss 1.22651756 epoch total loss 1.29526329\n",
      "Trained batch 996 batch loss 1.11639273 epoch total loss 1.29508364\n",
      "Trained batch 997 batch loss 1.2242558 epoch total loss 1.29501259\n",
      "Trained batch 998 batch loss 1.2394315 epoch total loss 1.2949568\n",
      "Trained batch 999 batch loss 1.46963811 epoch total loss 1.29513168\n",
      "Trained batch 1000 batch loss 1.32005179 epoch total loss 1.2951566\n",
      "Trained batch 1001 batch loss 1.34655666 epoch total loss 1.29520798\n",
      "Trained batch 1002 batch loss 1.51502454 epoch total loss 1.29542732\n",
      "Trained batch 1003 batch loss 1.33881152 epoch total loss 1.2954706\n",
      "Trained batch 1004 batch loss 1.32411695 epoch total loss 1.29549921\n",
      "Trained batch 1005 batch loss 1.26688623 epoch total loss 1.2954706\n",
      "Trained batch 1006 batch loss 1.2916888 epoch total loss 1.2954669\n",
      "Trained batch 1007 batch loss 1.30155694 epoch total loss 1.29547298\n",
      "Trained batch 1008 batch loss 1.38518882 epoch total loss 1.29556191\n",
      "Trained batch 1009 batch loss 1.2565732 epoch total loss 1.29552329\n",
      "Trained batch 1010 batch loss 1.36003792 epoch total loss 1.29558706\n",
      "Trained batch 1011 batch loss 1.18760669 epoch total loss 1.29548025\n",
      "Trained batch 1012 batch loss 1.22106755 epoch total loss 1.29540682\n",
      "Trained batch 1013 batch loss 1.12237608 epoch total loss 1.29523599\n",
      "Trained batch 1014 batch loss 1.12617576 epoch total loss 1.29506934\n",
      "Trained batch 1015 batch loss 1.20999742 epoch total loss 1.29498553\n",
      "Trained batch 1016 batch loss 1.38049793 epoch total loss 1.29506969\n",
      "Trained batch 1017 batch loss 1.33014 epoch total loss 1.29510415\n",
      "Trained batch 1018 batch loss 1.41654205 epoch total loss 1.29522347\n",
      "Trained batch 1019 batch loss 1.36537242 epoch total loss 1.29529226\n",
      "Trained batch 1020 batch loss 1.26131785 epoch total loss 1.295259\n",
      "Trained batch 1021 batch loss 1.21651256 epoch total loss 1.29518187\n",
      "Trained batch 1022 batch loss 1.34433901 epoch total loss 1.29523\n",
      "Trained batch 1023 batch loss 1.32802129 epoch total loss 1.2952621\n",
      "Trained batch 1024 batch loss 1.26721954 epoch total loss 1.29523468\n",
      "Trained batch 1025 batch loss 1.29324985 epoch total loss 1.29523265\n",
      "Trained batch 1026 batch loss 1.22630274 epoch total loss 1.29516554\n",
      "Trained batch 1027 batch loss 1.25798881 epoch total loss 1.2951293\n",
      "Trained batch 1028 batch loss 1.34154582 epoch total loss 1.29517448\n",
      "Trained batch 1029 batch loss 1.27298772 epoch total loss 1.2951529\n",
      "Trained batch 1030 batch loss 1.13434696 epoch total loss 1.29499674\n",
      "Trained batch 1031 batch loss 1.27576184 epoch total loss 1.29497814\n",
      "Trained batch 1032 batch loss 1.3124342 epoch total loss 1.29499495\n",
      "Trained batch 1033 batch loss 1.25724697 epoch total loss 1.29495835\n",
      "Trained batch 1034 batch loss 1.31565142 epoch total loss 1.29497838\n",
      "Trained batch 1035 batch loss 1.37210155 epoch total loss 1.29505289\n",
      "Trained batch 1036 batch loss 1.34478569 epoch total loss 1.29510081\n",
      "Trained batch 1037 batch loss 1.41118836 epoch total loss 1.29521275\n",
      "Trained batch 1038 batch loss 1.36831725 epoch total loss 1.2952832\n",
      "Trained batch 1039 batch loss 1.32909369 epoch total loss 1.29531574\n",
      "Trained batch 1040 batch loss 1.29563391 epoch total loss 1.29531598\n",
      "Trained batch 1041 batch loss 1.16595531 epoch total loss 1.29519176\n",
      "Trained batch 1042 batch loss 1.26297474 epoch total loss 1.29516089\n",
      "Trained batch 1043 batch loss 1.30191255 epoch total loss 1.29516733\n",
      "Trained batch 1044 batch loss 1.19422817 epoch total loss 1.29507065\n",
      "Trained batch 1045 batch loss 1.15658343 epoch total loss 1.29493809\n",
      "Trained batch 1046 batch loss 1.16307473 epoch total loss 1.29481208\n",
      "Trained batch 1047 batch loss 1.0895853 epoch total loss 1.2946161\n",
      "Trained batch 1048 batch loss 1.31041217 epoch total loss 1.29463112\n",
      "Trained batch 1049 batch loss 1.35691345 epoch total loss 1.29469049\n",
      "Trained batch 1050 batch loss 1.35993278 epoch total loss 1.29475272\n",
      "Trained batch 1051 batch loss 1.49585152 epoch total loss 1.29494405\n",
      "Trained batch 1052 batch loss 1.4175055 epoch total loss 1.29506052\n",
      "Trained batch 1053 batch loss 1.34127307 epoch total loss 1.2951045\n",
      "Trained batch 1054 batch loss 1.27423859 epoch total loss 1.29508471\n",
      "Trained batch 1055 batch loss 1.16176796 epoch total loss 1.29495835\n",
      "Trained batch 1056 batch loss 1.19428563 epoch total loss 1.29486299\n",
      "Trained batch 1057 batch loss 1.25148869 epoch total loss 1.29482198\n",
      "Trained batch 1058 batch loss 1.26439631 epoch total loss 1.29479325\n",
      "Trained batch 1059 batch loss 1.31632328 epoch total loss 1.29481351\n",
      "Trained batch 1060 batch loss 1.31577158 epoch total loss 1.2948333\n",
      "Trained batch 1061 batch loss 1.23529482 epoch total loss 1.29477727\n",
      "Trained batch 1062 batch loss 1.39133132 epoch total loss 1.29486823\n",
      "Trained batch 1063 batch loss 1.29755056 epoch total loss 1.29487073\n",
      "Trained batch 1064 batch loss 1.23646235 epoch total loss 1.2948159\n",
      "Trained batch 1065 batch loss 1.31261885 epoch total loss 1.29483259\n",
      "Trained batch 1066 batch loss 1.38422406 epoch total loss 1.29491651\n",
      "Trained batch 1067 batch loss 1.21671033 epoch total loss 1.2948432\n",
      "Trained batch 1068 batch loss 1.41380548 epoch total loss 1.29495454\n",
      "Trained batch 1069 batch loss 1.28863716 epoch total loss 1.2949487\n",
      "Trained batch 1070 batch loss 1.31033587 epoch total loss 1.294963\n",
      "Trained batch 1071 batch loss 1.26493132 epoch total loss 1.29493499\n",
      "Trained batch 1072 batch loss 1.35413206 epoch total loss 1.29499018\n",
      "Trained batch 1073 batch loss 1.23249578 epoch total loss 1.29493201\n",
      "Trained batch 1074 batch loss 1.26524019 epoch total loss 1.29490435\n",
      "Trained batch 1075 batch loss 1.40175378 epoch total loss 1.29500377\n",
      "Trained batch 1076 batch loss 1.36052704 epoch total loss 1.29506457\n",
      "Trained batch 1077 batch loss 1.35376453 epoch total loss 1.29511905\n",
      "Trained batch 1078 batch loss 1.17787838 epoch total loss 1.29501033\n",
      "Trained batch 1079 batch loss 1.13648176 epoch total loss 1.29486334\n",
      "Trained batch 1080 batch loss 1.18674421 epoch total loss 1.29476333\n",
      "Trained batch 1081 batch loss 1.19267392 epoch total loss 1.29466879\n",
      "Trained batch 1082 batch loss 1.35079706 epoch total loss 1.29472077\n",
      "Trained batch 1083 batch loss 1.31687462 epoch total loss 1.29474115\n",
      "Trained batch 1084 batch loss 1.29382062 epoch total loss 1.29474032\n",
      "Trained batch 1085 batch loss 1.32424533 epoch total loss 1.2947675\n",
      "Trained batch 1086 batch loss 1.25925851 epoch total loss 1.29473484\n",
      "Trained batch 1087 batch loss 1.35600436 epoch total loss 1.2947911\n",
      "Trained batch 1088 batch loss 1.28464079 epoch total loss 1.2947818\n",
      "Trained batch 1089 batch loss 1.12551522 epoch total loss 1.29462636\n",
      "Trained batch 1090 batch loss 1.27754474 epoch total loss 1.29461074\n",
      "Trained batch 1091 batch loss 1.30839956 epoch total loss 1.29462337\n",
      "Trained batch 1092 batch loss 1.19958878 epoch total loss 1.29453635\n",
      "Trained batch 1093 batch loss 1.35498953 epoch total loss 1.29459167\n",
      "Trained batch 1094 batch loss 1.34516299 epoch total loss 1.29463792\n",
      "Trained batch 1095 batch loss 1.31859934 epoch total loss 1.29465973\n",
      "Trained batch 1096 batch loss 1.3413868 epoch total loss 1.29470241\n",
      "Trained batch 1097 batch loss 1.40118909 epoch total loss 1.29479957\n",
      "Trained batch 1098 batch loss 1.33547413 epoch total loss 1.29483664\n",
      "Trained batch 1099 batch loss 1.43140149 epoch total loss 1.29496086\n",
      "Trained batch 1100 batch loss 1.18907011 epoch total loss 1.29486465\n",
      "Trained batch 1101 batch loss 1.24161124 epoch total loss 1.29481626\n",
      "Trained batch 1102 batch loss 1.23672843 epoch total loss 1.29476345\n",
      "Trained batch 1103 batch loss 1.16888356 epoch total loss 1.29464924\n",
      "Trained batch 1104 batch loss 1.11981344 epoch total loss 1.29449093\n",
      "Trained batch 1105 batch loss 1.17502034 epoch total loss 1.29438293\n",
      "Trained batch 1106 batch loss 1.27927661 epoch total loss 1.29436922\n",
      "Trained batch 1107 batch loss 1.20525 epoch total loss 1.29428875\n",
      "Trained batch 1108 batch loss 1.1665889 epoch total loss 1.29417348\n",
      "Trained batch 1109 batch loss 1.24873328 epoch total loss 1.29413259\n",
      "Trained batch 1110 batch loss 1.21248388 epoch total loss 1.29405904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1111 batch loss 1.13699663 epoch total loss 1.29391766\n",
      "Trained batch 1112 batch loss 1.2657553 epoch total loss 1.29389226\n",
      "Trained batch 1113 batch loss 1.39132512 epoch total loss 1.29397988\n",
      "Trained batch 1114 batch loss 1.33639061 epoch total loss 1.29401791\n",
      "Trained batch 1115 batch loss 1.33965969 epoch total loss 1.2940588\n",
      "Trained batch 1116 batch loss 1.24737763 epoch total loss 1.29401708\n",
      "Trained batch 1117 batch loss 1.26699162 epoch total loss 1.29399288\n",
      "Trained batch 1118 batch loss 1.11324787 epoch total loss 1.29383123\n",
      "Trained batch 1119 batch loss 1.08145607 epoch total loss 1.29364145\n",
      "Trained batch 1120 batch loss 1.2240591 epoch total loss 1.29357922\n",
      "Trained batch 1121 batch loss 1.43374741 epoch total loss 1.29370427\n",
      "Trained batch 1122 batch loss 1.34610391 epoch total loss 1.29375088\n",
      "Trained batch 1123 batch loss 1.24282718 epoch total loss 1.29370546\n",
      "Trained batch 1124 batch loss 1.22428274 epoch total loss 1.29364371\n",
      "Trained batch 1125 batch loss 1.18473721 epoch total loss 1.29354692\n",
      "Trained batch 1126 batch loss 1.25846744 epoch total loss 1.29351568\n",
      "Trained batch 1127 batch loss 1.12253284 epoch total loss 1.29336405\n",
      "Trained batch 1128 batch loss 1.15261114 epoch total loss 1.29323924\n",
      "Trained batch 1129 batch loss 1.2966876 epoch total loss 1.29324222\n",
      "Trained batch 1130 batch loss 1.33758569 epoch total loss 1.29328144\n",
      "Trained batch 1131 batch loss 1.33951914 epoch total loss 1.29332232\n",
      "Trained batch 1132 batch loss 1.30706453 epoch total loss 1.29333436\n",
      "Trained batch 1133 batch loss 1.14822412 epoch total loss 1.29320633\n",
      "Trained batch 1134 batch loss 1.2315805 epoch total loss 1.29315197\n",
      "Trained batch 1135 batch loss 1.13933468 epoch total loss 1.29301643\n",
      "Trained batch 1136 batch loss 1.26384366 epoch total loss 1.29299068\n",
      "Trained batch 1137 batch loss 1.09917939 epoch total loss 1.29282022\n",
      "Trained batch 1138 batch loss 1.13873625 epoch total loss 1.29268479\n",
      "Trained batch 1139 batch loss 1.23324907 epoch total loss 1.2926327\n",
      "Trained batch 1140 batch loss 1.19839454 epoch total loss 1.29255\n",
      "Trained batch 1141 batch loss 1.12924647 epoch total loss 1.29240692\n",
      "Trained batch 1142 batch loss 1.20997131 epoch total loss 1.29233468\n",
      "Trained batch 1143 batch loss 1.15439177 epoch total loss 1.29221404\n",
      "Trained batch 1144 batch loss 1.16735435 epoch total loss 1.29210484\n",
      "Trained batch 1145 batch loss 1.42055058 epoch total loss 1.29221702\n",
      "Trained batch 1146 batch loss 1.34608006 epoch total loss 1.29226398\n",
      "Trained batch 1147 batch loss 1.33031368 epoch total loss 1.29229724\n",
      "Trained batch 1148 batch loss 1.32837987 epoch total loss 1.2923286\n",
      "Trained batch 1149 batch loss 1.2666539 epoch total loss 1.29230618\n",
      "Trained batch 1150 batch loss 1.39751709 epoch total loss 1.29239762\n",
      "Trained batch 1151 batch loss 1.27088881 epoch total loss 1.29237902\n",
      "Trained batch 1152 batch loss 1.23389292 epoch total loss 1.29232824\n",
      "Trained batch 1153 batch loss 1.33358467 epoch total loss 1.292364\n",
      "Trained batch 1154 batch loss 1.19374812 epoch total loss 1.29227853\n",
      "Trained batch 1155 batch loss 1.124192 epoch total loss 1.29213297\n",
      "Trained batch 1156 batch loss 1.15229452 epoch total loss 1.2920121\n",
      "Trained batch 1157 batch loss 1.1985774 epoch total loss 1.29193127\n",
      "Trained batch 1158 batch loss 1.1572181 epoch total loss 1.29181504\n",
      "Trained batch 1159 batch loss 1.16191375 epoch total loss 1.29170287\n",
      "Trained batch 1160 batch loss 1.18434215 epoch total loss 1.29161024\n",
      "Trained batch 1161 batch loss 1.11221445 epoch total loss 1.29145575\n",
      "Trained batch 1162 batch loss 1.16773653 epoch total loss 1.29134929\n",
      "Trained batch 1163 batch loss 1.22033978 epoch total loss 1.29128826\n",
      "Trained batch 1164 batch loss 1.33991027 epoch total loss 1.29133\n",
      "Trained batch 1165 batch loss 1.25241339 epoch total loss 1.2912966\n",
      "Trained batch 1166 batch loss 1.32883835 epoch total loss 1.29132891\n",
      "Trained batch 1167 batch loss 1.18929458 epoch total loss 1.29124141\n",
      "Trained batch 1168 batch loss 1.23628628 epoch total loss 1.29119444\n",
      "Trained batch 1169 batch loss 1.22765422 epoch total loss 1.29114008\n",
      "Trained batch 1170 batch loss 1.37392223 epoch total loss 1.29121089\n",
      "Trained batch 1171 batch loss 1.28167248 epoch total loss 1.29120266\n",
      "Trained batch 1172 batch loss 1.37575865 epoch total loss 1.29127479\n",
      "Trained batch 1173 batch loss 1.31149173 epoch total loss 1.29129207\n",
      "Trained batch 1174 batch loss 1.11219072 epoch total loss 1.29113948\n",
      "Trained batch 1175 batch loss 1.2367655 epoch total loss 1.29109323\n",
      "Trained batch 1176 batch loss 1.16418374 epoch total loss 1.29098535\n",
      "Trained batch 1177 batch loss 1.22643173 epoch total loss 1.29093051\n",
      "Trained batch 1178 batch loss 1.25923347 epoch total loss 1.29090357\n",
      "Trained batch 1179 batch loss 1.45680618 epoch total loss 1.29104435\n",
      "Trained batch 1180 batch loss 1.32141578 epoch total loss 1.29107\n",
      "Trained batch 1181 batch loss 1.1440531 epoch total loss 1.29094553\n",
      "Trained batch 1182 batch loss 1.04314876 epoch total loss 1.29073584\n",
      "Trained batch 1183 batch loss 1.11883771 epoch total loss 1.29059064\n",
      "Trained batch 1184 batch loss 1.08420026 epoch total loss 1.29041636\n",
      "Trained batch 1185 batch loss 1.11393726 epoch total loss 1.29026735\n",
      "Trained batch 1186 batch loss 1.09123182 epoch total loss 1.2900995\n",
      "Trained batch 1187 batch loss 1.08409929 epoch total loss 1.28992593\n",
      "Trained batch 1188 batch loss 1.22749949 epoch total loss 1.28987348\n",
      "Trained batch 1189 batch loss 1.21700799 epoch total loss 1.28981221\n",
      "Trained batch 1190 batch loss 1.24517536 epoch total loss 1.28977466\n",
      "Trained batch 1191 batch loss 1.320454 epoch total loss 1.28980041\n",
      "Trained batch 1192 batch loss 1.32917929 epoch total loss 1.28983343\n",
      "Trained batch 1193 batch loss 1.25334096 epoch total loss 1.28980279\n",
      "Trained batch 1194 batch loss 1.24493325 epoch total loss 1.28976512\n",
      "Trained batch 1195 batch loss 1.41262484 epoch total loss 1.289868\n",
      "Trained batch 1196 batch loss 1.29706907 epoch total loss 1.28987408\n",
      "Trained batch 1197 batch loss 1.3384223 epoch total loss 1.28991449\n",
      "Trained batch 1198 batch loss 1.44187987 epoch total loss 1.29004145\n",
      "Trained batch 1199 batch loss 1.52314687 epoch total loss 1.29023588\n",
      "Trained batch 1200 batch loss 1.50034225 epoch total loss 1.290411\n",
      "Trained batch 1201 batch loss 1.18948841 epoch total loss 1.29032695\n",
      "Trained batch 1202 batch loss 1.20067573 epoch total loss 1.29025233\n",
      "Trained batch 1203 batch loss 1.21602559 epoch total loss 1.2901907\n",
      "Trained batch 1204 batch loss 1.22129405 epoch total loss 1.29013348\n",
      "Trained batch 1205 batch loss 1.16226554 epoch total loss 1.29002726\n",
      "Trained batch 1206 batch loss 1.19532871 epoch total loss 1.28994882\n",
      "Trained batch 1207 batch loss 1.22459471 epoch total loss 1.2898947\n",
      "Trained batch 1208 batch loss 1.12836421 epoch total loss 1.28976095\n",
      "Trained batch 1209 batch loss 1.11337662 epoch total loss 1.28961515\n",
      "Trained batch 1210 batch loss 1.19890308 epoch total loss 1.28954\n",
      "Trained batch 1211 batch loss 1.28214681 epoch total loss 1.28953397\n",
      "Trained batch 1212 batch loss 1.23479617 epoch total loss 1.28948879\n",
      "Trained batch 1213 batch loss 1.11856616 epoch total loss 1.28934777\n",
      "Trained batch 1214 batch loss 1.14212191 epoch total loss 1.28922653\n",
      "Trained batch 1215 batch loss 1.16509068 epoch total loss 1.28912425\n",
      "Trained batch 1216 batch loss 1.3240211 epoch total loss 1.28915298\n",
      "Trained batch 1217 batch loss 1.32760584 epoch total loss 1.28918457\n",
      "Trained batch 1218 batch loss 1.28822184 epoch total loss 1.28918374\n",
      "Trained batch 1219 batch loss 1.43942523 epoch total loss 1.289307\n",
      "Trained batch 1220 batch loss 1.34389806 epoch total loss 1.28935182\n",
      "Trained batch 1221 batch loss 1.27905369 epoch total loss 1.28934336\n",
      "Trained batch 1222 batch loss 1.39658 epoch total loss 1.2894311\n",
      "Trained batch 1223 batch loss 1.37578678 epoch total loss 1.28950167\n",
      "Trained batch 1224 batch loss 1.40961456 epoch total loss 1.2895999\n",
      "Trained batch 1225 batch loss 1.37086844 epoch total loss 1.28966618\n",
      "Trained batch 1226 batch loss 1.25992203 epoch total loss 1.28964186\n",
      "Trained batch 1227 batch loss 1.13444364 epoch total loss 1.28951538\n",
      "Trained batch 1228 batch loss 1.18155789 epoch total loss 1.2894274\n",
      "Trained batch 1229 batch loss 1.34634674 epoch total loss 1.28947365\n",
      "Trained batch 1230 batch loss 1.2857337 epoch total loss 1.28947067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1231 batch loss 1.37439358 epoch total loss 1.28953969\n",
      "Trained batch 1232 batch loss 1.18293047 epoch total loss 1.28945315\n",
      "Trained batch 1233 batch loss 1.19468689 epoch total loss 1.28937638\n",
      "Trained batch 1234 batch loss 1.2700659 epoch total loss 1.28936064\n",
      "Trained batch 1235 batch loss 1.22876668 epoch total loss 1.28931153\n",
      "Trained batch 1236 batch loss 1.29092717 epoch total loss 1.28931284\n",
      "Trained batch 1237 batch loss 1.28209758 epoch total loss 1.289307\n",
      "Trained batch 1238 batch loss 1.35155046 epoch total loss 1.2893573\n",
      "Trained batch 1239 batch loss 1.37686443 epoch total loss 1.28942788\n",
      "Trained batch 1240 batch loss 1.3474735 epoch total loss 1.28947473\n",
      "Trained batch 1241 batch loss 1.40752864 epoch total loss 1.28956985\n",
      "Trained batch 1242 batch loss 1.33089578 epoch total loss 1.28960311\n",
      "Trained batch 1243 batch loss 1.27138972 epoch total loss 1.28958845\n",
      "Trained batch 1244 batch loss 1.32977271 epoch total loss 1.28962076\n",
      "Trained batch 1245 batch loss 1.2651844 epoch total loss 1.28960109\n",
      "Trained batch 1246 batch loss 1.24321413 epoch total loss 1.28956378\n",
      "Trained batch 1247 batch loss 1.22708416 epoch total loss 1.28951371\n",
      "Trained batch 1248 batch loss 1.32587 epoch total loss 1.28954291\n",
      "Trained batch 1249 batch loss 1.22657442 epoch total loss 1.28949249\n",
      "Trained batch 1250 batch loss 1.17393041 epoch total loss 1.2894\n",
      "Trained batch 1251 batch loss 1.18452203 epoch total loss 1.28931618\n",
      "Trained batch 1252 batch loss 1.11793411 epoch total loss 1.28917933\n",
      "Trained batch 1253 batch loss 1.20880377 epoch total loss 1.28911519\n",
      "Trained batch 1254 batch loss 1.26294041 epoch total loss 1.28909433\n",
      "Trained batch 1255 batch loss 1.27750146 epoch total loss 1.28908503\n",
      "Trained batch 1256 batch loss 1.32987773 epoch total loss 1.28911746\n",
      "Trained batch 1257 batch loss 1.28899777 epoch total loss 1.28911734\n",
      "Trained batch 1258 batch loss 1.36156392 epoch total loss 1.28917491\n",
      "Trained batch 1259 batch loss 1.32465374 epoch total loss 1.28920317\n",
      "Trained batch 1260 batch loss 1.26403713 epoch total loss 1.28918326\n",
      "Trained batch 1261 batch loss 1.31099415 epoch total loss 1.28920054\n",
      "Trained batch 1262 batch loss 1.31749642 epoch total loss 1.28922296\n",
      "Trained batch 1263 batch loss 1.27105522 epoch total loss 1.28920853\n",
      "Trained batch 1264 batch loss 1.26060474 epoch total loss 1.28918588\n",
      "Trained batch 1265 batch loss 1.29612195 epoch total loss 1.28919137\n",
      "Trained batch 1266 batch loss 1.26027799 epoch total loss 1.2891686\n",
      "Trained batch 1267 batch loss 1.1925596 epoch total loss 1.2890923\n",
      "Trained batch 1268 batch loss 1.21553588 epoch total loss 1.28903425\n",
      "Trained batch 1269 batch loss 1.20564508 epoch total loss 1.28896856\n",
      "Trained batch 1270 batch loss 1.23163104 epoch total loss 1.2889235\n",
      "Trained batch 1271 batch loss 1.38266206 epoch total loss 1.28899729\n",
      "Trained batch 1272 batch loss 1.51501966 epoch total loss 1.28917491\n",
      "Trained batch 1273 batch loss 1.51732445 epoch total loss 1.28935421\n",
      "Trained batch 1274 batch loss 1.46106541 epoch total loss 1.28948903\n",
      "Trained batch 1275 batch loss 1.31149459 epoch total loss 1.28950632\n",
      "Trained batch 1276 batch loss 1.14093053 epoch total loss 1.28938985\n",
      "Trained batch 1277 batch loss 1.13010073 epoch total loss 1.28926516\n",
      "Trained batch 1278 batch loss 0.991319358 epoch total loss 1.28903198\n",
      "Trained batch 1279 batch loss 1.06520736 epoch total loss 1.28885698\n",
      "Trained batch 1280 batch loss 1.18440437 epoch total loss 1.28877544\n",
      "Trained batch 1281 batch loss 1.31504893 epoch total loss 1.28879595\n",
      "Trained batch 1282 batch loss 1.2205075 epoch total loss 1.28874266\n",
      "Trained batch 1283 batch loss 1.31387472 epoch total loss 1.28876221\n",
      "Trained batch 1284 batch loss 1.29639125 epoch total loss 1.28876817\n",
      "Trained batch 1285 batch loss 1.14897895 epoch total loss 1.28865933\n",
      "Trained batch 1286 batch loss 1.01554048 epoch total loss 1.2884469\n",
      "Trained batch 1287 batch loss 1.25777376 epoch total loss 1.28842318\n",
      "Trained batch 1288 batch loss 1.3053937 epoch total loss 1.28843629\n",
      "Trained batch 1289 batch loss 1.34622812 epoch total loss 1.28848112\n",
      "Trained batch 1290 batch loss 1.45940793 epoch total loss 1.28861356\n",
      "Trained batch 1291 batch loss 1.31969666 epoch total loss 1.28863764\n",
      "Trained batch 1292 batch loss 1.3501265 epoch total loss 1.2886852\n",
      "Trained batch 1293 batch loss 1.30779254 epoch total loss 1.2887\n",
      "Trained batch 1294 batch loss 1.42063403 epoch total loss 1.28880191\n",
      "Trained batch 1295 batch loss 1.28087521 epoch total loss 1.28879583\n",
      "Trained batch 1296 batch loss 1.25669324 epoch total loss 1.28877103\n",
      "Trained batch 1297 batch loss 1.30888891 epoch total loss 1.28878653\n",
      "Trained batch 1298 batch loss 1.35131693 epoch total loss 1.28883469\n",
      "Trained batch 1299 batch loss 1.33058953 epoch total loss 1.28886688\n",
      "Trained batch 1300 batch loss 1.3848145 epoch total loss 1.28894067\n",
      "Trained batch 1301 batch loss 1.25642824 epoch total loss 1.28891563\n",
      "Trained batch 1302 batch loss 1.35859275 epoch total loss 1.28896916\n",
      "Trained batch 1303 batch loss 1.1970191 epoch total loss 1.28889859\n",
      "Trained batch 1304 batch loss 1.34824777 epoch total loss 1.28894413\n",
      "Trained batch 1305 batch loss 1.27720594 epoch total loss 1.28893518\n",
      "Trained batch 1306 batch loss 1.31148505 epoch total loss 1.28895247\n",
      "Trained batch 1307 batch loss 1.19863081 epoch total loss 1.28888333\n",
      "Trained batch 1308 batch loss 1.2024852 epoch total loss 1.28881729\n",
      "Trained batch 1309 batch loss 1.23141837 epoch total loss 1.28877354\n",
      "Trained batch 1310 batch loss 1.14242 epoch total loss 1.28866184\n",
      "Trained batch 1311 batch loss 1.14355421 epoch total loss 1.28855109\n",
      "Trained batch 1312 batch loss 1.19119847 epoch total loss 1.28847694\n",
      "Trained batch 1313 batch loss 1.14644229 epoch total loss 1.2883687\n",
      "Trained batch 1314 batch loss 1.26663482 epoch total loss 1.28835213\n",
      "Trained batch 1315 batch loss 1.16171527 epoch total loss 1.28825593\n",
      "Trained batch 1316 batch loss 1.1632117 epoch total loss 1.28816092\n",
      "Trained batch 1317 batch loss 1.18653822 epoch total loss 1.28808367\n",
      "Trained batch 1318 batch loss 1.1494751 epoch total loss 1.28797853\n",
      "Trained batch 1319 batch loss 1.29316342 epoch total loss 1.28798246\n",
      "Trained batch 1320 batch loss 1.30265605 epoch total loss 1.28799355\n",
      "Trained batch 1321 batch loss 1.33701813 epoch total loss 1.28803062\n",
      "Trained batch 1322 batch loss 1.28785157 epoch total loss 1.28803051\n",
      "Trained batch 1323 batch loss 1.26185083 epoch total loss 1.28801072\n",
      "Trained batch 1324 batch loss 1.43371582 epoch total loss 1.28812075\n",
      "Trained batch 1325 batch loss 1.31893694 epoch total loss 1.28814411\n",
      "Trained batch 1326 batch loss 1.27602267 epoch total loss 1.28813493\n",
      "Trained batch 1327 batch loss 1.18821645 epoch total loss 1.28805959\n",
      "Trained batch 1328 batch loss 1.15549195 epoch total loss 1.28795981\n",
      "Trained batch 1329 batch loss 1.21514356 epoch total loss 1.28790498\n",
      "Trained batch 1330 batch loss 1.22007918 epoch total loss 1.28785396\n",
      "Trained batch 1331 batch loss 1.27871096 epoch total loss 1.28784716\n",
      "Trained batch 1332 batch loss 1.22566926 epoch total loss 1.28780043\n",
      "Trained batch 1333 batch loss 1.20190573 epoch total loss 1.28773606\n",
      "Trained batch 1334 batch loss 1.20592201 epoch total loss 1.28767467\n",
      "Trained batch 1335 batch loss 1.24117947 epoch total loss 1.28763986\n",
      "Trained batch 1336 batch loss 1.1906687 epoch total loss 1.28756738\n",
      "Trained batch 1337 batch loss 1.15785193 epoch total loss 1.28747034\n",
      "Trained batch 1338 batch loss 1.15694785 epoch total loss 1.28737271\n",
      "Trained batch 1339 batch loss 1.22110021 epoch total loss 1.28732324\n",
      "Trained batch 1340 batch loss 1.23079038 epoch total loss 1.28728104\n",
      "Trained batch 1341 batch loss 1.2170167 epoch total loss 1.2872287\n",
      "Trained batch 1342 batch loss 1.24476337 epoch total loss 1.28719711\n",
      "Trained batch 1343 batch loss 1.20827031 epoch total loss 1.28713822\n",
      "Trained batch 1344 batch loss 1.11861062 epoch total loss 1.28701293\n",
      "Trained batch 1345 batch loss 1.28132272 epoch total loss 1.28700876\n",
      "Trained batch 1346 batch loss 1.41945994 epoch total loss 1.28710711\n",
      "Trained batch 1347 batch loss 1.33337235 epoch total loss 1.28714144\n",
      "Trained batch 1348 batch loss 1.38096714 epoch total loss 1.28721106\n",
      "Trained batch 1349 batch loss 1.2505039 epoch total loss 1.28718388\n",
      "Trained batch 1350 batch loss 1.20491195 epoch total loss 1.28712296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1351 batch loss 1.24347329 epoch total loss 1.28709066\n",
      "Trained batch 1352 batch loss 1.17835665 epoch total loss 1.28701019\n",
      "Trained batch 1353 batch loss 1.22820735 epoch total loss 1.28696668\n",
      "Trained batch 1354 batch loss 1.35998654 epoch total loss 1.28702068\n",
      "Trained batch 1355 batch loss 1.2648468 epoch total loss 1.28700435\n",
      "Trained batch 1356 batch loss 1.24756455 epoch total loss 1.28697526\n",
      "Trained batch 1357 batch loss 1.29260814 epoch total loss 1.28697944\n",
      "Trained batch 1358 batch loss 1.23489654 epoch total loss 1.28694105\n",
      "Trained batch 1359 batch loss 1.17838359 epoch total loss 1.28686106\n",
      "Trained batch 1360 batch loss 1.1891278 epoch total loss 1.28678918\n",
      "Trained batch 1361 batch loss 1.24834549 epoch total loss 1.28676093\n",
      "Trained batch 1362 batch loss 1.14636111 epoch total loss 1.28665781\n",
      "Trained batch 1363 batch loss 1.15628445 epoch total loss 1.2865622\n",
      "Trained batch 1364 batch loss 1.17660725 epoch total loss 1.28648162\n",
      "Trained batch 1365 batch loss 1.21535707 epoch total loss 1.28642941\n",
      "Trained batch 1366 batch loss 1.33872926 epoch total loss 1.28646779\n",
      "Trained batch 1367 batch loss 1.26924133 epoch total loss 1.28645515\n",
      "Trained batch 1368 batch loss 1.30130136 epoch total loss 1.286466\n",
      "Trained batch 1369 batch loss 1.23011303 epoch total loss 1.28642488\n",
      "Trained batch 1370 batch loss 1.27715373 epoch total loss 1.28641808\n",
      "Trained batch 1371 batch loss 1.16539896 epoch total loss 1.28632975\n",
      "Trained batch 1372 batch loss 1.29077721 epoch total loss 1.28633296\n",
      "Trained batch 1373 batch loss 1.19717216 epoch total loss 1.286268\n",
      "Trained batch 1374 batch loss 1.16605663 epoch total loss 1.2861805\n",
      "Trained batch 1375 batch loss 1.33668137 epoch total loss 1.28621721\n",
      "Trained batch 1376 batch loss 1.30766737 epoch total loss 1.28623283\n",
      "Trained batch 1377 batch loss 1.28149772 epoch total loss 1.28622937\n",
      "Trained batch 1378 batch loss 1.4386605 epoch total loss 1.28634\n",
      "Trained batch 1379 batch loss 1.42902374 epoch total loss 1.28644347\n",
      "Trained batch 1380 batch loss 1.23307729 epoch total loss 1.28640485\n",
      "Trained batch 1381 batch loss 1.3298403 epoch total loss 1.28643632\n",
      "Trained batch 1382 batch loss 1.37808657 epoch total loss 1.2865026\n",
      "Trained batch 1383 batch loss 1.3137095 epoch total loss 1.28652227\n",
      "Trained batch 1384 batch loss 1.22112155 epoch total loss 1.28647494\n",
      "Trained batch 1385 batch loss 1.21298587 epoch total loss 1.2864219\n",
      "Trained batch 1386 batch loss 1.17420399 epoch total loss 1.28634095\n",
      "Trained batch 1387 batch loss 1.07722878 epoch total loss 1.28619015\n",
      "Trained batch 1388 batch loss 1.23492289 epoch total loss 1.2861532\n",
      "Epoch 3 train loss 1.2861531972885132\n",
      "Validated batch 1 batch loss 1.26092434\n",
      "Validated batch 2 batch loss 1.14935732\n",
      "Validated batch 3 batch loss 1.26369643\n",
      "Validated batch 4 batch loss 1.15623009\n",
      "Validated batch 5 batch loss 1.26183009\n",
      "Validated batch 6 batch loss 1.27098286\n",
      "Validated batch 7 batch loss 1.25755107\n",
      "Validated batch 8 batch loss 1.39858961\n",
      "Validated batch 9 batch loss 1.34412241\n",
      "Validated batch 10 batch loss 1.26543283\n",
      "Validated batch 11 batch loss 1.29237282\n",
      "Validated batch 12 batch loss 1.35921335\n",
      "Validated batch 13 batch loss 1.33812129\n",
      "Validated batch 14 batch loss 1.31130052\n",
      "Validated batch 15 batch loss 1.31566715\n",
      "Validated batch 16 batch loss 1.34420693\n",
      "Validated batch 17 batch loss 1.29161727\n",
      "Validated batch 18 batch loss 1.15176582\n",
      "Validated batch 19 batch loss 1.2546829\n",
      "Validated batch 20 batch loss 1.30627894\n",
      "Validated batch 21 batch loss 1.26297021\n",
      "Validated batch 22 batch loss 1.26989579\n",
      "Validated batch 23 batch loss 1.22355151\n",
      "Validated batch 24 batch loss 1.3260721\n",
      "Validated batch 25 batch loss 1.28085971\n",
      "Validated batch 26 batch loss 1.16847897\n",
      "Validated batch 27 batch loss 1.17109883\n",
      "Validated batch 28 batch loss 1.18781257\n",
      "Validated batch 29 batch loss 1.30393445\n",
      "Validated batch 30 batch loss 1.20992231\n",
      "Validated batch 31 batch loss 1.22446835\n",
      "Validated batch 32 batch loss 1.28931308\n",
      "Validated batch 33 batch loss 1.26992989\n",
      "Validated batch 34 batch loss 1.18248451\n",
      "Validated batch 35 batch loss 1.13171387\n",
      "Validated batch 36 batch loss 1.29347789\n",
      "Validated batch 37 batch loss 1.31562889\n",
      "Validated batch 38 batch loss 1.38768232\n",
      "Validated batch 39 batch loss 1.36745477\n",
      "Validated batch 40 batch loss 1.23967695\n",
      "Validated batch 41 batch loss 1.41299319\n",
      "Validated batch 42 batch loss 1.23569608\n",
      "Validated batch 43 batch loss 1.31375337\n",
      "Validated batch 44 batch loss 1.30194712\n",
      "Validated batch 45 batch loss 1.06106043\n",
      "Validated batch 46 batch loss 1.30159605\n",
      "Validated batch 47 batch loss 1.33234072\n",
      "Validated batch 48 batch loss 1.24532461\n",
      "Validated batch 49 batch loss 1.21694911\n",
      "Validated batch 50 batch loss 1.21731973\n",
      "Validated batch 51 batch loss 1.29380608\n",
      "Validated batch 52 batch loss 1.39800406\n",
      "Validated batch 53 batch loss 1.19509554\n",
      "Validated batch 54 batch loss 1.3027575\n",
      "Validated batch 55 batch loss 1.22707283\n",
      "Validated batch 56 batch loss 1.2800858\n",
      "Validated batch 57 batch loss 1.24405801\n",
      "Validated batch 58 batch loss 1.14781868\n",
      "Validated batch 59 batch loss 1.42331696\n",
      "Validated batch 60 batch loss 1.18262255\n",
      "Validated batch 61 batch loss 1.33363152\n",
      "Validated batch 62 batch loss 1.22114062\n",
      "Validated batch 63 batch loss 1.34244347\n",
      "Validated batch 64 batch loss 1.14939034\n",
      "Validated batch 65 batch loss 1.26990092\n",
      "Validated batch 66 batch loss 1.22932577\n",
      "Validated batch 67 batch loss 1.21779621\n",
      "Validated batch 68 batch loss 1.30500019\n",
      "Validated batch 69 batch loss 1.25029874\n",
      "Validated batch 70 batch loss 1.18913865\n",
      "Validated batch 71 batch loss 1.2662456\n",
      "Validated batch 72 batch loss 1.33433497\n",
      "Validated batch 73 batch loss 1.18234575\n",
      "Validated batch 74 batch loss 1.31871283\n",
      "Validated batch 75 batch loss 1.41493547\n",
      "Validated batch 76 batch loss 1.12372792\n",
      "Validated batch 77 batch loss 1.26825976\n",
      "Validated batch 78 batch loss 1.26075697\n",
      "Validated batch 79 batch loss 1.30568016\n",
      "Validated batch 80 batch loss 1.2971493\n",
      "Validated batch 81 batch loss 1.15941095\n",
      "Validated batch 82 batch loss 1.08349049\n",
      "Validated batch 83 batch loss 1.24135268\n",
      "Validated batch 84 batch loss 1.19920969\n",
      "Validated batch 85 batch loss 1.21838379\n",
      "Validated batch 86 batch loss 1.27047086\n",
      "Validated batch 87 batch loss 1.17594504\n",
      "Validated batch 88 batch loss 1.24279237\n",
      "Validated batch 89 batch loss 1.34062457\n",
      "Validated batch 90 batch loss 1.31251049\n",
      "Validated batch 91 batch loss 1.23910284\n",
      "Validated batch 92 batch loss 1.16639459\n",
      "Validated batch 93 batch loss 1.24966717\n",
      "Validated batch 94 batch loss 1.08950305\n",
      "Validated batch 95 batch loss 1.26706254\n",
      "Validated batch 96 batch loss 1.16890883\n",
      "Validated batch 97 batch loss 1.22372794\n",
      "Validated batch 98 batch loss 1.23663592\n",
      "Validated batch 99 batch loss 1.26052332\n",
      "Validated batch 100 batch loss 1.2234118\n",
      "Validated batch 101 batch loss 1.28401625\n",
      "Validated batch 102 batch loss 1.19772851\n",
      "Validated batch 103 batch loss 1.31186318\n",
      "Validated batch 104 batch loss 1.26347911\n",
      "Validated batch 105 batch loss 1.17931116\n",
      "Validated batch 106 batch loss 1.19239366\n",
      "Validated batch 107 batch loss 1.28908277\n",
      "Validated batch 108 batch loss 1.2033335\n",
      "Validated batch 109 batch loss 1.37968409\n",
      "Validated batch 110 batch loss 1.26694751\n",
      "Validated batch 111 batch loss 1.21344876\n",
      "Validated batch 112 batch loss 1.31941831\n",
      "Validated batch 113 batch loss 1.22740364\n",
      "Validated batch 114 batch loss 1.17769408\n",
      "Validated batch 115 batch loss 1.24827361\n",
      "Validated batch 116 batch loss 1.36760497\n",
      "Validated batch 117 batch loss 1.25766611\n",
      "Validated batch 118 batch loss 1.20112467\n",
      "Validated batch 119 batch loss 1.24008799\n",
      "Validated batch 120 batch loss 1.17296469\n",
      "Validated batch 121 batch loss 1.27722239\n",
      "Validated batch 122 batch loss 1.25708473\n",
      "Validated batch 123 batch loss 1.19405198\n",
      "Validated batch 124 batch loss 1.19573808\n",
      "Validated batch 125 batch loss 1.25663853\n",
      "Validated batch 126 batch loss 1.29735637\n",
      "Validated batch 127 batch loss 1.31373262\n",
      "Validated batch 128 batch loss 1.27148533\n",
      "Validated batch 129 batch loss 1.18488479\n",
      "Validated batch 130 batch loss 1.22991908\n",
      "Validated batch 131 batch loss 1.2787956\n",
      "Validated batch 132 batch loss 1.25499392\n",
      "Validated batch 133 batch loss 1.29140949\n",
      "Validated batch 134 batch loss 1.30851102\n",
      "Validated batch 135 batch loss 1.46134925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 136 batch loss 1.39796066\n",
      "Validated batch 137 batch loss 1.2643292\n",
      "Validated batch 138 batch loss 1.17313933\n",
      "Validated batch 139 batch loss 1.14950991\n",
      "Validated batch 140 batch loss 1.29006219\n",
      "Validated batch 141 batch loss 1.23511934\n",
      "Validated batch 142 batch loss 1.15193772\n",
      "Validated batch 143 batch loss 1.27443206\n",
      "Validated batch 144 batch loss 1.2553972\n",
      "Validated batch 145 batch loss 1.31942081\n",
      "Validated batch 146 batch loss 1.33995485\n",
      "Validated batch 147 batch loss 1.28945577\n",
      "Validated batch 148 batch loss 1.22002661\n",
      "Validated batch 149 batch loss 1.29362869\n",
      "Validated batch 150 batch loss 1.28513622\n",
      "Validated batch 151 batch loss 1.25363159\n",
      "Validated batch 152 batch loss 1.30911875\n",
      "Validated batch 153 batch loss 1.36427903\n",
      "Validated batch 154 batch loss 1.25447094\n",
      "Validated batch 155 batch loss 1.36983299\n",
      "Validated batch 156 batch loss 1.21113729\n",
      "Validated batch 157 batch loss 1.19432867\n",
      "Validated batch 158 batch loss 1.2179482\n",
      "Validated batch 159 batch loss 1.16442871\n",
      "Validated batch 160 batch loss 1.34726703\n",
      "Validated batch 161 batch loss 1.20736051\n",
      "Validated batch 162 batch loss 1.22218871\n",
      "Validated batch 163 batch loss 1.27118075\n",
      "Validated batch 164 batch loss 1.26213276\n",
      "Validated batch 165 batch loss 1.13618326\n",
      "Validated batch 166 batch loss 1.19308364\n",
      "Validated batch 167 batch loss 1.33513689\n",
      "Validated batch 168 batch loss 1.16282821\n",
      "Validated batch 169 batch loss 1.14436531\n",
      "Validated batch 170 batch loss 1.16142178\n",
      "Validated batch 171 batch loss 1.2254827\n",
      "Validated batch 172 batch loss 1.12072062\n",
      "Validated batch 173 batch loss 1.25868487\n",
      "Validated batch 174 batch loss 1.11931968\n",
      "Validated batch 175 batch loss 1.27274227\n",
      "Validated batch 176 batch loss 1.3081584\n",
      "Validated batch 177 batch loss 1.31993783\n",
      "Validated batch 178 batch loss 1.17866516\n",
      "Validated batch 179 batch loss 1.37796187\n",
      "Validated batch 180 batch loss 1.07642961\n",
      "Validated batch 181 batch loss 1.07186902\n",
      "Validated batch 182 batch loss 1.21485114\n",
      "Validated batch 183 batch loss 1.17015219\n",
      "Validated batch 184 batch loss 1.34129477\n",
      "Validated batch 185 batch loss 1.37948632\n",
      "Epoch 3 val loss 1.2535542249679565\n",
      "Model /aiffel/aiffel/mpii/my_models/model-epoch-3-loss-1.2536.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "# 실행방법\n",
    "\n",
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')  \n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')  \n",
    "epochs = 3\n",
    "batch_size = 16  \n",
    "num_heatmap = 16  \n",
    "learning_rate = 0.0007\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536479",
   "metadata": {},
   "source": [
    "### 둠칫둠칫 댄스타임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850430a",
   "metadata": {},
   "source": [
    "#### 예측 엔진 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5da79",
   "metadata": {},
   "source": [
    "\n",
    "이제 학습이 끝난 모델이 얼마나 잘 예측하는지 확인해 볼 시간입니다. 미리 학습된 모델을 불러옵시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd27c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "\n",
    "# WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n",
    "# model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
    "\n",
    "# model.load_weights(best_model_file) # best_model_file생성 후 바로 사용 시\n",
    "BEST_MODEL_PATH = os.path.join(PROJECT_PATH, 'best_models','model-epoch-3-loss-1.2536.h5')\n",
    "model.load_weights(BEST_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ee2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ecd69d",
   "metadata": {},
   "source": [
    "학습에 사용했던 keypoint 들을 사용해야 하기 때문에 필요한 변수를 지정해 줍니다. 변수에 저장되는 것은 해당 부위를 나타내는 인덱스예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e6623",
   "metadata": {},
   "source": [
    "모델을 학습할 때 라벨이 되는 좌표를 heatmap으로 바꿨던 것이 기억 나시나요? 학습을 heatmap으로 했기 때문에 모델이 추론해 내놓은 결과도 heatmap입니다. 그래서 이 heatmap으로부터 좌표를 추출해야해요. heatmap중에 최대값을 갖는 지점을 찾아내면 되겠네요. heatmap에서 최대값을 찾는 함수를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cb640",
   "metadata": {},
   "source": [
    "위 함수만으로는 256x256 이미지에 64x64 heatmap max 값을 표현할 때 quantization 오차가 발생하기 때문에 실제 계산에서는 3x3 필터를 이용해서 근사치를 구해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ccbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f6b09",
   "metadata": {},
   "source": [
    "이제 모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수를 만들어 줍니다. 편리하게 사용할 수 있도록이요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56841e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fab39",
   "metadata": {},
   "source": [
    "거의 다 온 것 같습니다. 이제 그림만 그려주면 완성될 것 같네요. 그림을 그릴 때는 두 가지 그림을 그려볼 겁니다. keypoint들과 뼈대입니다. keypoint들은 관절 역할을 하고 keypoint들을 연결시킨 것이 뼈대가 되겠네요.\n",
    "\n",
    "두 가지 함수를 각각 작성해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e80362",
   "metadata": {},
   "source": [
    "자 이제 테스트 이미지를 이용해 모델의 성능을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dcada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
